{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12454526,"sourceType":"datasetVersion","datasetId":7856353}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"954d6bd4-b5eb-49e7-8187-a217b9dcac74","cell_type":"code","source":"!pip install transformers==4.44.2 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T06:59:54.734854Z","iopub.execute_input":"2025-07-13T06:59:54.735339Z","iopub.status.idle":"2025-07-13T07:00:10.673049Z","shell.execute_reply.started":"2025-07-13T06:59:54.735314Z","shell.execute_reply":"2025-07-13T07:00:10.672164Z"}},"outputs":[{"name":"stdout","text":"Collecting transformers==4.44.2\n  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (2.32.4)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (0.5.3)\nCollecting tokenizers<0.20,>=0.19 (from transformers==4.44.2)\n  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (2025.5.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.44.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.44.2) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.44.2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.44.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.44.2) (2024.2.0)\nDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.52.4\n    Uninstalling transformers-4.52.4:\n      Successfully uninstalled transformers-4.52.4\nSuccessfully installed tokenizers-0.19.1 transformers-4.44.2\n","output_type":"stream"}],"execution_count":1},{"id":"dd321882-ff9d-42d4-89ea-cf58885b8d7c","cell_type":"code","source":"!pip install joblib==1.4.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:00:10.674670Z","iopub.execute_input":"2025-07-13T07:00:10.674912Z","iopub.status.idle":"2025-07-13T07:00:14.403695Z","shell.execute_reply.started":"2025-07-13T07:00:10.674884Z","shell.execute_reply":"2025-07-13T07:00:14.403065Z"}},"outputs":[{"name":"stdout","text":"Collecting joblib==1.4.2\n  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\nDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: joblib\n  Attempting uninstall: joblib\n    Found existing installation: joblib 1.5.1\n    Uninstalling joblib-1.5.1:\n      Successfully uninstalled joblib-1.5.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed joblib-1.4.2\n","output_type":"stream"}],"execution_count":2},{"id":"33d58b53-5425-427d-b900-fc0b6846ca64","cell_type":"code","source":"!pip install scikit-learn==1.6.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:00:14.405007Z","iopub.execute_input":"2025-07-13T07:00:14.405290Z","iopub.status.idle":"2025-07-13T07:00:21.795092Z","shell.execute_reply.started":"2025-07-13T07:00:14.405255Z","shell.execute_reply":"2025-07-13T07:00:21.794359Z"}},"outputs":[{"name":"stdout","text":"Collecting scikit-learn==1.6.0\n  Downloading scikit_learn-1.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.0) (1.26.4)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.0) (1.15.3)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.0) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.0) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.6.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.6.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.6.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.6.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.6.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.6.0) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.6.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.6.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.5->scikit-learn==1.6.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.5->scikit-learn==1.6.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.5->scikit-learn==1.6.0) (2024.2.0)\nDownloading scikit_learn-1.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: scikit-learn\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scikit-learn-1.6.0\n","output_type":"stream"}],"execution_count":3},{"id":"0be4620b-a715-4cbe-9c5a-6331cb1ecf15","cell_type":"code","source":"!pip install numpy==1.26.4 pandas==2.2.3 scipy==1.13.1 lightgbm==4.5.0 xgboost==2.1.3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:00:21.797006Z","iopub.execute_input":"2025-07-13T07:00:21.797221Z","iopub.status.idle":"2025-07-13T07:00:40.539347Z","shell.execute_reply.started":"2025-07-13T07:00:21.797199Z","shell.execute_reply":"2025-07-13T07:00:40.538662Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: pandas==2.2.3 in /usr/local/lib/python3.11/dist-packages (2.2.3)\nCollecting scipy==1.13.1\n  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: lightgbm==4.5.0 in /usr/local/lib/python3.11/dist-packages (4.5.0)\nCollecting xgboost==2.1.3\n  Downloading xgboost-2.1.3-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3) (2025.2)\nRequirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost==2.1.3) (2.21.5)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.3) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.26.4) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.26.4) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy==1.26.4) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy==1.26.4) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy==1.26.4) (2024.2.0)\nDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading xgboost-2.1.3-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.9/153.9 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scipy, xgboost\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.15.3\n    Uninstalling scipy-1.15.3:\n      Successfully uninstalled scipy-1.15.3\n  Attempting uninstall: xgboost\n    Found existing installation: xgboost 2.0.3\n    Uninstalling xgboost-2.0.3:\n      Successfully uninstalled xgboost-2.0.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scipy-1.13.1 xgboost-2.1.3\n","output_type":"stream"}],"execution_count":4},{"id":"7b285d19-55cb-4a81-bbc7-810ef8b7b37a","cell_type":"code","source":"!pip install seaborn==0.13.2 tqdm==4.66.5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:00:40.540275Z","iopub.execute_input":"2025-07-13T07:00:40.540507Z","iopub.status.idle":"2025-07-13T07:00:44.870630Z","shell.execute_reply.started":"2025-07-13T07:00:40.540484Z","shell.execute_reply":"2025-07-13T07:00:44.869927Z"}},"outputs":[{"name":"stdout","text":"Collecting seaborn==0.13.2\n  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\nCollecting tqdm==4.66.5\n  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn==0.13.2) (1.26.4)\nRequirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn==0.13.2) (2.2.3)\nRequirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn==0.13.2) (3.7.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2) (11.2.1)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.20->seaborn==0.13.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.20->seaborn==0.13.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.20->seaborn==0.13.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.20->seaborn==0.13.2) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.20->seaborn==0.13.2) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.20->seaborn==0.13.2) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn==0.13.2) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn==0.13.2) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn==0.13.2) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy!=1.24.0,>=1.20->seaborn==0.13.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy!=1.24.0,>=1.20->seaborn==0.13.2) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy!=1.24.0,>=1.20->seaborn==0.13.2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy!=1.24.0,>=1.20->seaborn==0.13.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy!=1.24.0,>=1.20->seaborn==0.13.2) (2024.2.0)\nDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tqdm, seaborn\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.67.1\n    Uninstalling tqdm-4.67.1:\n      Successfully uninstalled tqdm-4.67.1\n  Attempting uninstall: seaborn\n    Found existing installation: seaborn 0.12.2\n    Uninstalling seaborn-0.12.2:\n      Successfully uninstalled seaborn-0.12.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ndataproc-spark-connect 0.7.5 requires tqdm>=4.67, but you have tqdm 4.66.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed seaborn-0.13.2 tqdm-4.66.5\n","output_type":"stream"}],"execution_count":5},{"id":"c4a67708-1471-44d0-aae4-dabee689cac9","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"77ea0e60-9cfe-42de-b934-664587e8c25d","cell_type":"code","source":"import os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport numpy as np\nimport random\nfrom transformers import AutoModel, AutoTokenizer,AutoModelForSequenceClassification, AdamW\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, random_split\n\n# Load datasets\ntrain_df = pd.read_csv('/kaggle/input/rrck-set/Train_RRCK.csv')\ntrain_df = train_df[['ID', 'SMILES', 'Permeability']]\ntest_df = pd.read_csv('/kaggle/input/rrck-set/Test_RRCK.csv')\ntest_df = test_df[['ID', 'SMILES', 'Permeability']]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:00:59.606757Z","iopub.execute_input":"2025-07-13T07:00:59.607517Z","iopub.status.idle":"2025-07-13T07:01:07.484024Z","shell.execute_reply.started":"2025-07-13T07:00:59.607484Z","shell.execute_reply":"2025-07-13T07:01:07.483403Z"}},"outputs":[],"execution_count":6},{"id":"4f1bd791-38e5-4a4e-900f-28bbbf691e7e","cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\"ibm/MoLFormer-XL-both-10pct\", deterministic_eval=True, trust_remote_code=True, num_labels=1)\ntokenizer = AutoTokenizer.from_pretrained(\"ibm/MoLFormer-XL-both-10pct\", trust_remote_code=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:03:45.361467Z","iopub.execute_input":"2025-07-13T07:03:45.362068Z","iopub.status.idle":"2025-07-13T07:03:46.328471Z","shell.execute_reply.started":"2025-07-13T07:03:45.362042Z","shell.execute_reply":"2025-07-13T07:03:46.327950Z"}},"outputs":[{"name":"stderr","text":"Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":25},{"id":"05ab00aa-cce1-4775-b3ed-e21d7d27420d","cell_type":"code","source":"#Custom dataset class\nclass SMILESDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length=325):\n        self.tokenizer = tokenizer\n        self.dataframe = dataframe\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        smiles = self.dataframe.iloc[idx]['SMILES']\n        permeability = self.dataframe.iloc[idx]['Permeability']\n        inputs = self.tokenizer(smiles, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=self.max_length)\n        \n        input_ids = inputs['input_ids'].squeeze(0)  # Shape: (sequence_length,)\n        attention_mask = inputs['attention_mask'].squeeze(0)  # Shape: (sequence_length,)\n        \n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'labels': torch.tensor(permeability, dtype=torch.float)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:03:49.030037Z","iopub.execute_input":"2025-07-13T07:03:49.030281Z","iopub.status.idle":"2025-07-13T07:03:49.035654Z","shell.execute_reply.started":"2025-07-13T07:03:49.030264Z","shell.execute_reply":"2025-07-13T07:03:49.035097Z"}},"outputs":[],"execution_count":26},{"id":"36e5e17b-ca6a-448b-baf2-a471eecae492","cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:03:52.524785Z","iopub.execute_input":"2025-07-13T07:03:52.525415Z","iopub.status.idle":"2025-07-13T07:03:52.613097Z","shell.execute_reply.started":"2025-07-13T07:03:52.525390Z","shell.execute_reply":"2025-07-13T07:03:52.612575Z"}},"outputs":[],"execution_count":27},{"id":"6cb4e4d1-cda6-42c4-b628-69bf690be558","cell_type":"code","source":"train_dataset = SMILESDataset(train_df, tokenizer)\ntest_dataset = SMILESDataset(test_df, tokenizer)\nbatch_size = 16\n#Data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:03:54.981850Z","iopub.execute_input":"2025-07-13T07:03:54.982434Z","iopub.status.idle":"2025-07-13T07:03:54.987189Z","shell.execute_reply.started":"2025-07-13T07:03:54.982413Z","shell.execute_reply":"2025-07-13T07:03:54.986598Z"}},"outputs":[],"execution_count":28},{"id":"ba8955db-521c-4936-8961-0e1221f2565c","cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\nnum_epochs = 20","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:03:56.893982Z","iopub.execute_input":"2025-07-13T07:03:56.894247Z","iopub.status.idle":"2025-07-13T07:03:56.901273Z","shell.execute_reply.started":"2025-07-13T07:03:56.894231Z","shell.execute_reply":"2025-07-13T07:03:56.900518Z"}},"outputs":[],"execution_count":29},{"id":"9290db40-0a74-4930-9107-d7e60cec9358","cell_type":"code","source":"#Training loop\nfrom tqdm import tqdm\nfor epoch in range(num_epochs):\n    print(f\"Entered Epoch {epoch + 1}\")\n    model.train()\n    train_loss = 0\n\n    for batch in tqdm(train_loader, desc=f'Training Epoch {epoch + 1}/{num_epochs}', unit='batch'):\n        optimizer.zero_grad()\n\n        # Move all batch tensors to device\n        batch = {k: v.to(device) for k, v in batch.items()}\n        labels = batch[\"labels\"].unsqueeze(1)  # still shape: (batch_size, 1)\n\n        # Forward pass\n        outputs = model(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n            labels=labels\n        )\n        loss = outputs.loss\n        train_loss += loss.item()\n\n        # Backprop and optimizer step\n        loss.backward()\n        optimizer.step()\n\n    avg_train_loss = train_loss / len(train_loader)\n    print(f'Epoch {epoch + 1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:03:58.835282Z","iopub.execute_input":"2025-07-13T07:03:58.835533Z","iopub.status.idle":"2025-07-13T07:05:19.498145Z","shell.execute_reply.started":"2025-07-13T07:03:58.835514Z","shell.execute_reply":"2025-07-13T07:05:19.497465Z"}},"outputs":[{"name":"stdout","text":"Entered Epoch 1\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1/20: 100%|██████████| 9/9 [00:03<00:00,  2.33batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20 - Train Loss: 8.5978\nEntered Epoch 2\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2/20: 100%|██████████| 9/9 [00:04<00:00,  2.24batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/20 - Train Loss: 0.6955\nEntered Epoch 3\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 3/20: 100%|██████████| 9/9 [00:04<00:00,  2.23batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/20 - Train Loss: 0.5678\nEntered Epoch 4\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 4/20: 100%|██████████| 9/9 [00:04<00:00,  2.21batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/20 - Train Loss: 0.5918\nEntered Epoch 5\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 5/20: 100%|██████████| 9/9 [00:04<00:00,  2.20batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/20 - Train Loss: 0.5972\nEntered Epoch 6\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 6/20: 100%|██████████| 9/9 [00:04<00:00,  2.18batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/20 - Train Loss: 0.5163\nEntered Epoch 7\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 7/20: 100%|██████████| 9/9 [00:04<00:00,  2.17batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/20 - Train Loss: 0.4326\nEntered Epoch 8\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 8/20: 100%|██████████| 9/9 [00:04<00:00,  2.18batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/20 - Train Loss: 0.3866\nEntered Epoch 9\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 9/20: 100%|██████████| 9/9 [00:04<00:00,  2.21batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/20 - Train Loss: 0.4117\nEntered Epoch 10\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 10/20: 100%|██████████| 9/9 [00:04<00:00,  2.21batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/20 - Train Loss: 0.4477\nEntered Epoch 11\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 11/20: 100%|██████████| 9/9 [00:04<00:00,  2.22batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/20 - Train Loss: 0.3590\nEntered Epoch 12\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 12/20: 100%|██████████| 9/9 [00:04<00:00,  2.23batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/20 - Train Loss: 0.3649\nEntered Epoch 13\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 13/20: 100%|██████████| 9/9 [00:04<00:00,  2.24batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/20 - Train Loss: 0.3375\nEntered Epoch 14\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 14/20: 100%|██████████| 9/9 [00:03<00:00,  2.25batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/20 - Train Loss: 0.3346\nEntered Epoch 15\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 15/20: 100%|██████████| 9/9 [00:03<00:00,  2.25batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/20 - Train Loss: 0.2763\nEntered Epoch 16\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 16/20: 100%|██████████| 9/9 [00:03<00:00,  2.26batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/20 - Train Loss: 0.2825\nEntered Epoch 17\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 17/20: 100%|██████████| 9/9 [00:03<00:00,  2.26batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/20 - Train Loss: 0.2619\nEntered Epoch 18\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 18/20: 100%|██████████| 9/9 [00:03<00:00,  2.27batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/20 - Train Loss: 0.2898\nEntered Epoch 19\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 19/20: 100%|██████████| 9/9 [00:03<00:00,  2.26batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/20 - Train Loss: 0.2607\nEntered Epoch 20\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 20/20: 100%|██████████| 9/9 [00:03<00:00,  2.27batch/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 20/20 - Train Loss: 0.2292\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":30},{"id":"29c59014","cell_type":"code","source":"# Save the model after training\nmodel_name = 'MoLFormer-XL-both-10pct_model_1_rrck'\nmodel_save_path = f'/kaggle/working/{model_name}'\nos.makedirs(model_save_path, exist_ok=True)\n\ntokenizer.save_pretrained(model_save_path)\nmodel.save_pretrained(model_save_path, safe_serialization=False)\n\nprint(f'Model and tokenizer saved to {model_save_path}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:05:19.499634Z","iopub.execute_input":"2025-07-13T07:05:19.499879Z","iopub.status.idle":"2025-07-13T07:05:20.063030Z","shell.execute_reply.started":"2025-07-13T07:05:19.499862Z","shell.execute_reply":"2025-07-13T07:05:20.062264Z"}},"outputs":[{"name":"stdout","text":"Model and tokenizer saved to /kaggle/working/MoLFormer-XL-both-10pct_model_1_rrck\n","output_type":"stream"}],"execution_count":31},{"id":"b170bb7d-5df7-4b82-955f-0a3e58fbd500","cell_type":"code","source":"from scipy.stats import pearsonr, spearmanr\n\n# Set model to eval mode\nmodel.eval()\ntest_loss = 0\ntest_true_labels = []\npredictions = []\n\n# No gradient calculation during evaluation\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc='Testing', unit='batch'):\n      \n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].unsqueeze(1).to(device).float()\n\n        # Forward pass\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        test_loss += loss.item()\n\n        # Collect predictions and true labels (back to CPU for evaluation)\n        test_true_labels.extend(labels.cpu().numpy())\n        preds = outputs.logits.squeeze().cpu().numpy()  # Shape: (batch_size,)\n        predictions.extend(preds)\n\n# Final test loss\navg_test_loss = test_loss / len(test_loader)\nprint(f'Test Loss: {avg_test_loss:.4f}')\n\ntest_true_labels = np.array(test_true_labels).flatten()\npredictions = np.array(predictions)\nprint(test_true_labels.shape)\nprint(predictions.shape)\n# Performance metrics\nmse = mean_squared_error(test_true_labels, predictions)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(test_true_labels, predictions)\nr2 = r2_score(test_true_labels, predictions)\nPCC,_ = pearsonr(test_true_labels, predictions)\nSCC,_ = spearmanr(test_true_labels, predictions)\n\n# Print performance metrics\nprint(f'Mean Squared Error: {mse:.4f}')\nprint(f'Root Mean Squared Error: {rmse:.4f}')\nprint(f'Mean Absolute Error: {mae:.4f}')\nprint(f'R^2 Score: {r2:.4f}')\nprint(f'Pearson Correlation Coefficient: {PCC:.4f}')\nprint(f'Spearman Correlation Coefficient: {SCC:.4f}')\n\n# Print hyperparameters\nprint(\"Hyperparameters:\")\nprint(f\"Learning Rate: {5e-5}\")\nprint(f\"Batch Size: 16\")\nprint(f\"Epochs: {num_epochs}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:05:20.063852Z","iopub.execute_input":"2025-07-13T07:05:20.064126Z","iopub.status.idle":"2025-07-13T07:05:20.474246Z","shell.execute_reply.started":"2025-07-13T07:05:20.064108Z","shell.execute_reply":"2025-07-13T07:05:20.473379Z"}},"outputs":[{"name":"stderr","text":"Testing: 100%|██████████| 3/3 [00:00<00:00,  7.66batch/s]","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.4137\n(36,)\n(36,)\nMean Squared Error: 0.5352\nRoot Mean Squared Error: 0.7316\nMean Absolute Error: 0.5483\nR^2 Score: 0.2596\nPearson Correlation Coefficient: 0.5174\nSpearman Correlation Coefficient: 0.6000\nHyperparameters:\nLearning Rate: 5e-05\nBatch Size: 16\nEpochs: 20\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":32},{"id":"3a3a3c4f-358d-4c3c-a889-1a0f7e922e86","cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"id":"d3c1014b-07d3-47c1-ae82-36a0de05cb30","cell_type":"code","source":"#MolFormer_XL Fine tuned Model_1 SMILES Embeddings\nimport os\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom transformers import AutoTokenizer, AutoModel\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# Define the model name and path\nmodel_name = 'MoLFormer-XL-both-10pct_model_1_rrck'\nmodel_save_path = f'/kaggle/working/{model_name}'\n\nif not os.path.exists(model_save_path):\n    raise FileNotFoundError(f\"The model directory {model_save_path} does not exist.\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_save_path, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(model_save_path, trust_remote_code=True).to(device) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:05:20.475652Z","iopub.execute_input":"2025-07-13T07:05:20.475859Z","iopub.status.idle":"2025-07-13T07:05:21.189330Z","shell.execute_reply.started":"2025-07-13T07:05:20.475843Z","shell.execute_reply":"2025-07-13T07:05:21.188792Z"}},"outputs":[],"execution_count":33},{"id":"3567b06a-a6e1-4d8b-9045-fbee8c512898","cell_type":"code","source":"# Load datasets\ntrain_df = pd.read_csv('/kaggle/input/rrck-set/Train_RRCK.csv')\ntrain_df = train_df[['ID', 'SMILES', 'Permeability']]\ntest_df = pd.read_csv('/kaggle/input/rrck-set/Test_RRCK.csv')\ntest_df = test_df[['ID', 'SMILES', 'Permeability']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:05:21.190001Z","iopub.execute_input":"2025-07-13T07:05:21.190244Z","iopub.status.idle":"2025-07-13T07:05:21.203569Z","shell.execute_reply.started":"2025-07-13T07:05:21.190226Z","shell.execute_reply":"2025-07-13T07:05:21.203016Z"}},"outputs":[],"execution_count":34},{"id":"5da51286-aa61-4d50-acfb-f9a89c049327","cell_type":"code","source":"train_encodings = tokenizer(list(train_df['SMILES']), truncation=True, padding=True, max_length=325, return_tensors=\"pt\")\ntest_encodings = tokenizer(list(test_df['SMILES']), truncation=True, padding=True, max_length=325, return_tensors=\"pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:05:21.204195Z","iopub.execute_input":"2025-07-13T07:05:21.204348Z","iopub.status.idle":"2025-07-13T07:05:21.242477Z","shell.execute_reply.started":"2025-07-13T07:05:21.204334Z","shell.execute_reply":"2025-07-13T07:05:21.241813Z"}},"outputs":[],"execution_count":35},{"id":"ce14b286-75fa-4744-b14d-d64051d95034","cell_type":"code","source":"from tqdm import tqdm \nbatch_size = 16 \n\ndef generate_embeddings(encodings, batch_size):\n    embeddings = []\n    model.eval() \n    with torch.no_grad():\n        for i in tqdm(range(0, len(encodings['input_ids']), batch_size), desc=\"Processing batches\"):\n            batch = {key: val[i:i + batch_size].to(device) for key, val in encodings.items()}  \n            outputs = model(**batch)\n            embeddings.append(outputs.last_hidden_state)\n    return torch.cat(embeddings, dim=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:05:21.243187Z","iopub.execute_input":"2025-07-13T07:05:21.243409Z","iopub.status.idle":"2025-07-13T07:05:21.248488Z","shell.execute_reply.started":"2025-07-13T07:05:21.243390Z","shell.execute_reply":"2025-07-13T07:05:21.247746Z"}},"outputs":[],"execution_count":36},{"id":"9a6462f5-bf7c-4541-9c3f-480ba8a6930a","cell_type":"code","source":"train_embeddings = generate_embeddings(train_encodings, batch_size)\nprint(train_embeddings.shape)\ntrain_embeddings = torch.mean(train_embeddings, dim=1)\nprint(train_embeddings.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:05:21.249089Z","iopub.execute_input":"2025-07-13T07:05:21.249267Z","iopub.status.idle":"2025-07-13T07:05:21.992373Z","shell.execute_reply.started":"2025-07-13T07:05:21.249253Z","shell.execute_reply":"2025-07-13T07:05:21.991764Z"}},"outputs":[{"name":"stderr","text":"Processing batches: 100%|██████████| 9/9 [00:00<00:00, 12.44it/s]","output_type":"stream"},{"name":"stdout","text":"torch.Size([140, 175, 768])\ntorch.Size([140, 768])\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":37},{"id":"4946aa76-c528-4187-92b1-86531d92c493","cell_type":"code","source":"column_names = [f'x_fine_emb_MFXL{i}' for i in range(train_embeddings.shape[1])]\nembeddings_df = pd.DataFrame(data=train_embeddings.cpu().numpy(), columns=column_names)\ntrain_data = pd.concat([train_df, embeddings_df], axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:05:21.993226Z","iopub.execute_input":"2025-07-13T07:05:21.993715Z","iopub.status.idle":"2025-07-13T07:05:22.042161Z","shell.execute_reply.started":"2025-07-13T07:05:21.993697Z","shell.execute_reply":"2025-07-13T07:05:22.041648Z"}},"outputs":[],"execution_count":38},{"id":"120c3704-b8c6-4f63-823a-192c7df98112","cell_type":"code","source":"test_embeddings = generate_embeddings(test_encodings, batch_size)\nprint(test_embeddings.shape)\ntest_embeddings = torch.mean(test_embeddings, dim=1)\nprint(test_embeddings.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:05:22.043747Z","iopub.execute_input":"2025-07-13T07:05:22.043959Z","iopub.status.idle":"2025-07-13T07:05:22.242786Z","shell.execute_reply.started":"2025-07-13T07:05:22.043921Z","shell.execute_reply":"2025-07-13T07:05:22.242100Z"}},"outputs":[{"name":"stderr","text":"Processing batches: 100%|██████████| 3/3 [00:00<00:00, 15.69it/s]","output_type":"stream"},{"name":"stdout","text":"torch.Size([36, 174, 768])\ntorch.Size([36, 768])\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":39},{"id":"4b1c98c1-e61a-4363-90ce-0a776782ba8a","cell_type":"code","source":"column_names = [f'x_fine_emb_MFXL{i}' for i in range(test_embeddings.shape[1])]\nembeddings_df = pd.DataFrame(data=test_embeddings.cpu().numpy(), columns=column_names)\ntest_data = pd.concat([test_df, embeddings_df], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:05:22.243534Z","iopub.execute_input":"2025-07-13T07:05:22.243774Z","iopub.status.idle":"2025-07-13T07:05:22.251449Z","shell.execute_reply.started":"2025-07-13T07:05:22.243754Z","shell.execute_reply":"2025-07-13T07:05:22.250875Z"}},"outputs":[],"execution_count":40},{"id":"e50f9b93-befe-4ede-a418-0550c3b8346b","cell_type":"code","source":"train_data.to_csv(\"/kaggle/working/Train_MoLFormer-XL-both-10pct_model_1_fine_tuned_embeddings_rrck.csv\",index=False)\ntest_data.to_csv(\"/kaggle/working/Test_MoLFormer-XL-both-10pct_model_1_fine_tuned_embeddings_rrck.csv\",index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:07:38.257460Z","iopub.execute_input":"2025-07-13T07:07:38.257742Z","iopub.status.idle":"2025-07-13T07:07:38.427703Z","shell.execute_reply.started":"2025-07-13T07:07:38.257720Z","shell.execute_reply":"2025-07-13T07:07:38.426984Z"}},"outputs":[],"execution_count":46},{"id":"786620b2","cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"id":"92ed4099-571b-4754-8c59-32dad30bb8f5","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neural_network import MLPRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom scipy.stats import pearsonr, spearmanr\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:07:11.502821Z","iopub.execute_input":"2025-07-13T07:07:11.503319Z","iopub.status.idle":"2025-07-13T07:07:16.489606Z","shell.execute_reply.started":"2025-07-13T07:07:11.503295Z","shell.execute_reply":"2025-07-13T07:07:16.489070Z"}},"outputs":[],"execution_count":42},{"id":"aee84f62","cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/working/Train_MoLFormer-XL-both-10pct_model_1_fine_tuned_embeddings_rrck.csv\")\ntest_data = pd.read_csv(\"/kaggle/working/Test_MoLFormer-XL-both-10pct_model_1_fine_tuned_embeddings_rrck.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:08:21.131449Z","iopub.execute_input":"2025-07-13T07:08:21.131735Z","iopub.status.idle":"2025-07-13T07:08:21.180781Z","shell.execute_reply.started":"2025-07-13T07:08:21.131713Z","shell.execute_reply":"2025-07-13T07:08:21.180286Z"}},"outputs":[],"execution_count":49},{"id":"aeb8e51f-eca2-45b7-b7c2-5c185e9406ec","cell_type":"code","source":"def train_and_test_predict(models, X_train, y_train, X_test, y_test):\n    kf = KFold(n_splits=5, shuffle=True, random_state=101)\n    results = {}\n    predictions = []  \n\n    for model in models:\n        model_name = model.__class__.__name__\n        predictions_train = []\n        actual_y_train = []\n\n        test_predictions_folds = []\n\n        \n\n        for train_index, val_index in kf.split(X_train):\n            X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n            y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n\n            model.fit(X_train_fold, y_train_fold)\n\n            y_pred_fold = model.predict(X_val_fold)\n            y_pred_fold = np.clip(y_pred_fold, -10, -3.9)\n            predictions_train.extend(y_pred_fold)\n            actual_y_train.extend(y_val_fold)\n\n            predictions_test_fold = model.predict(X_test)\n            predictions_test_fold = np.clip(predictions_test_fold, -10, -3.9)\n            test_predictions_folds.append(predictions_test_fold)\n\n\n        mse_train = mean_squared_error(actual_y_train, predictions_train)\n        mae_train = mean_absolute_error(actual_y_train, predictions_train)\n        rmse_train = np.sqrt(mse_train)\n        r2_train = r2_score(actual_y_train, predictions_train)\n        pearson_train, _ = pearsonr(actual_y_train, predictions_train)\n        spearman_train, _ = spearmanr(actual_y_train, predictions_train)\n\n\n        predictions_test_mean = np.mean(test_predictions_folds, axis=0)\n        predictions_test_std = np.std(test_predictions_folds, axis=0)\n\n        mse_test = mean_squared_error(y_test, predictions_test_mean)\n        mae_test = mean_absolute_error(y_test, predictions_test_mean)\n        rmse_test = np.sqrt(mse_test)\n        r2_test = r2_score(y_test, predictions_test_mean)\n        print(r2_test)\n        pearson_test, _ = pearsonr(y_test, predictions_test_mean)\n        spearman_test, _ = spearmanr(y_test, predictions_test_mean)\n        \n        \n\n        predictions.append({\n            'Model': model_name,\n            'Y Train pred': predictions_train,\n            'Y Test actual': y_test,\n            'Test prediction folds': test_predictions_folds,\n            'Test Predictions Mean': predictions_test_mean,\n            'Test Predictions Std': predictions_test_std,\n\n        })\n\n        results[model_name] = {\n            'Train MSE (5 fold cv)': f\"{mse_train:.4f}\",\n            'Train MAE (5 fold cv)': f\"{mae_train:.4f}\",\n            'Train RMSE (5 fold cv)': f\"{rmse_train:.4f}\",\n            'Train RMSE (5 fold cv)': f\"{rmse_train:.4f}\",\n            'Train R2 (5 fold cv)': f\"{r2_train:.4f}\",\n            'Train PCC (5 fold cv)': f\"{pearson_train:.4f}\",\n            'Train SCC (5 fold cv)': f\"{spearman_train:.4f}\",\n            'Test MSE': f\"{mse_test:.4f}\",\n            'Test MAE': f\"{mae_test:.4f}\",\n            'Test RMSE': f\"{rmse_test:.4f}\",\n            'Test R2': f\"{r2_test:.4f}\",\n            'Test Pearson Correlation': f\"{pearson_test:.4f}\",\n            'Test Spearman Correlation': f\"{spearman_test:.4f}\",\n        }\n\n    results_df = pd.DataFrame(results).T\n    predictions_df = pd.DataFrame(predictions)\n\n    return results_df, predictions_df\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:08:23.356617Z","iopub.execute_input":"2025-07-13T07:08:23.357103Z","iopub.status.idle":"2025-07-13T07:08:23.368445Z","shell.execute_reply.started":"2025-07-13T07:08:23.357081Z","shell.execute_reply":"2025-07-13T07:08:23.367756Z"}},"outputs":[],"execution_count":50},{"id":"152c2ac3-89bd-4d22-a778-6781d46adc13","cell_type":"code","source":"X_train = train_data.drop(['ID','SMILES','Permeability'],axis=1)\ny_train = train_data['Permeability']\nprint(\"X_train shape: \",X_train.shape)\nprint(\"y_train shape: \",y_train.shape)\nprint(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n\nX_test = test_data.drop(['ID','SMILES','Permeability'],axis=1)\ny_test = test_data['Permeability']\nprint(\"X_test shape: \",X_test.shape)\nprint(\"y_test shape: \",y_test.shape)\nprint(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nX_train = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\nX_test = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\nmodels = [\n    lgb.LGBMRegressor(objective='regression',metric='rmse',boosting_type='gbdt',num_leaves=31,learning_rate=0.05,random_state=101),\n    DecisionTreeRegressor(random_state=101),\n    RandomForestRegressor(n_jobs=-1, random_state=101),\n    GradientBoostingRegressor(random_state=101),\n    AdaBoostRegressor(random_state=101),\n    xgb.XGBRegressor(random_state=101),\n    ExtraTreesRegressor(n_jobs=-1, n_estimators=100, random_state=101),\n    LinearRegression(), \n    KNeighborsRegressor(n_neighbors=3),\n    SVR(),  \n    MLPRegressor(random_state=101)\n]\nresult_df, prediction_df = train_and_test_predict(models, X_train,y_train, X_test,  y_test)\nresult_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:08:27.000493Z","iopub.execute_input":"2025-07-13T07:08:27.000973Z","iopub.status.idle":"2025-07-13T07:09:05.128964Z","shell.execute_reply.started":"2025-07-13T07:08:27.000950Z","shell.execute_reply":"2025-07-13T07:09:05.128209Z"}},"outputs":[{"name":"stdout","text":"X_train shape:  (140, 768)\ny_train shape:  (140,)\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nX_test shape:  (36, 768)\ny_test shape:  (36,)\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003720 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 29951\n[LightGBM] [Info] Number of data points in the train set: 112, number of used features: 768\n[LightGBM] [Info] Start training from score -5.527679\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001172 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 29952\n[LightGBM] [Info] Number of data points in the train set: 112, number of used features: 768\n[LightGBM] [Info] Start training from score -5.528750\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001288 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 29952\n[LightGBM] [Info] Number of data points in the train set: 112, number of used features: 768\n[LightGBM] [Info] Start training from score -5.549911\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001152 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 29952\n[LightGBM] [Info] Number of data points in the train set: 112, number of used features: 768\n[LightGBM] [Info] Start training from score -5.586518\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001207 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 29952\n[LightGBM] [Info] Number of data points in the train set: 112, number of used features: 768\n[LightGBM] [Info] Start training from score -5.496429\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n0.3517785452661978\n0.4845954774792648\n0.33632900734299\n0.39166488567465263\n0.373959450889105\n0.41262366481110146\n0.35264615298771773\n-0.020085139661258244\n0.23309444552391523\n0.30168247645223956\n-0.010683513842305814\n","output_type":"stream"},{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"                          Train MSE (5 fold cv) Train MAE (5 fold cv)  \\\nLGBMRegressor                            0.1779                0.3114   \nDecisionTreeRegressor                    0.3389                0.4221   \nRandomForestRegressor                    0.1743                0.3150   \nGradientBoostingRegressor                0.1933                0.3294   \nAdaBoostRegressor                        0.1821                0.3304   \nXGBRegressor                             0.2303                0.3541   \nExtraTreesRegressor                      0.1628                0.3068   \nLinearRegression                         0.3154                0.4361   \nKNeighborsRegressor                      0.2394                0.3468   \nSVR                                      0.1787                0.3133   \nMLPRegressor                             0.7564                0.6614   \n\n                          Train RMSE (5 fold cv) Train R2 (5 fold cv)  \\\nLGBMRegressor                             0.4218               0.6031   \nDecisionTreeRegressor                     0.5821               0.2441   \nRandomForestRegressor                     0.4175               0.6111   \nGradientBoostingRegressor                 0.4396               0.5689   \nAdaBoostRegressor                         0.4267               0.5939   \nXGBRegressor                              0.4799               0.4863   \nExtraTreesRegressor                       0.4035               0.6369   \nLinearRegression                          0.5616               0.2965   \nKNeighborsRegressor                       0.4893               0.4659   \nSVR                                       0.4227               0.6014   \nMLPRegressor                              0.8697              -0.6873   \n\n                          Train PCC (5 fold cv) Train SCC (5 fold cv)  \\\nLGBMRegressor                            0.7784                0.7841   \nDecisionTreeRegressor                    0.6152                0.6228   \nRandomForestRegressor                    0.7839                0.7974   \nGradientBoostingRegressor                0.7552                0.7631   \nAdaBoostRegressor                        0.7716                0.7799   \nXGBRegressor                             0.7047                0.7125   \nExtraTreesRegressor                      0.7994                0.8139   \nLinearRegression                         0.6408                0.6532   \nKNeighborsRegressor                      0.6994                0.7186   \nSVR                                      0.7788                0.7993   \nMLPRegressor                             0.4536                0.4341   \n\n                          Test MSE Test MAE Test RMSE  Test R2  \\\nLGBMRegressor               0.4686   0.4581    0.6845   0.3518   \nDecisionTreeRegressor       0.3726   0.4280    0.6104   0.4846   \nRandomForestRegressor       0.4797   0.4724    0.6926   0.3363   \nGradientBoostingRegressor   0.4397   0.4523    0.6631   0.3917   \nAdaBoostRegressor           0.4525   0.4635    0.6727   0.3740   \nXGBRegressor                0.4246   0.4436    0.6516   0.4126   \nExtraTreesRegressor         0.4679   0.4681    0.6841   0.3526   \nLinearRegression            0.7374   0.6336    0.8587  -0.0201   \nKNeighborsRegressor         0.5544   0.4759    0.7445   0.2331   \nSVR                         0.5048   0.4559    0.7105   0.3017   \nMLPRegressor                0.7306   0.6725    0.8547  -0.0107   \n\n                          Test Pearson Correlation Test Spearman Correlation  \nLGBMRegressor                               0.6830                    0.7358  \nDecisionTreeRegressor                       0.7591                    0.7672  \nRandomForestRegressor                       0.6796                    0.7327  \nGradientBoostingRegressor                   0.7094                    0.7574  \nAdaBoostRegressor                           0.7078                    0.7528  \nXGBRegressor                                0.7042                    0.7666  \nExtraTreesRegressor                         0.6860                    0.7498  \nLinearRegression                            0.4955                    0.5857  \nKNeighborsRegressor                         0.6123                    0.7147  \nSVR                                         0.6426                    0.7597  \nMLPRegressor                                0.5941                    0.6472  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Train MSE (5 fold cv)</th>\n      <th>Train MAE (5 fold cv)</th>\n      <th>Train RMSE (5 fold cv)</th>\n      <th>Train R2 (5 fold cv)</th>\n      <th>Train PCC (5 fold cv)</th>\n      <th>Train SCC (5 fold cv)</th>\n      <th>Test MSE</th>\n      <th>Test MAE</th>\n      <th>Test RMSE</th>\n      <th>Test R2</th>\n      <th>Test Pearson Correlation</th>\n      <th>Test Spearman Correlation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>LGBMRegressor</th>\n      <td>0.1779</td>\n      <td>0.3114</td>\n      <td>0.4218</td>\n      <td>0.6031</td>\n      <td>0.7784</td>\n      <td>0.7841</td>\n      <td>0.4686</td>\n      <td>0.4581</td>\n      <td>0.6845</td>\n      <td>0.3518</td>\n      <td>0.6830</td>\n      <td>0.7358</td>\n    </tr>\n    <tr>\n      <th>DecisionTreeRegressor</th>\n      <td>0.3389</td>\n      <td>0.4221</td>\n      <td>0.5821</td>\n      <td>0.2441</td>\n      <td>0.6152</td>\n      <td>0.6228</td>\n      <td>0.3726</td>\n      <td>0.4280</td>\n      <td>0.6104</td>\n      <td>0.4846</td>\n      <td>0.7591</td>\n      <td>0.7672</td>\n    </tr>\n    <tr>\n      <th>RandomForestRegressor</th>\n      <td>0.1743</td>\n      <td>0.3150</td>\n      <td>0.4175</td>\n      <td>0.6111</td>\n      <td>0.7839</td>\n      <td>0.7974</td>\n      <td>0.4797</td>\n      <td>0.4724</td>\n      <td>0.6926</td>\n      <td>0.3363</td>\n      <td>0.6796</td>\n      <td>0.7327</td>\n    </tr>\n    <tr>\n      <th>GradientBoostingRegressor</th>\n      <td>0.1933</td>\n      <td>0.3294</td>\n      <td>0.4396</td>\n      <td>0.5689</td>\n      <td>0.7552</td>\n      <td>0.7631</td>\n      <td>0.4397</td>\n      <td>0.4523</td>\n      <td>0.6631</td>\n      <td>0.3917</td>\n      <td>0.7094</td>\n      <td>0.7574</td>\n    </tr>\n    <tr>\n      <th>AdaBoostRegressor</th>\n      <td>0.1821</td>\n      <td>0.3304</td>\n      <td>0.4267</td>\n      <td>0.5939</td>\n      <td>0.7716</td>\n      <td>0.7799</td>\n      <td>0.4525</td>\n      <td>0.4635</td>\n      <td>0.6727</td>\n      <td>0.3740</td>\n      <td>0.7078</td>\n      <td>0.7528</td>\n    </tr>\n    <tr>\n      <th>XGBRegressor</th>\n      <td>0.2303</td>\n      <td>0.3541</td>\n      <td>0.4799</td>\n      <td>0.4863</td>\n      <td>0.7047</td>\n      <td>0.7125</td>\n      <td>0.4246</td>\n      <td>0.4436</td>\n      <td>0.6516</td>\n      <td>0.4126</td>\n      <td>0.7042</td>\n      <td>0.7666</td>\n    </tr>\n    <tr>\n      <th>ExtraTreesRegressor</th>\n      <td>0.1628</td>\n      <td>0.3068</td>\n      <td>0.4035</td>\n      <td>0.6369</td>\n      <td>0.7994</td>\n      <td>0.8139</td>\n      <td>0.4679</td>\n      <td>0.4681</td>\n      <td>0.6841</td>\n      <td>0.3526</td>\n      <td>0.6860</td>\n      <td>0.7498</td>\n    </tr>\n    <tr>\n      <th>LinearRegression</th>\n      <td>0.3154</td>\n      <td>0.4361</td>\n      <td>0.5616</td>\n      <td>0.2965</td>\n      <td>0.6408</td>\n      <td>0.6532</td>\n      <td>0.7374</td>\n      <td>0.6336</td>\n      <td>0.8587</td>\n      <td>-0.0201</td>\n      <td>0.4955</td>\n      <td>0.5857</td>\n    </tr>\n    <tr>\n      <th>KNeighborsRegressor</th>\n      <td>0.2394</td>\n      <td>0.3468</td>\n      <td>0.4893</td>\n      <td>0.4659</td>\n      <td>0.6994</td>\n      <td>0.7186</td>\n      <td>0.5544</td>\n      <td>0.4759</td>\n      <td>0.7445</td>\n      <td>0.2331</td>\n      <td>0.6123</td>\n      <td>0.7147</td>\n    </tr>\n    <tr>\n      <th>SVR</th>\n      <td>0.1787</td>\n      <td>0.3133</td>\n      <td>0.4227</td>\n      <td>0.6014</td>\n      <td>0.7788</td>\n      <td>0.7993</td>\n      <td>0.5048</td>\n      <td>0.4559</td>\n      <td>0.7105</td>\n      <td>0.3017</td>\n      <td>0.6426</td>\n      <td>0.7597</td>\n    </tr>\n    <tr>\n      <th>MLPRegressor</th>\n      <td>0.7564</td>\n      <td>0.6614</td>\n      <td>0.8697</td>\n      <td>-0.6873</td>\n      <td>0.4536</td>\n      <td>0.4341</td>\n      <td>0.7306</td>\n      <td>0.6725</td>\n      <td>0.8547</td>\n      <td>-0.0107</td>\n      <td>0.5941</td>\n      <td>0.6472</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":51},{"id":"23e780f0-2276-47b0-ba5d-10ad01a541a5","cell_type":"code","source":"result_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:09:16.561435Z","iopub.execute_input":"2025-07-13T07:09:16.562320Z","iopub.status.idle":"2025-07-13T07:09:16.574007Z","shell.execute_reply.started":"2025-07-13T07:09:16.562295Z","shell.execute_reply":"2025-07-13T07:09:16.573341Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"                          Train MSE (5 fold cv) Train MAE (5 fold cv)  \\\nLGBMRegressor                            0.1779                0.3114   \nDecisionTreeRegressor                    0.3389                0.4221   \nRandomForestRegressor                    0.1743                0.3150   \nGradientBoostingRegressor                0.1933                0.3294   \nAdaBoostRegressor                        0.1821                0.3304   \nXGBRegressor                             0.2303                0.3541   \nExtraTreesRegressor                      0.1628                0.3068   \nLinearRegression                         0.3154                0.4361   \nKNeighborsRegressor                      0.2394                0.3468   \nSVR                                      0.1787                0.3133   \nMLPRegressor                             0.7564                0.6614   \n\n                          Train RMSE (5 fold cv) Train R2 (5 fold cv)  \\\nLGBMRegressor                             0.4218               0.6031   \nDecisionTreeRegressor                     0.5821               0.2441   \nRandomForestRegressor                     0.4175               0.6111   \nGradientBoostingRegressor                 0.4396               0.5689   \nAdaBoostRegressor                         0.4267               0.5939   \nXGBRegressor                              0.4799               0.4863   \nExtraTreesRegressor                       0.4035               0.6369   \nLinearRegression                          0.5616               0.2965   \nKNeighborsRegressor                       0.4893               0.4659   \nSVR                                       0.4227               0.6014   \nMLPRegressor                              0.8697              -0.6873   \n\n                          Train PCC (5 fold cv) Train SCC (5 fold cv)  \\\nLGBMRegressor                            0.7784                0.7841   \nDecisionTreeRegressor                    0.6152                0.6228   \nRandomForestRegressor                    0.7839                0.7974   \nGradientBoostingRegressor                0.7552                0.7631   \nAdaBoostRegressor                        0.7716                0.7799   \nXGBRegressor                             0.7047                0.7125   \nExtraTreesRegressor                      0.7994                0.8139   \nLinearRegression                         0.6408                0.6532   \nKNeighborsRegressor                      0.6994                0.7186   \nSVR                                      0.7788                0.7993   \nMLPRegressor                             0.4536                0.4341   \n\n                          Test MSE Test MAE Test RMSE  Test R2  \\\nLGBMRegressor               0.4686   0.4581    0.6845   0.3518   \nDecisionTreeRegressor       0.3726   0.4280    0.6104   0.4846   \nRandomForestRegressor       0.4797   0.4724    0.6926   0.3363   \nGradientBoostingRegressor   0.4397   0.4523    0.6631   0.3917   \nAdaBoostRegressor           0.4525   0.4635    0.6727   0.3740   \nXGBRegressor                0.4246   0.4436    0.6516   0.4126   \nExtraTreesRegressor         0.4679   0.4681    0.6841   0.3526   \nLinearRegression            0.7374   0.6336    0.8587  -0.0201   \nKNeighborsRegressor         0.5544   0.4759    0.7445   0.2331   \nSVR                         0.5048   0.4559    0.7105   0.3017   \nMLPRegressor                0.7306   0.6725    0.8547  -0.0107   \n\n                          Test Pearson Correlation Test Spearman Correlation  \nLGBMRegressor                               0.6830                    0.7358  \nDecisionTreeRegressor                       0.7591                    0.7672  \nRandomForestRegressor                       0.6796                    0.7327  \nGradientBoostingRegressor                   0.7094                    0.7574  \nAdaBoostRegressor                           0.7078                    0.7528  \nXGBRegressor                                0.7042                    0.7666  \nExtraTreesRegressor                         0.6860                    0.7498  \nLinearRegression                            0.4955                    0.5857  \nKNeighborsRegressor                         0.6123                    0.7147  \nSVR                                         0.6426                    0.7597  \nMLPRegressor                                0.5941                    0.6472  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Train MSE (5 fold cv)</th>\n      <th>Train MAE (5 fold cv)</th>\n      <th>Train RMSE (5 fold cv)</th>\n      <th>Train R2 (5 fold cv)</th>\n      <th>Train PCC (5 fold cv)</th>\n      <th>Train SCC (5 fold cv)</th>\n      <th>Test MSE</th>\n      <th>Test MAE</th>\n      <th>Test RMSE</th>\n      <th>Test R2</th>\n      <th>Test Pearson Correlation</th>\n      <th>Test Spearman Correlation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>LGBMRegressor</th>\n      <td>0.1779</td>\n      <td>0.3114</td>\n      <td>0.4218</td>\n      <td>0.6031</td>\n      <td>0.7784</td>\n      <td>0.7841</td>\n      <td>0.4686</td>\n      <td>0.4581</td>\n      <td>0.6845</td>\n      <td>0.3518</td>\n      <td>0.6830</td>\n      <td>0.7358</td>\n    </tr>\n    <tr>\n      <th>DecisionTreeRegressor</th>\n      <td>0.3389</td>\n      <td>0.4221</td>\n      <td>0.5821</td>\n      <td>0.2441</td>\n      <td>0.6152</td>\n      <td>0.6228</td>\n      <td>0.3726</td>\n      <td>0.4280</td>\n      <td>0.6104</td>\n      <td>0.4846</td>\n      <td>0.7591</td>\n      <td>0.7672</td>\n    </tr>\n    <tr>\n      <th>RandomForestRegressor</th>\n      <td>0.1743</td>\n      <td>0.3150</td>\n      <td>0.4175</td>\n      <td>0.6111</td>\n      <td>0.7839</td>\n      <td>0.7974</td>\n      <td>0.4797</td>\n      <td>0.4724</td>\n      <td>0.6926</td>\n      <td>0.3363</td>\n      <td>0.6796</td>\n      <td>0.7327</td>\n    </tr>\n    <tr>\n      <th>GradientBoostingRegressor</th>\n      <td>0.1933</td>\n      <td>0.3294</td>\n      <td>0.4396</td>\n      <td>0.5689</td>\n      <td>0.7552</td>\n      <td>0.7631</td>\n      <td>0.4397</td>\n      <td>0.4523</td>\n      <td>0.6631</td>\n      <td>0.3917</td>\n      <td>0.7094</td>\n      <td>0.7574</td>\n    </tr>\n    <tr>\n      <th>AdaBoostRegressor</th>\n      <td>0.1821</td>\n      <td>0.3304</td>\n      <td>0.4267</td>\n      <td>0.5939</td>\n      <td>0.7716</td>\n      <td>0.7799</td>\n      <td>0.4525</td>\n      <td>0.4635</td>\n      <td>0.6727</td>\n      <td>0.3740</td>\n      <td>0.7078</td>\n      <td>0.7528</td>\n    </tr>\n    <tr>\n      <th>XGBRegressor</th>\n      <td>0.2303</td>\n      <td>0.3541</td>\n      <td>0.4799</td>\n      <td>0.4863</td>\n      <td>0.7047</td>\n      <td>0.7125</td>\n      <td>0.4246</td>\n      <td>0.4436</td>\n      <td>0.6516</td>\n      <td>0.4126</td>\n      <td>0.7042</td>\n      <td>0.7666</td>\n    </tr>\n    <tr>\n      <th>ExtraTreesRegressor</th>\n      <td>0.1628</td>\n      <td>0.3068</td>\n      <td>0.4035</td>\n      <td>0.6369</td>\n      <td>0.7994</td>\n      <td>0.8139</td>\n      <td>0.4679</td>\n      <td>0.4681</td>\n      <td>0.6841</td>\n      <td>0.3526</td>\n      <td>0.6860</td>\n      <td>0.7498</td>\n    </tr>\n    <tr>\n      <th>LinearRegression</th>\n      <td>0.3154</td>\n      <td>0.4361</td>\n      <td>0.5616</td>\n      <td>0.2965</td>\n      <td>0.6408</td>\n      <td>0.6532</td>\n      <td>0.7374</td>\n      <td>0.6336</td>\n      <td>0.8587</td>\n      <td>-0.0201</td>\n      <td>0.4955</td>\n      <td>0.5857</td>\n    </tr>\n    <tr>\n      <th>KNeighborsRegressor</th>\n      <td>0.2394</td>\n      <td>0.3468</td>\n      <td>0.4893</td>\n      <td>0.4659</td>\n      <td>0.6994</td>\n      <td>0.7186</td>\n      <td>0.5544</td>\n      <td>0.4759</td>\n      <td>0.7445</td>\n      <td>0.2331</td>\n      <td>0.6123</td>\n      <td>0.7147</td>\n    </tr>\n    <tr>\n      <th>SVR</th>\n      <td>0.1787</td>\n      <td>0.3133</td>\n      <td>0.4227</td>\n      <td>0.6014</td>\n      <td>0.7788</td>\n      <td>0.7993</td>\n      <td>0.5048</td>\n      <td>0.4559</td>\n      <td>0.7105</td>\n      <td>0.3017</td>\n      <td>0.6426</td>\n      <td>0.7597</td>\n    </tr>\n    <tr>\n      <th>MLPRegressor</th>\n      <td>0.7564</td>\n      <td>0.6614</td>\n      <td>0.8697</td>\n      <td>-0.6873</td>\n      <td>0.4536</td>\n      <td>0.4341</td>\n      <td>0.7306</td>\n      <td>0.6725</td>\n      <td>0.8547</td>\n      <td>-0.0107</td>\n      <td>0.5941</td>\n      <td>0.6472</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":52},{"id":"70548cc9-30e7-43c9-b52c-dca917d0997e","cell_type":"code","source":"prediction_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:09:19.510183Z","iopub.execute_input":"2025-07-13T07:09:19.510670Z","iopub.status.idle":"2025-07-13T07:09:19.571750Z","shell.execute_reply.started":"2025-07-13T07:09:19.510645Z","shell.execute_reply":"2025-07-13T07:09:19.571176Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"                        Model  \\\n0               LGBMRegressor   \n1       DecisionTreeRegressor   \n2       RandomForestRegressor   \n3   GradientBoostingRegressor   \n4           AdaBoostRegressor   \n5                XGBRegressor   \n6         ExtraTreesRegressor   \n7            LinearRegression   \n8         KNeighborsRegressor   \n9                         SVR   \n10               MLPRegressor   \n\n                                         Y Train pred  \\\n0   [-6.131856730754717, -6.095925465822843, -5.09...   \n1   [-6.1, -6.0, -4.9, -6.0, -5.32, -5.225, -5.21,...   \n2   [-6.132700000000001, -6.2154, -5.2443500000000...   \n3   [-6.18696465286236, -6.058535174005092, -5.144...   \n4   [-6.096714285714286, -6.314285714285716, -5.25...   \n5   [-6.0668097, -6.2447257, -5.0898356, -5.503218...   \n6   [-6.11355, -6.300200000000001, -5.197450000000...   \n7   [-6.389936526473965, -5.6108950689970865, -4.6...   \n8   [-5.983333333333333, -6.19, -5.073333333333333...   \n9   [-6.047331748097223, -5.993975433849565, -5.21...   \n10  [-7.442681385612703, -5.9962582107934566, -4.6...   \n\n                                        Y Test actual  \\\n0   0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...   \n1   0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...   \n2   0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...   \n3   0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...   \n4   0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...   \n5   0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...   \n6   0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...   \n7   0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...   \n8   0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...   \n9   0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...   \n10  0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...   \n\n                                Test prediction folds  \\\n0   [[-6.033667667547009, -6.025059089263897, -5.9...   \n1   [[-6.13, -5.87, -6.0, -5.255, -5.4, -5.15, -6....   \n2   [[-6.101849999999998, -6.072199999999999, -5.9...   \n3   [[-6.093204980075011, -5.956532275052277, -5.7...   \n4   [[-6.062692307692308, -6.02652777777778, -6.06...   \n5   [[-6.0385394, -6.0762153, -5.558069, -5.284430...   \n6   [[-6.160350000000002, -5.958300000000001, -6.1...   \n7   [[-5.858658029986998, -5.859929116788184, -5.0...   \n8   [[-5.983333333333333, -5.983333333333333, -5.6...   \n9   [[-6.001129160369564, -5.970357535452868, -5.7...   \n10  [[-5.5315039339367935, -5.866261590232455, -6....   \n\n                                Test Predictions Mean  \\\n0   [-6.12455799176154, -6.049080747696183, -6.023...   \n1   [-6.33, -5.964, -6.305999999999999, -5.3569999...   \n2   [-6.17883, -6.11134, -6.040540000000003, -5.57...   \n3   [-6.194287543815038, -6.03677837676691, -6.031...   \n4   [-6.156240847264204, -6.127888919399949, -6.11...   \n5   [-6.2569604, -5.991304, -6.09345, -5.510153, -...   \n6   [-6.2032799999999995, -6.009900000000001, -6.0...   \n7   [-5.950801520973468, -5.881382003768335, -5.34...   \n8   [-6.171333333333333, -6.118666666666667, -5.77...   \n9   [-6.155655861341624, -6.023742911280105, -5.78...   \n10  [-5.471691095799962, -5.757570010293771, -6.64...   \n\n                                 Test Predictions Std  \n0   [0.08458366012141755, 0.09828094917691112, 0.0...  \n1   [0.30059940119700856, 0.07472616676907744, 0.2...  \n2   [0.08618099906592067, 0.07834035613909358, 0.0...  \n3   [0.12503854509643222, 0.1071336591043751, 0.16...  \n4   [0.07840913743515762, 0.11598442352981538, 0.1...  \n5   [0.23637684, 0.047515005, 0.3311254, 0.1898408...  \n6   [0.06673719053121793, 0.07271965346452199, 0.0...  \n7   [0.1923862637087018, 0.1483455470078063, 0.401...  \n8   [0.09918781287145202, 0.11400974617208051, 0.1...  \n9   [0.08323368443142613, 0.028384854727813057, 0....  \n10  [0.21441922522345636, 0.26402988823042667, 0.3...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Y Train pred</th>\n      <th>Y Test actual</th>\n      <th>Test prediction folds</th>\n      <th>Test Predictions Mean</th>\n      <th>Test Predictions Std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LGBMRegressor</td>\n      <td>[-6.131856730754717, -6.095925465822843, -5.09...</td>\n      <td>0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...</td>\n      <td>[[-6.033667667547009, -6.025059089263897, -5.9...</td>\n      <td>[-6.12455799176154, -6.049080747696183, -6.023...</td>\n      <td>[0.08458366012141755, 0.09828094917691112, 0.0...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DecisionTreeRegressor</td>\n      <td>[-6.1, -6.0, -4.9, -6.0, -5.32, -5.225, -5.21,...</td>\n      <td>0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...</td>\n      <td>[[-6.13, -5.87, -6.0, -5.255, -5.4, -5.15, -6....</td>\n      <td>[-6.33, -5.964, -6.305999999999999, -5.3569999...</td>\n      <td>[0.30059940119700856, 0.07472616676907744, 0.2...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>RandomForestRegressor</td>\n      <td>[-6.132700000000001, -6.2154, -5.2443500000000...</td>\n      <td>0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...</td>\n      <td>[[-6.101849999999998, -6.072199999999999, -5.9...</td>\n      <td>[-6.17883, -6.11134, -6.040540000000003, -5.57...</td>\n      <td>[0.08618099906592067, 0.07834035613909358, 0.0...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>GradientBoostingRegressor</td>\n      <td>[-6.18696465286236, -6.058535174005092, -5.144...</td>\n      <td>0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...</td>\n      <td>[[-6.093204980075011, -5.956532275052277, -5.7...</td>\n      <td>[-6.194287543815038, -6.03677837676691, -6.031...</td>\n      <td>[0.12503854509643222, 0.1071336591043751, 0.16...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AdaBoostRegressor</td>\n      <td>[-6.096714285714286, -6.314285714285716, -5.25...</td>\n      <td>0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...</td>\n      <td>[[-6.062692307692308, -6.02652777777778, -6.06...</td>\n      <td>[-6.156240847264204, -6.127888919399949, -6.11...</td>\n      <td>[0.07840913743515762, 0.11598442352981538, 0.1...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>XGBRegressor</td>\n      <td>[-6.0668097, -6.2447257, -5.0898356, -5.503218...</td>\n      <td>0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...</td>\n      <td>[[-6.0385394, -6.0762153, -5.558069, -5.284430...</td>\n      <td>[-6.2569604, -5.991304, -6.09345, -5.510153, -...</td>\n      <td>[0.23637684, 0.047515005, 0.3311254, 0.1898408...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ExtraTreesRegressor</td>\n      <td>[-6.11355, -6.300200000000001, -5.197450000000...</td>\n      <td>0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...</td>\n      <td>[[-6.160350000000002, -5.958300000000001, -6.1...</td>\n      <td>[-6.2032799999999995, -6.009900000000001, -6.0...</td>\n      <td>[0.06673719053121793, 0.07271965346452199, 0.0...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>LinearRegression</td>\n      <td>[-6.389936526473965, -5.6108950689970865, -4.6...</td>\n      <td>0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...</td>\n      <td>[[-5.858658029986998, -5.859929116788184, -5.0...</td>\n      <td>[-5.950801520973468, -5.881382003768335, -5.34...</td>\n      <td>[0.1923862637087018, 0.1483455470078063, 0.401...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>KNeighborsRegressor</td>\n      <td>[-5.983333333333333, -6.19, -5.073333333333333...</td>\n      <td>0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...</td>\n      <td>[[-5.983333333333333, -5.983333333333333, -5.6...</td>\n      <td>[-6.171333333333333, -6.118666666666667, -5.77...</td>\n      <td>[0.09918781287145202, 0.11400974617208051, 0.1...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>SVR</td>\n      <td>[-6.047331748097223, -5.993975433849565, -5.21...</td>\n      <td>0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...</td>\n      <td>[[-6.001129160369564, -5.970357535452868, -5.7...</td>\n      <td>[-6.155655861341624, -6.023742911280105, -5.78...</td>\n      <td>[0.08323368443142613, 0.028384854727813057, 0....</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>MLPRegressor</td>\n      <td>[-7.442681385612703, -5.9962582107934566, -4.6...</td>\n      <td>0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...</td>\n      <td>[[-5.5315039339367935, -5.866261590232455, -6....</td>\n      <td>[-5.471691095799962, -5.757570010293771, -6.64...</td>\n      <td>[0.21441922522345636, 0.26402988823042667, 0.3...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":53},{"id":"a0e20820-200e-44a2-a375-5f1b4986a587","cell_type":"code","source":"result_df.to_csv('/kaggle/working/Results_MoLFormer-XL-both-10pct_model_1_fine_tuned_embeddings.csv')\nprediction_df.to_csv('/kaggle/working/Prediction_data_MoLFormer-XL-both-10pct_model_1_fine_tuned_embeddings.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:09:25.895463Z","iopub.execute_input":"2025-07-13T07:09:25.895767Z","iopub.status.idle":"2025-07-13T07:09:25.932363Z","shell.execute_reply.started":"2025-07-13T07:09:25.895746Z","shell.execute_reply":"2025-07-13T07:09:25.931712Z"}},"outputs":[],"execution_count":54},{"id":"3e6925e9-f01c-43fe-bb1f-f1150e01d157","cell_type":"code","source":"result_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:09:28.003355Z","iopub.execute_input":"2025-07-13T07:09:28.003849Z","iopub.status.idle":"2025-07-13T07:09:28.015443Z","shell.execute_reply.started":"2025-07-13T07:09:28.003828Z","shell.execute_reply":"2025-07-13T07:09:28.014830Z"}},"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"                          Train MSE (5 fold cv) Train MAE (5 fold cv)  \\\nLGBMRegressor                            0.1779                0.3114   \nDecisionTreeRegressor                    0.3389                0.4221   \nRandomForestRegressor                    0.1743                0.3150   \nGradientBoostingRegressor                0.1933                0.3294   \nAdaBoostRegressor                        0.1821                0.3304   \nXGBRegressor                             0.2303                0.3541   \nExtraTreesRegressor                      0.1628                0.3068   \nLinearRegression                         0.3154                0.4361   \nKNeighborsRegressor                      0.2394                0.3468   \nSVR                                      0.1787                0.3133   \nMLPRegressor                             0.7564                0.6614   \n\n                          Train RMSE (5 fold cv) Train R2 (5 fold cv)  \\\nLGBMRegressor                             0.4218               0.6031   \nDecisionTreeRegressor                     0.5821               0.2441   \nRandomForestRegressor                     0.4175               0.6111   \nGradientBoostingRegressor                 0.4396               0.5689   \nAdaBoostRegressor                         0.4267               0.5939   \nXGBRegressor                              0.4799               0.4863   \nExtraTreesRegressor                       0.4035               0.6369   \nLinearRegression                          0.5616               0.2965   \nKNeighborsRegressor                       0.4893               0.4659   \nSVR                                       0.4227               0.6014   \nMLPRegressor                              0.8697              -0.6873   \n\n                          Train PCC (5 fold cv) Train SCC (5 fold cv)  \\\nLGBMRegressor                            0.7784                0.7841   \nDecisionTreeRegressor                    0.6152                0.6228   \nRandomForestRegressor                    0.7839                0.7974   \nGradientBoostingRegressor                0.7552                0.7631   \nAdaBoostRegressor                        0.7716                0.7799   \nXGBRegressor                             0.7047                0.7125   \nExtraTreesRegressor                      0.7994                0.8139   \nLinearRegression                         0.6408                0.6532   \nKNeighborsRegressor                      0.6994                0.7186   \nSVR                                      0.7788                0.7993   \nMLPRegressor                             0.4536                0.4341   \n\n                          Test MSE Test MAE Test RMSE  Test R2  \\\nLGBMRegressor               0.4686   0.4581    0.6845   0.3518   \nDecisionTreeRegressor       0.3726   0.4280    0.6104   0.4846   \nRandomForestRegressor       0.4797   0.4724    0.6926   0.3363   \nGradientBoostingRegressor   0.4397   0.4523    0.6631   0.3917   \nAdaBoostRegressor           0.4525   0.4635    0.6727   0.3740   \nXGBRegressor                0.4246   0.4436    0.6516   0.4126   \nExtraTreesRegressor         0.4679   0.4681    0.6841   0.3526   \nLinearRegression            0.7374   0.6336    0.8587  -0.0201   \nKNeighborsRegressor         0.5544   0.4759    0.7445   0.2331   \nSVR                         0.5048   0.4559    0.7105   0.3017   \nMLPRegressor                0.7306   0.6725    0.8547  -0.0107   \n\n                          Test Pearson Correlation Test Spearman Correlation  \nLGBMRegressor                               0.6830                    0.7358  \nDecisionTreeRegressor                       0.7591                    0.7672  \nRandomForestRegressor                       0.6796                    0.7327  \nGradientBoostingRegressor                   0.7094                    0.7574  \nAdaBoostRegressor                           0.7078                    0.7528  \nXGBRegressor                                0.7042                    0.7666  \nExtraTreesRegressor                         0.6860                    0.7498  \nLinearRegression                            0.4955                    0.5857  \nKNeighborsRegressor                         0.6123                    0.7147  \nSVR                                         0.6426                    0.7597  \nMLPRegressor                                0.5941                    0.6472  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Train MSE (5 fold cv)</th>\n      <th>Train MAE (5 fold cv)</th>\n      <th>Train RMSE (5 fold cv)</th>\n      <th>Train R2 (5 fold cv)</th>\n      <th>Train PCC (5 fold cv)</th>\n      <th>Train SCC (5 fold cv)</th>\n      <th>Test MSE</th>\n      <th>Test MAE</th>\n      <th>Test RMSE</th>\n      <th>Test R2</th>\n      <th>Test Pearson Correlation</th>\n      <th>Test Spearman Correlation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>LGBMRegressor</th>\n      <td>0.1779</td>\n      <td>0.3114</td>\n      <td>0.4218</td>\n      <td>0.6031</td>\n      <td>0.7784</td>\n      <td>0.7841</td>\n      <td>0.4686</td>\n      <td>0.4581</td>\n      <td>0.6845</td>\n      <td>0.3518</td>\n      <td>0.6830</td>\n      <td>0.7358</td>\n    </tr>\n    <tr>\n      <th>DecisionTreeRegressor</th>\n      <td>0.3389</td>\n      <td>0.4221</td>\n      <td>0.5821</td>\n      <td>0.2441</td>\n      <td>0.6152</td>\n      <td>0.6228</td>\n      <td>0.3726</td>\n      <td>0.4280</td>\n      <td>0.6104</td>\n      <td>0.4846</td>\n      <td>0.7591</td>\n      <td>0.7672</td>\n    </tr>\n    <tr>\n      <th>RandomForestRegressor</th>\n      <td>0.1743</td>\n      <td>0.3150</td>\n      <td>0.4175</td>\n      <td>0.6111</td>\n      <td>0.7839</td>\n      <td>0.7974</td>\n      <td>0.4797</td>\n      <td>0.4724</td>\n      <td>0.6926</td>\n      <td>0.3363</td>\n      <td>0.6796</td>\n      <td>0.7327</td>\n    </tr>\n    <tr>\n      <th>GradientBoostingRegressor</th>\n      <td>0.1933</td>\n      <td>0.3294</td>\n      <td>0.4396</td>\n      <td>0.5689</td>\n      <td>0.7552</td>\n      <td>0.7631</td>\n      <td>0.4397</td>\n      <td>0.4523</td>\n      <td>0.6631</td>\n      <td>0.3917</td>\n      <td>0.7094</td>\n      <td>0.7574</td>\n    </tr>\n    <tr>\n      <th>AdaBoostRegressor</th>\n      <td>0.1821</td>\n      <td>0.3304</td>\n      <td>0.4267</td>\n      <td>0.5939</td>\n      <td>0.7716</td>\n      <td>0.7799</td>\n      <td>0.4525</td>\n      <td>0.4635</td>\n      <td>0.6727</td>\n      <td>0.3740</td>\n      <td>0.7078</td>\n      <td>0.7528</td>\n    </tr>\n    <tr>\n      <th>XGBRegressor</th>\n      <td>0.2303</td>\n      <td>0.3541</td>\n      <td>0.4799</td>\n      <td>0.4863</td>\n      <td>0.7047</td>\n      <td>0.7125</td>\n      <td>0.4246</td>\n      <td>0.4436</td>\n      <td>0.6516</td>\n      <td>0.4126</td>\n      <td>0.7042</td>\n      <td>0.7666</td>\n    </tr>\n    <tr>\n      <th>ExtraTreesRegressor</th>\n      <td>0.1628</td>\n      <td>0.3068</td>\n      <td>0.4035</td>\n      <td>0.6369</td>\n      <td>0.7994</td>\n      <td>0.8139</td>\n      <td>0.4679</td>\n      <td>0.4681</td>\n      <td>0.6841</td>\n      <td>0.3526</td>\n      <td>0.6860</td>\n      <td>0.7498</td>\n    </tr>\n    <tr>\n      <th>LinearRegression</th>\n      <td>0.3154</td>\n      <td>0.4361</td>\n      <td>0.5616</td>\n      <td>0.2965</td>\n      <td>0.6408</td>\n      <td>0.6532</td>\n      <td>0.7374</td>\n      <td>0.6336</td>\n      <td>0.8587</td>\n      <td>-0.0201</td>\n      <td>0.4955</td>\n      <td>0.5857</td>\n    </tr>\n    <tr>\n      <th>KNeighborsRegressor</th>\n      <td>0.2394</td>\n      <td>0.3468</td>\n      <td>0.4893</td>\n      <td>0.4659</td>\n      <td>0.6994</td>\n      <td>0.7186</td>\n      <td>0.5544</td>\n      <td>0.4759</td>\n      <td>0.7445</td>\n      <td>0.2331</td>\n      <td>0.6123</td>\n      <td>0.7147</td>\n    </tr>\n    <tr>\n      <th>SVR</th>\n      <td>0.1787</td>\n      <td>0.3133</td>\n      <td>0.4227</td>\n      <td>0.6014</td>\n      <td>0.7788</td>\n      <td>0.7993</td>\n      <td>0.5048</td>\n      <td>0.4559</td>\n      <td>0.7105</td>\n      <td>0.3017</td>\n      <td>0.6426</td>\n      <td>0.7597</td>\n    </tr>\n    <tr>\n      <th>MLPRegressor</th>\n      <td>0.7564</td>\n      <td>0.6614</td>\n      <td>0.8697</td>\n      <td>-0.6873</td>\n      <td>0.4536</td>\n      <td>0.4341</td>\n      <td>0.7306</td>\n      <td>0.6725</td>\n      <td>0.8547</td>\n      <td>-0.0107</td>\n      <td>0.5941</td>\n      <td>0.6472</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":55},{"id":"cc4fc0ad-61d4-4662-91aa-543114899f32","cell_type":"code","source":"prediction_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T07:09:44.884596Z","iopub.execute_input":"2025-07-13T07:09:44.885446Z","iopub.status.idle":"2025-07-13T07:09:44.948525Z","shell.execute_reply.started":"2025-07-13T07:09:44.885419Z","shell.execute_reply":"2025-07-13T07:09:44.947843Z"}},"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"                        Model  \\\n0               LGBMRegressor   \n1       DecisionTreeRegressor   \n2       RandomForestRegressor   \n3   GradientBoostingRegressor   \n4           AdaBoostRegressor   \n5                XGBRegressor   \n6         ExtraTreesRegressor   \n7            LinearRegression   \n8         KNeighborsRegressor   \n9                         SVR   \n10               MLPRegressor   \n\n                                         Y Train pred  \\\n0   [-6.131856730754717, -6.095925465822843, -5.09...   \n1   [-6.1, -6.0, -4.9, -6.0, -5.32, -5.225, -5.21,...   \n2   [-6.132700000000001, -6.2154, -5.2443500000000...   \n3   [-6.18696465286236, -6.058535174005092, -5.144...   \n4   [-6.096714285714286, -6.314285714285716, -5.25...   \n5   [-6.0668097, -6.2447257, -5.0898356, -5.503218...   \n6   [-6.11355, -6.300200000000001, -5.197450000000...   \n7   [-6.389936526473965, -5.6108950689970865, -4.6...   \n8   [-5.983333333333333, -6.19, -5.073333333333333...   \n9   [-6.047331748097223, -5.993975433849565, -5.21...   \n10  [-7.442681385612703, -5.9962582107934566, -4.6...   \n\n                                        Y Test actual  \\\n0   0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...   \n1   0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...   \n2   0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...   \n3   0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...   \n4   0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...   \n5   0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...   \n6   0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...   \n7   0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...   \n8   0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...   \n9   0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...   \n10  0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...   \n\n                                Test prediction folds  \\\n0   [[-6.033667667547009, -6.025059089263897, -5.9...   \n1   [[-6.13, -5.87, -6.0, -5.255, -5.4, -5.15, -6....   \n2   [[-6.101849999999998, -6.072199999999999, -5.9...   \n3   [[-6.093204980075011, -5.956532275052277, -5.7...   \n4   [[-6.062692307692308, -6.02652777777778, -6.06...   \n5   [[-6.0385394, -6.0762153, -5.558069, -5.284430...   \n6   [[-6.160350000000002, -5.958300000000001, -6.1...   \n7   [[-5.858658029986998, -5.859929116788184, -5.0...   \n8   [[-5.983333333333333, -5.983333333333333, -5.6...   \n9   [[-6.001129160369564, -5.970357535452868, -5.7...   \n10  [[-5.5315039339367935, -5.866261590232455, -6....   \n\n                                Test Predictions Mean  \\\n0   [-6.12455799176154, -6.049080747696183, -6.023...   \n1   [-6.33, -5.964, -6.305999999999999, -5.3569999...   \n2   [-6.17883, -6.11134, -6.040540000000003, -5.57...   \n3   [-6.194287543815038, -6.03677837676691, -6.031...   \n4   [-6.156240847264204, -6.127888919399949, -6.11...   \n5   [-6.2569604, -5.991304, -6.09345, -5.510153, -...   \n6   [-6.2032799999999995, -6.009900000000001, -6.0...   \n7   [-5.950801520973468, -5.881382003768335, -5.34...   \n8   [-6.171333333333333, -6.118666666666667, -5.77...   \n9   [-6.155655861341624, -6.023742911280105, -5.78...   \n10  [-5.471691095799962, -5.757570010293771, -6.64...   \n\n                                 Test Predictions Std  \n0   [0.08458366012141755, 0.09828094917691112, 0.0...  \n1   [0.30059940119700856, 0.07472616676907744, 0.2...  \n2   [0.08618099906592067, 0.07834035613909358, 0.0...  \n3   [0.12503854509643222, 0.1071336591043751, 0.16...  \n4   [0.07840913743515762, 0.11598442352981538, 0.1...  \n5   [0.23637684, 0.047515005, 0.3311254, 0.1898408...  \n6   [0.06673719053121793, 0.07271965346452199, 0.0...  \n7   [0.1923862637087018, 0.1483455470078063, 0.401...  \n8   [0.09918781287145202, 0.11400974617208051, 0.1...  \n9   [0.08323368443142613, 0.028384854727813057, 0....  \n10  [0.21441922522345636, 0.26402988823042667, 0.3...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Y Train pred</th>\n      <th>Y Test actual</th>\n      <th>Test prediction folds</th>\n      <th>Test Predictions Mean</th>\n      <th>Test Predictions Std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LGBMRegressor</td>\n      <td>[-6.131856730754717, -6.095925465822843, -5.09...</td>\n      <td>0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...</td>\n      <td>[[-6.033667667547009, -6.025059089263897, -5.9...</td>\n      <td>[-6.12455799176154, -6.049080747696183, -6.023...</td>\n      <td>[0.08458366012141755, 0.09828094917691112, 0.0...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DecisionTreeRegressor</td>\n      <td>[-6.1, -6.0, -4.9, -6.0, -5.32, -5.225, -5.21,...</td>\n      <td>0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...</td>\n      <td>[[-6.13, -5.87, -6.0, -5.255, -5.4, -5.15, -6....</td>\n      <td>[-6.33, -5.964, -6.305999999999999, -5.3569999...</td>\n      <td>[0.30059940119700856, 0.07472616676907744, 0.2...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>RandomForestRegressor</td>\n      <td>[-6.132700000000001, -6.2154, -5.2443500000000...</td>\n      <td>0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...</td>\n      <td>[[-6.101849999999998, -6.072199999999999, -5.9...</td>\n      <td>[-6.17883, -6.11134, -6.040540000000003, -5.57...</td>\n      <td>[0.08618099906592067, 0.07834035613909358, 0.0...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>GradientBoostingRegressor</td>\n      <td>[-6.18696465286236, -6.058535174005092, -5.144...</td>\n      <td>0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...</td>\n      <td>[[-6.093204980075011, -5.956532275052277, -5.7...</td>\n      <td>[-6.194287543815038, -6.03677837676691, -6.031...</td>\n      <td>[0.12503854509643222, 0.1071336591043751, 0.16...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AdaBoostRegressor</td>\n      <td>[-6.096714285714286, -6.314285714285716, -5.25...</td>\n      <td>0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...</td>\n      <td>[[-6.062692307692308, -6.02652777777778, -6.06...</td>\n      <td>[-6.156240847264204, -6.127888919399949, -6.11...</td>\n      <td>[0.07840913743515762, 0.11598442352981538, 0.1...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>XGBRegressor</td>\n      <td>[-6.0668097, -6.2447257, -5.0898356, -5.503218...</td>\n      <td>0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...</td>\n      <td>[[-6.0385394, -6.0762153, -5.558069, -5.284430...</td>\n      <td>[-6.2569604, -5.991304, -6.09345, -5.510153, -...</td>\n      <td>[0.23637684, 0.047515005, 0.3311254, 0.1898408...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ExtraTreesRegressor</td>\n      <td>[-6.11355, -6.300200000000001, -5.197450000000...</td>\n      <td>0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...</td>\n      <td>[[-6.160350000000002, -5.958300000000001, -6.1...</td>\n      <td>[-6.2032799999999995, -6.009900000000001, -6.0...</td>\n      <td>[0.06673719053121793, 0.07271965346452199, 0.0...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>LinearRegression</td>\n      <td>[-6.389936526473965, -5.6108950689970865, -4.6...</td>\n      <td>0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...</td>\n      <td>[[-5.858658029986998, -5.859929116788184, -5.0...</td>\n      <td>[-5.950801520973468, -5.881382003768335, -5.34...</td>\n      <td>[0.1923862637087018, 0.1483455470078063, 0.401...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>KNeighborsRegressor</td>\n      <td>[-5.983333333333333, -6.19, -5.073333333333333...</td>\n      <td>0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...</td>\n      <td>[[-5.983333333333333, -5.983333333333333, -5.6...</td>\n      <td>[-6.171333333333333, -6.118666666666667, -5.77...</td>\n      <td>[0.09918781287145202, 0.11400974617208051, 0.1...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>SVR</td>\n      <td>[-6.047331748097223, -5.993975433849565, -5.21...</td>\n      <td>0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...</td>\n      <td>[[-6.001129160369564, -5.970357535452868, -5.7...</td>\n      <td>[-6.155655861341624, -6.023742911280105, -5.78...</td>\n      <td>[0.08323368443142613, 0.028384854727813057, 0....</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>MLPRegressor</td>\n      <td>[-7.442681385612703, -5.9962582107934566, -4.6...</td>\n      <td>0    -6.340\n1    -5.760\n2    -8.000\n3    -6.46...</td>\n      <td>[[-5.5315039339367935, -5.866261590232455, -6....</td>\n      <td>[-5.471691095799962, -5.757570010293771, -6.64...</td>\n      <td>[0.21441922522345636, 0.26402988823042667, 0.3...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":57},{"id":"5297f462-92c4-4e5e-af59-4592bc7b0bfb","cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"id":"86856608-754a-403a-a734-e96ffdf89a0c","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"42b5540e-9ecb-4bf2-a7dc-21dc5c6ce314","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5da82995-273f-4a75-94fc-d3d53cfc3640","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}