{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7fbe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starts from here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression  # LogisticRegression is not used for regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "def remove_low_variance_columns(df, threshold=0.005):\n",
    "    # df = df.drop(['ID','SMILES','Permeability'],axis=1)\n",
    "    variances = df.var()\n",
    "    \n",
    "    # Columns with variance below the threshold\n",
    "    low_variance_columns = variances[variances < threshold].index.tolist()\n",
    "    \n",
    "    df_cleaned = df.drop(columns=low_variance_columns)\n",
    "    \n",
    "    return df_cleaned, low_variance_columns\n",
    "\n",
    "def features(df, target_column='Permeability', threshold=0.9):\n",
    "    correlation_matrix = df.corr()\n",
    "    \n",
    "    features_to_drop = set()\n",
    "    \n",
    "    for feature in correlation_matrix.columns:\n",
    "        if feature == target_column:\n",
    "            continue \n",
    "        target_corr = correlation_matrix[target_column][feature]\n",
    "        \n",
    "        for other_feature in correlation_matrix.columns:\n",
    "            if other_feature == feature or other_feature == target_column:\n",
    "                continue\n",
    "            \n",
    "            if abs(correlation_matrix[feature][other_feature]) > threshold:\n",
    "                other_target_corr = correlation_matrix[target_column][other_feature]\n",
    "\n",
    "                if abs(other_target_corr) < abs(target_corr):\n",
    "                    features_to_drop.add(other_feature)\n",
    "                else:\n",
    "                    features_to_drop.add(feature)\n",
    "    selected_features = [col for col in df.columns if col not in features_to_drop and col != target_column]\n",
    "    \n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3964a1b0-6584-4a55-9ba3-48100a1a9941",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2667770/1337071642.py:2: DtypeWarning: Columns (1275,1277,1280,1285,1292,1298,1304,1354,1356,1359,1364,1371,1377,1383,1579,1580,1581,1583,1584,1590,1595,1596,1597) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_desc_train = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Descriptors/Train_2d_3d_all_descriptors.csv')\n",
      "/tmp/ipykernel_2667770/1337071642.py:10: DtypeWarning: Columns (1275,1277,1292,1298,1354,1356,1371,1377,1579,1580,1581,1583,1584,1595,1596,1597) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_desc_test = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Descriptors/Test_2d_3d_all_descriptors.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Data Loading completed\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "(5568, 271)\n",
      "(1392, 271)\n",
      "(5568, 1128)\n",
      "(1392, 1128)\n",
      "(5568, 758)\n",
      "(1392, 758)\n",
      "(5568, 12)\n",
      "(1392, 12)\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Data Processing completed\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "(5568, 271)\n",
      "(1392, 271)\n",
      "(5568, 1128)\n",
      "(1392, 1128)\n",
      "(5568, 758)\n",
      "(1392, 758)\n",
      "(5568, 12)\n",
      "(1392, 12)\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "4765     3  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.000000   \n",
      "4772     4  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.100000   \n",
      "4766     6  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.300000   \n",
      "4767     7  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.300000   \n",
      "4585    11  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.470000   \n",
      "...    ...                                                ...           ...   \n",
      "607   8512  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](C(...     -6.640000   \n",
      "726   8514  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -5.929053   \n",
      "772   8516  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.199359   \n",
      "5468  8517  CC[C@@H]1c2nnc(o2)[C@H](Cc2ccccc2)NC(=O)CNC(=O...     -6.780000   \n",
      "5397  8518  NCCCC[C@@H]1NC(=O)[C@H](Cc2ccccc2)NC(=O)[C@H](...     -5.505000   \n",
      "\n",
      "      MinEStateIndex       qed       SPS  FpDensityMorgan1  BCUT2D_MWHI  \\\n",
      "4765        0.423845 -0.582130 -0.083445         -0.544377    -0.255732   \n",
      "4772        0.423845 -0.582130 -0.083445         -0.544377    -0.255732   \n",
      "4766        0.423845 -0.582130 -0.083445         -0.544377    -0.255732   \n",
      "4767        0.423845 -0.582130 -0.083445         -0.544377    -0.255732   \n",
      "4585        0.616720 -0.412259 -0.263969         -0.463275    -0.202503   \n",
      "...              ...       ...       ...               ...          ...   \n",
      "607         0.192516 -0.775650  0.259993         -2.407555    -0.272146   \n",
      "726         0.653737 -0.018994  1.601452         -0.588614    -0.273699   \n",
      "772         0.632929 -0.119292  1.086333         -0.700759    -0.273718   \n",
      "5468        1.217004  1.450851 -0.469980          2.939673    -0.227462   \n",
      "5397       -0.865635 -1.498214 -1.692007          0.059878    -0.255008   \n",
      "\n",
      "        AvgIpc  BalabanJ_x  ...      RNCS    LOBMAX    LOBMIN   MOMI-XY  \\\n",
      "4765 -0.023178    0.366371  ...  5.020426 -0.327012 -0.489038  0.266542   \n",
      "4772 -0.023178    0.366371  ...  2.590824  0.131149 -1.095841 -0.505283   \n",
      "4766 -0.023178    0.366371  ...  2.513747  0.238967  0.707090  0.072232   \n",
      "4767 -0.023178    0.366371  ...  2.533854 -0.692595 -0.836966 -0.608377   \n",
      "4585  0.013981    0.379551  ...  1.763717  1.273519  1.549498 -0.585647   \n",
      "...        ...         ...  ...       ...       ...       ...       ...   \n",
      "607  -0.094881    5.092978  ... -0.102534  0.460486  0.914258 -0.137682   \n",
      "726   0.694517   -1.332423  ...  0.870492 -0.662150 -0.976313  0.961296   \n",
      "772   0.638444   -0.952883  ...  0.126184 -0.213779  0.029722  0.975247   \n",
      "5468  0.080729   -0.661118  ... -1.035147  0.502289 -0.168095 -0.134893   \n",
      "5397 -0.635613    0.246641  ...  0.337173  4.135680  2.983486 -2.197778   \n",
      "\n",
      "      geomShape    RDF55p       P1m        Dv       L3p        Ki  \n",
      "4765   0.429391 -1.250047 -0.635846  0.724053 -0.517977 -0.255556  \n",
      "4772   1.091360 -1.030083 -0.203460 -2.226817  0.397570 -1.294787  \n",
      "4766  -0.175018 -1.437993  0.034947  0.462258 -0.716847 -0.078421  \n",
      "4767   0.679871 -0.929081 -0.839721 -1.495106  0.448322 -1.515073  \n",
      "4585   0.440959 -0.686745  0.705614 -1.460487 -0.494421 -0.063346  \n",
      "...         ...       ...       ...       ...       ...       ...  \n",
      "607   -0.023103  1.381124  0.575002  1.270101  0.105768  0.374513  \n",
      "726    0.400530  0.922606 -0.148038 -1.060872 -0.536387  0.423812  \n",
      "772    1.085739 -0.423244 -0.190998 -0.208842 -0.348220  0.370241  \n",
      "5468   1.234705 -2.128040 -0.089959 -0.638086 -0.925282  0.211735  \n",
      "5397   1.055760 -1.314734  1.798569 -1.487228 -0.339001  2.168920  \n",
      "\n",
      "[5568 rows x 271 columns]\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "1192     8  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -7.300000   \n",
      "1145    15  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -6.520000   \n",
      "841     17  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -7.290000   \n",
      "1146    18  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.750000   \n",
      "755     19  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@H]...     -6.950000   \n",
      "...    ...                                                ...           ...   \n",
      "1147  8475  CC(C)C[C@@H]1NC(=O)[C@@H](CC(C)C)N(C)C(=O)[C@H...     -5.960000   \n",
      "174   8483  CC(C)C[C@@H]1NC(=O)[C@H](C)N(C)C(=O)[C@H](CC2C...     -6.009141   \n",
      "1193  8487  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -6.495000   \n",
      "122   8513  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](CC...     -8.200000   \n",
      "201   8515  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.566211   \n",
      "\n",
      "      MinEStateIndex       qed       SPS  FpDensityMorgan1  BCUT2D_MWHI  \\\n",
      "1192        0.423845 -0.582130 -0.083445         -0.544377    -0.255732   \n",
      "1145        0.616720 -0.412259 -0.263969         -0.463275    -0.202503   \n",
      "841         0.523673  0.155095  1.135189         -1.046749    -0.202495   \n",
      "1146        0.616720 -0.412259 -0.263969         -0.463275    -0.202503   \n",
      "755         0.471903 -0.491085  0.431312         -1.132457    -0.202496   \n",
      "...              ...       ...       ...               ...          ...   \n",
      "1147        0.424645 -0.287590 -0.115302         -0.087259    -0.255729   \n",
      "174         0.640728 -0.049837  1.524206         -1.029680    -0.273694   \n",
      "1193        0.423845 -0.582130 -0.083445         -0.544377    -0.255732   \n",
      "122        -0.506947 -1.372119 -0.235774         -2.058103    -0.271840   \n",
      "201         0.638770  0.031128  1.205006         -0.368018    -0.273707   \n",
      "\n",
      "        AvgIpc  BalabanJ_x  ...      RNCS    LOBMAX    LOBMIN   MOMI-XY  \\\n",
      "1192 -0.023178    0.366371  ...  2.574068 -0.821963 -1.083327  1.060758   \n",
      "1145  0.013981    0.379551  ...  1.588178 -0.241147  0.251452 -0.551026   \n",
      "841   0.520753   -0.868326  ...  1.308687 -0.513032 -1.127321  1.085821   \n",
      "1146  0.013981    0.379551  ...  1.841734 -0.458404 -0.736761  0.822209   \n",
      "755   0.390021   -0.316906  ...  1.484576 -0.470343 -0.250467  0.933780   \n",
      "...        ...         ...  ...       ...       ...       ...       ...   \n",
      "1147  0.005182    0.457186  ...  2.394727 -1.346452 -1.111705  0.412881   \n",
      "174   0.702472   -1.307483  ...  1.030806 -0.290446  0.008297  1.548563   \n",
      "1193 -0.023178    0.366371  ...  2.553961 -1.241267 -1.131854  0.930482   \n",
      "122  -2.350518    5.422188  ...  0.719497 -1.136004 -0.674079  0.042385   \n",
      "201   0.631631   -0.948414  ... -0.008013 -1.068484 -1.150767  2.145945   \n",
      "\n",
      "      geomShape    RDF55p       P1m        Dv       L3p        Ki  \n",
      "1192  -0.355074 -1.000562 -0.284243 -0.133650 -1.438494  0.379207  \n",
      "1145   0.020201 -0.966691 -0.230741  0.710936  0.119026 -0.392815  \n",
      "841   -0.858263 -0.400679 -0.134555 -1.026565 -1.335448  0.567891  \n",
      "1146  -0.123442 -0.913022 -0.362147 -1.465924 -0.734346 -0.029197  \n",
      "755   -0.248220 -0.538319 -0.065438 -0.974718 -1.192115  0.484648  \n",
      "...         ...       ...       ...       ...       ...       ...  \n",
      "1147  -0.638297 -0.515289 -1.129021 -0.319862  0.132162 -0.877849  \n",
      "174    0.679744  0.193513 -1.171116  0.890599  0.724314 -0.163931  \n",
      "1193   0.306923 -0.784517 -1.401740 -1.610402 -0.648101 -0.333739  \n",
      "122   -2.669840  1.757360 -0.714835  2.143854  2.270087 -0.896843  \n",
      "201    0.429596 -0.270032 -0.719844 -0.924084 -1.074489  0.707120  \n",
      "\n",
      "[1392 rows x 271 columns]\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "4765     3  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.000000   \n",
      "4772     4  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.100000   \n",
      "4766     6  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.300000   \n",
      "4767     7  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.300000   \n",
      "4585    11  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.470000   \n",
      "...    ...                                                ...           ...   \n",
      "607   8512  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](C(...     -6.640000   \n",
      "726   8514  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -5.929053   \n",
      "772   8516  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.199359   \n",
      "5468  8517  CC[C@@H]1c2nnc(o2)[C@H](Cc2ccccc2)NC(=O)CNC(=O...     -6.780000   \n",
      "5397  8518  NCCCC[C@@H]1NC(=O)[C@H](Cc2ccccc2)NC(=O)[C@H](...     -5.505000   \n",
      "\n",
      "      Morgan_fp_2  Morgan_fp_5  Morgan_fp_13  Morgan_fp_19  Morgan_fp_42  \\\n",
      "4765    -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "4772    -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "4766    -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "4767    -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "4585    -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "...           ...          ...           ...           ...           ...   \n",
      "607     -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "726      2.326961     0.122258     -0.181197     -0.768672     -0.296163   \n",
      "772      2.326961     0.122258     -0.181197     -0.768672     -0.296163   \n",
      "5468    -0.429745     0.122258     -0.181197     -0.768672     -0.296163   \n",
      "5397    -0.429745    -8.179391     -0.181197     -0.768672     -0.296163   \n",
      "\n",
      "      Morgan_fp_84  Morgan_fp_94  ...   SubFPC2  SubFPC100  SubFPC101  \\\n",
      "4765     -0.373919     -0.406921  ... -0.639589   -0.50779  -0.605817   \n",
      "4772     -0.373919     -0.406921  ... -0.639589   -0.50779  -0.605817   \n",
      "4766     -0.373919     -0.406921  ... -0.639589   -0.50779  -0.605817   \n",
      "4767     -0.373919     -0.406921  ... -0.639589   -0.50779  -0.605817   \n",
      "4585     -0.373919     -0.406921  ... -0.639589   -0.50779  -0.605817   \n",
      "...            ...           ...  ...       ...        ...        ...   \n",
      "607      -0.373919     -0.406921  ... -0.639589   -0.50779  -0.605817   \n",
      "726      -0.373919     -0.406921  ...  6.363355   -0.50779  -0.605817   \n",
      "772      -0.373919     -0.406921  ...  4.495903   -0.50779  -0.605817   \n",
      "5468     -0.373919     -0.406921  ... -1.573315   -0.50779  -0.605817   \n",
      "5397     -0.373919     -0.406921  ... -1.106452   -0.50779  -0.605817   \n",
      "\n",
      "      SubFPC127  SubFPC135  SubFPC137  SubFPC143  SubFPC182  SubFPC275  \\\n",
      "4765  -0.208683  -0.060896  -0.040882   -0.05219  -0.093837  -0.943041   \n",
      "4772  -0.208683  -0.060896  -0.040882   -0.05219  -0.093837  -0.943041   \n",
      "4766  -0.208683  -0.060896  -0.040882   -0.05219  -0.093837  -0.943041   \n",
      "4767  -0.208683  -0.060896  -0.040882   -0.05219  -0.093837  -0.943041   \n",
      "4585  -0.208683  -0.060896  -0.040882   -0.05219  -0.093837  -0.943041   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "607    2.434504  -0.060896  -0.040882   -0.05219  -0.093837   1.818822   \n",
      "726   -0.208683  -0.060896  -0.040882   -0.05219  -0.093837   2.371194   \n",
      "772   -0.208683  -0.060896  -0.040882   -0.05219  -0.093837   2.371194   \n",
      "5468  -2.323232  -0.060896  -0.040882   -0.05219   9.581758   0.161704   \n",
      "5397  -0.737320  -0.060896  -0.040882   -0.05219  -0.093837  -1.495414   \n",
      "\n",
      "      SubFPC302  \n",
      "4765  -0.421116  \n",
      "4772  -0.421116  \n",
      "4766  -0.421116  \n",
      "4767  -0.421116  \n",
      "4585  -0.196025  \n",
      "...         ...  \n",
      "607    0.479248  \n",
      "726   -0.646207  \n",
      "772   -0.421116  \n",
      "5468  -1.546572  \n",
      "5397  -0.196025  \n",
      "\n",
      "[5568 rows x 1128 columns]\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "1192     8  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -7.300000   \n",
      "1145    15  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -6.520000   \n",
      "841     17  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -7.290000   \n",
      "1146    18  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.750000   \n",
      "755     19  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@H]...     -6.950000   \n",
      "...    ...                                                ...           ...   \n",
      "1147  8475  CC(C)C[C@@H]1NC(=O)[C@@H](CC(C)C)N(C)C(=O)[C@H...     -5.960000   \n",
      "174   8483  CC(C)C[C@@H]1NC(=O)[C@H](C)N(C)C(=O)[C@H](CC2C...     -6.009141   \n",
      "1193  8487  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -6.495000   \n",
      "122   8513  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](CC...     -8.200000   \n",
      "201   8515  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.566211   \n",
      "\n",
      "      Morgan_fp_2  Morgan_fp_5  Morgan_fp_13  Morgan_fp_19  Morgan_fp_42  \\\n",
      "1192    -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "1145    -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "841     -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "1146    -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "755     -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "...           ...          ...           ...           ...           ...   \n",
      "1147    -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "174      2.326961     0.122258     -0.181197     -0.768672     -0.296163   \n",
      "1193    -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "122     -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "201      2.326961     0.122258     -0.181197     -0.768672     -0.296163   \n",
      "\n",
      "      Morgan_fp_84  Morgan_fp_94  ...   SubFPC2  SubFPC100  SubFPC101  \\\n",
      "1192     -0.373919     -0.406921  ... -0.639589   -0.50779  -0.605817   \n",
      "1145     -0.373919     -0.406921  ... -0.639589   -0.50779  -0.605817   \n",
      "841      -0.373919     -0.406921  ...  0.761000   -0.50779  -0.605817   \n",
      "1146     -0.373919     -0.406921  ... -0.639589   -0.50779  -0.605817   \n",
      "755      -0.373919     -0.406921  ...  0.294137   -0.50779  -0.605817   \n",
      "...            ...           ...  ...       ...        ...        ...   \n",
      "1147     -0.373919     -0.406921  ... -0.639589   -0.50779  -0.605817   \n",
      "174      -0.373919     -0.406921  ...  5.896492   -0.50779  -0.605817   \n",
      "1193     -0.373919     -0.406921  ... -0.639589   -0.50779  -0.605817   \n",
      "122      -0.373919     -0.406921  ...  0.761000   -0.50779  -0.605817   \n",
      "201      -0.373919     -0.406921  ...  4.029040   -0.50779  -0.605817   \n",
      "\n",
      "      SubFPC127  SubFPC135  SubFPC137  SubFPC143  SubFPC182  SubFPC275  \\\n",
      "1192  -0.208683  -0.060896  -0.040882   -0.05219  -0.093837  -0.943041   \n",
      "1145  -0.208683  -0.060896  -0.040882   -0.05219  -0.093837  -0.943041   \n",
      "841    0.319955  -0.060896  -0.040882   -0.05219  -0.093837  -0.390669   \n",
      "1146  -0.208683  -0.060896  -0.040882   -0.05219  -0.093837  -0.943041   \n",
      "755    0.319955  -0.060896  -0.040882   -0.05219  -0.093837  -0.390669   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "1147  -0.208683  -0.060896  -0.040882   -0.05219  -0.093837  -0.943041   \n",
      "174   -0.208683  -0.060896  -0.040882   -0.05219  -0.093837   2.371194   \n",
      "1193  -0.208683  -0.060896  -0.040882   -0.05219  -0.093837  -0.943041   \n",
      "122    2.434504  -0.060896  -0.040882   -0.05219  -0.093837   1.818822   \n",
      "201   -0.208683  -0.060896  -0.040882   -0.05219  -0.093837   2.371194   \n",
      "\n",
      "      SubFPC302  \n",
      "1192  -0.421116  \n",
      "1145  -0.196025  \n",
      "841   -0.646207  \n",
      "1146  -0.196025  \n",
      "755   -0.196025  \n",
      "...         ...  \n",
      "1147  -0.421116  \n",
      "174   -0.646207  \n",
      "1193  -0.421116  \n",
      "122    1.379613  \n",
      "201   -0.646207  \n",
      "\n",
      "[1392 rows x 1128 columns]\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "4765     3  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.000000   \n",
      "4772     4  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.100000   \n",
      "4766     6  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.300000   \n",
      "4767     7  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.300000   \n",
      "4585    11  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.470000   \n",
      "...    ...                                                ...           ...   \n",
      "607   8512  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](C(...     -6.640000   \n",
      "726   8514  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -5.929053   \n",
      "772   8516  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.199359   \n",
      "5468  8517  CC[C@@H]1c2nnc(o2)[C@H](Cc2ccccc2)NC(=O)CNC(=O...     -6.780000   \n",
      "5397  8518  NCCCC[C@@H]1NC(=O)[C@H](Cc2ccccc2)NC(=O)[C@H](...     -5.505000   \n",
      "\n",
      "      x_fine_emb_MFXL0  x_fine_emb_MFXL1  x_fine_emb_MFXL2  x_fine_emb_MFXL3  \\\n",
      "4765         -2.122422          1.147510         -0.627659          1.653762   \n",
      "4772         -3.267542          1.073391         -0.209054          0.443883   \n",
      "4766         -3.002931          1.295640          0.813363          1.314649   \n",
      "4767         -1.842162          1.054307          0.213031          1.793280   \n",
      "4585         -2.106422          1.215424         -3.512944         -0.676041   \n",
      "...                ...               ...               ...               ...   \n",
      "607          -1.851381          0.108503          2.362213         -4.212483   \n",
      "726           1.102033         -0.955575         -1.264599         -2.347400   \n",
      "772           0.750314         -0.668910         -1.050737         -2.285963   \n",
      "5468         -2.560984          1.011480          0.815804         -0.464647   \n",
      "5397         -1.621205          0.616807          0.970138          4.363248   \n",
      "\n",
      "      x_fine_emb_MFXL4  x_fine_emb_MFXL5  x_fine_emb_MFXL6  ...  \\\n",
      "4765          1.654852          2.011274          2.544794  ...   \n",
      "4772          2.186735          1.653986          1.932193  ...   \n",
      "4766          1.170899          1.436321          2.040036  ...   \n",
      "4767          1.289195          2.025044          2.685546  ...   \n",
      "4585          1.867048          1.625098          2.493037  ...   \n",
      "...                ...               ...               ...  ...   \n",
      "607           0.630250         -0.347428         -1.176430  ...   \n",
      "726          -0.851512         -1.684385          1.773667  ...   \n",
      "772          -0.859163         -1.548883          1.573688  ...   \n",
      "5468          1.284211         -0.182989         -0.476694  ...   \n",
      "5397         -2.722272          1.275290          1.091457  ...   \n",
      "\n",
      "      x_fine_emb_MFXL758  x_fine_emb_MFXL759  x_fine_emb_MFXL760  \\\n",
      "4765           -1.071306            1.868809           -1.581889   \n",
      "4772           -0.562022            2.539468           -0.814543   \n",
      "4766           -0.106171            1.776777           -0.572394   \n",
      "4767           -0.940032            1.438628           -1.248522   \n",
      "4585           -2.178598            2.874377           -2.088220   \n",
      "...                  ...                 ...                 ...   \n",
      "607            -0.866846            0.110616           -1.687932   \n",
      "726            -0.487984            0.295598            1.369837   \n",
      "772            -0.306684           -0.797541            1.473635   \n",
      "5468           -1.357991            2.009908           -1.225909   \n",
      "5397            0.495604            1.144872           -0.133799   \n",
      "\n",
      "      x_fine_emb_MFXL761  x_fine_emb_MFXL762  x_fine_emb_MFXL763  \\\n",
      "4765           -0.422563           -1.156847           -1.927335   \n",
      "4772           -0.419681           -0.967847           -3.831952   \n",
      "4766            0.344545           -1.288168           -2.681324   \n",
      "4767           -0.893246           -1.181363           -1.688922   \n",
      "4585           -1.196720           -0.697837           -0.355373   \n",
      "...                  ...                 ...                 ...   \n",
      "607            -2.731316            2.905098           -1.904500   \n",
      "726            -2.069568            0.573895            0.598811   \n",
      "772            -1.459228            0.596947            0.356315   \n",
      "5468            0.897100            1.963339           -0.680554   \n",
      "5397           -0.348794            0.415931           -2.701041   \n",
      "\n",
      "      x_fine_emb_MFXL764  x_fine_emb_MFXL765  x_fine_emb_MFXL766  \\\n",
      "4765            1.534425            2.238415            0.542137   \n",
      "4772            2.702819            2.253270           -0.404184   \n",
      "4766            1.699027            2.039126           -0.496907   \n",
      "4767            1.409813            2.139524            0.642545   \n",
      "4585            0.870366            0.745226           -0.501020   \n",
      "...                  ...                 ...                 ...   \n",
      "607             2.729993           -1.557689           -1.922636   \n",
      "726             1.522412           -1.513424           -0.375645   \n",
      "772             0.819689           -1.523811           -0.417917   \n",
      "5468            1.353444            0.638834           -0.991576   \n",
      "5397            1.946350            1.750392           -1.340038   \n",
      "\n",
      "      x_fine_emb_MFXL767  \n",
      "4765            2.905066  \n",
      "4772            3.304215  \n",
      "4766            2.924381  \n",
      "4767            2.537911  \n",
      "4585            1.531339  \n",
      "...                  ...  \n",
      "607             0.536152  \n",
      "726            -0.970615  \n",
      "772            -1.297706  \n",
      "5468            1.648650  \n",
      "5397            1.870947  \n",
      "\n",
      "[5568 rows x 758 columns]\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "1192     8  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -7.300000   \n",
      "1145    15  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -6.520000   \n",
      "841     17  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -7.290000   \n",
      "1146    18  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.750000   \n",
      "755     19  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@H]...     -6.950000   \n",
      "...    ...                                                ...           ...   \n",
      "1147  8475  CC(C)C[C@@H]1NC(=O)[C@@H](CC(C)C)N(C)C(=O)[C@H...     -5.960000   \n",
      "174   8483  CC(C)C[C@@H]1NC(=O)[C@H](C)N(C)C(=O)[C@H](CC2C...     -6.009141   \n",
      "1193  8487  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -6.495000   \n",
      "122   8513  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](CC...     -8.200000   \n",
      "201   8515  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.566211   \n",
      "\n",
      "      x_fine_emb_MFXL0  x_fine_emb_MFXL1  x_fine_emb_MFXL2  x_fine_emb_MFXL3  \\\n",
      "1192         -2.343506          1.139884         -1.019681          1.482168   \n",
      "1145         -1.874322          1.283046         -3.320531         -0.724183   \n",
      "841          -2.580788          1.081337         -3.492761         -0.422667   \n",
      "1146         -2.402330          0.916008         -4.223656         -0.389233   \n",
      "755          -2.332136          1.063892         -3.923251         -0.003484   \n",
      "...                ...               ...               ...               ...   \n",
      "1147         -0.684747          0.270972          0.323990          0.168266   \n",
      "174          -0.621277          0.438098         -0.102045         -1.783234   \n",
      "1193         -2.243296          1.192508         -0.978036          1.460417   \n",
      "122          -3.886704         -0.985142          1.869315         -3.656563   \n",
      "201           0.782988         -0.818559         -0.928657         -2.282151   \n",
      "\n",
      "      x_fine_emb_MFXL4  x_fine_emb_MFXL5  x_fine_emb_MFXL6  ...  \\\n",
      "1192          1.767813          1.884289          2.501027  ...   \n",
      "1145          1.806769          1.556525          2.613990  ...   \n",
      "841           1.857349          1.757706          2.207338  ...   \n",
      "1146          1.760759          1.042106          2.107901  ...   \n",
      "755           2.067028          1.239307          2.380293  ...   \n",
      "...                ...               ...               ...  ...   \n",
      "1147          0.256428          1.132567          1.891585  ...   \n",
      "174           0.879148          0.362656          1.511093  ...   \n",
      "1193          1.870237          1.965938          2.625938  ...   \n",
      "122           0.318389         -0.590369         -1.679794  ...   \n",
      "201          -0.831194         -1.527292          1.355936  ...   \n",
      "\n",
      "      x_fine_emb_MFXL758  x_fine_emb_MFXL759  x_fine_emb_MFXL760  \\\n",
      "1192           -1.117933            2.174261           -1.373366   \n",
      "1145           -2.224494            2.834726           -1.993815   \n",
      "841            -3.379367            3.047782           -1.733128   \n",
      "1146           -2.551050            2.637986           -1.633309   \n",
      "755            -3.372566            2.768433           -1.676417   \n",
      "...                  ...                 ...                 ...   \n",
      "1147           -0.321255            2.172985           -0.011186   \n",
      "174            -0.204461            1.177312            0.095418   \n",
      "1193           -1.284695            1.836633           -1.545386   \n",
      "122            -0.692843            1.338598           -1.659924   \n",
      "201            -0.022711           -0.817836            1.759791   \n",
      "\n",
      "      x_fine_emb_MFXL761  x_fine_emb_MFXL762  x_fine_emb_MFXL763  \\\n",
      "1192           -0.340619           -1.014842           -1.886554   \n",
      "1145           -1.089232           -0.754110           -0.065039   \n",
      "841            -0.439290           -2.217576           -0.547052   \n",
      "1146           -1.440142           -1.730826           -0.742176   \n",
      "755            -0.901384           -2.095378           -0.827348   \n",
      "...                  ...                 ...                 ...   \n",
      "1147            0.482989           -0.647298           -2.232294   \n",
      "174            -4.315770           -1.146564           -0.357660   \n",
      "1193           -0.355382           -1.261443           -1.803061   \n",
      "122             0.508099            1.404201           -4.118768   \n",
      "201            -1.093112            0.860230            0.224163   \n",
      "\n",
      "      x_fine_emb_MFXL764  x_fine_emb_MFXL765  x_fine_emb_MFXL766  \\\n",
      "1192            1.402676            2.352400            0.282934   \n",
      "1145            0.789885            0.639027           -0.544002   \n",
      "841             2.665555            0.737522           -1.150064   \n",
      "1146            1.311888            0.362385           -0.549036   \n",
      "755             2.044574            0.918320           -1.139046   \n",
      "...                  ...                 ...                 ...   \n",
      "1147            1.282146            1.606716            1.467822   \n",
      "174             2.761389           -0.447501            1.219025   \n",
      "1193            1.555169            2.209891            0.572275   \n",
      "122             5.337712            0.294290            2.863697   \n",
      "201             1.027944           -1.467143           -0.770435   \n",
      "\n",
      "      x_fine_emb_MFXL767  \n",
      "1192            3.243281  \n",
      "1145            1.456025  \n",
      "841             2.119310  \n",
      "1146            1.921803  \n",
      "755             1.936370  \n",
      "...                  ...  \n",
      "1147            1.972242  \n",
      "174            -0.154299  \n",
      "1193            3.061655  \n",
      "122             4.873982  \n",
      "201            -1.373395  \n",
      "\n",
      "[1392 rows x 758 columns]\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "4765     3  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.000000   \n",
      "4772     4  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.100000   \n",
      "4766     6  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.300000   \n",
      "4767     7  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.300000   \n",
      "4585    11  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.470000   \n",
      "...    ...                                                ...           ...   \n",
      "607   8512  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](C(...     -6.640000   \n",
      "726   8514  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -5.929053   \n",
      "772   8516  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.199359   \n",
      "5468  8517  CC[C@@H]1c2nnc(o2)[C@H](Cc2ccccc2)NC(=O)CNC(=O...     -6.780000   \n",
      "5397  8518  NCCCC[C@@H]1NC(=O)[C@H](Cc2ccccc2)NC(=O)[C@H](...     -5.505000   \n",
      "\n",
      "      Degree_Cl   Degree_N  Degree_C  Degree_F  Degree_S  Degree_O  Aromatic  \\\n",
      "4765  -0.102598   0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -0.259371   \n",
      "4772  -0.102598   0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -0.259371   \n",
      "4766  -0.102598   0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -0.259371   \n",
      "4767  -0.102598   0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -0.259371   \n",
      "4585  -0.102598   0.091271 -0.173109 -0.102598 -0.211545  1.359442 -0.259371   \n",
      "...         ...        ...       ...       ...       ...       ...       ...   \n",
      "607   -0.102598   0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -1.193911   \n",
      "726   -0.102598   0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -0.415128   \n",
      "772   -0.102598   0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -0.415128   \n",
      "5468  -0.102598   0.091271 -0.173109 -0.102598 -0.211545  1.359442  0.519413   \n",
      "5397  -0.102598 -10.956435 -0.173109 -0.102598 -0.211545 -0.735596  0.675170   \n",
      "\n",
      "      Overall_Formal_Charge  Is_Aromatic  \n",
      "4765              -0.850527     0.640295  \n",
      "4772              -0.850527     0.640295  \n",
      "4766              -0.850527     0.640295  \n",
      "4767              -0.850527     0.640295  \n",
      "4585              -0.850527     0.640295  \n",
      "...                     ...          ...  \n",
      "607                0.729383    -1.561781  \n",
      "726                0.357640     0.640295  \n",
      "772                0.357640     0.640295  \n",
      "5468              -1.222271     0.640295  \n",
      "5397              -0.339380     0.640295  \n",
      "\n",
      "[5568 rows x 12 columns]\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "1192     8  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -7.300000   \n",
      "1145    15  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -6.520000   \n",
      "841     17  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -7.290000   \n",
      "1146    18  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.750000   \n",
      "755     19  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@H]...     -6.950000   \n",
      "...    ...                                                ...           ...   \n",
      "1147  8475  CC(C)C[C@@H]1NC(=O)[C@@H](CC(C)C)N(C)C(=O)[C@H...     -5.960000   \n",
      "174   8483  CC(C)C[C@@H]1NC(=O)[C@H](C)N(C)C(=O)[C@H](CC2C...     -6.009141   \n",
      "1193  8487  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -6.495000   \n",
      "122   8513  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](CC...     -8.200000   \n",
      "201   8515  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.566211   \n",
      "\n",
      "      Degree_Cl  Degree_N  Degree_C  Degree_F  Degree_S  Degree_O  Aromatic  \\\n",
      "1192  -0.102598  0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -0.259371   \n",
      "1145  -0.102598  0.091271 -0.173109 -0.102598 -0.211545  1.359442 -0.259371   \n",
      "841   -0.102598  0.091271 -0.173109 -0.102598 -0.211545  1.359442 -0.259371   \n",
      "1146  -0.102598  0.091271 -0.173109 -0.102598 -0.211545  1.359442 -0.259371   \n",
      "755   -0.102598  0.091271 -0.173109 -0.102598 -0.211545  1.359442 -0.259371   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "1147  -0.102598  0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -0.259371   \n",
      "174   -0.102598  0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -0.415128   \n",
      "1193  -0.102598  0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -0.259371   \n",
      "122   -0.102598  0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -1.193911   \n",
      "201   -0.102598  0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -0.415128   \n",
      "\n",
      "      Overall_Formal_Charge  Is_Aromatic  \n",
      "1192              -0.850527     0.640295  \n",
      "1145              -0.850527     0.640295  \n",
      "841               -0.432315     0.640295  \n",
      "1146              -0.850527     0.640295  \n",
      "755               -0.432315     0.640295  \n",
      "...                     ...          ...  \n",
      "1147              -0.850527     0.640295  \n",
      "174                0.357640     0.640295  \n",
      "1193              -0.850527     0.640295  \n",
      "122                1.101127    -1.561781  \n",
      "201                0.357640     0.640295  \n",
      "\n",
      "[1392 rows x 12 columns]\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    }
   ],
   "source": [
    "# 2D and 3D descriptors dataframes\n",
    "df_desc_train = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Descriptors/Train_2d_3d_all_descriptors.csv')\n",
    "df_train = df_desc_train.sort_values(by='ID')\n",
    "df_train =df_train.dropna()\n",
    "train = df_train.drop(['ID','SMILES'],axis=1)\n",
    "train = train.select_dtypes(include=['number'])\n",
    "train, _ = remove_low_variance_columns(train)\n",
    "selected_features = features(train, \"Permeability\")\n",
    "df_desc_train = pd.concat( [df_train[['ID','SMILES','Permeability']],df_train[selected_features] ], axis=1)\n",
    "df_desc_test = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Descriptors/Test_2d_3d_all_descriptors.csv')\n",
    "df_desc_test = df_desc_test.sort_values(by='ID')\n",
    "df_desc_test =df_desc_test.dropna()\n",
    "df_desc_test =  df_desc_test[df_desc_train.columns]\n",
    "\n",
    "# Fingerprints\n",
    "df_fp_train = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Fingerprints/Train/All_fingerprints_train.csv')\n",
    "df_train = df_fp_train.sort_values(by='ID')\n",
    "df_train = df_train.dropna()\n",
    "train = df_train.drop(['ID','SMILES'],axis=1)\n",
    "train = train.select_dtypes(include=['number'])\n",
    "train, _ = remove_low_variance_columns(train)\n",
    "selected_features = features(train, \"Permeability\")\n",
    "df_fp_train = pd.concat( [df_train[['ID','SMILES','Permeability']],df_train[selected_features] ], axis=1)\n",
    "df_fp_test = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Fingerprints/Test/All_fingerprints_test.csv')\n",
    "df_fp_test = df_fp_test.sort_values(by='ID')\n",
    "df_fp_test = df_fp_test.dropna()\n",
    "df_fp_test =  df_fp_test[df_fp_train.columns]\n",
    "\n",
    "\n",
    "#Smiles Embeddings\n",
    "df_emb_train = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Embeddings/Train_MoLFormer-XL-both-10pct_model_1_fine_tuned_embeddings.csv')\n",
    "df_train = df_emb_train.sort_values(by='ID')\n",
    "df_train = df_train.dropna()\n",
    "train = df_train.drop(['ID','SMILES'],axis=1)\n",
    "train = train.select_dtypes(include=['number'])\n",
    "train, _ = remove_low_variance_columns(train)\n",
    "selected_features = features(train, \"Permeability\")\n",
    "df_emb_train = pd.concat( [df_train[['ID','SMILES','Permeability']],df_train[selected_features] ], axis=1)\n",
    "df_emb_test = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Embeddings/Test_MoLFormer-XL-both-10pct_model_1_fine_tuned_embeddings.csv')\n",
    "df_emb_test = df_emb_test.sort_values(by='ID')\n",
    "df_emb_test = df_emb_test.dropna()\n",
    "df_emb_test =  df_emb_test[df_emb_train.columns]\n",
    "\n",
    "#ATomic features\n",
    "df_atomic_train = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Atomic/Train_all_atomic_desc.csv')\n",
    "df_train = df_atomic_train.sort_values(by='ID')\n",
    "df_train = df_train.dropna()\n",
    "train = df_train.drop(['ID','SMILES'],axis=1)\n",
    "train = train.select_dtypes(include=['number'])\n",
    "train, _ = remove_low_variance_columns(train)\n",
    "selected_features = features(train, \"Permeability\")\n",
    "df_atomic_train = pd.concat( [df_train[['ID','SMILES','Permeability']],df_train[selected_features] ], axis=1)\n",
    "# df_atomic_train =pd.concat( [df_train['SMILES'], df_train.select_dtypes(include=['number'])], axis=1)\n",
    "df_atomic_test = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Atomic/Test_all_atomic_desc.csv')\n",
    "df_atomic_test = df_atomic_test.sort_values(by='ID')\n",
    "df_atomic_test = df_atomic_test.dropna()\n",
    "df_atomic_test =  df_atomic_test[df_atomic_train.columns]\n",
    "\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Data Loading completed')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "df_fp_test = df_fp_test[df_fp_test['ID'].isin(df_desc_test['ID'])]\n",
    "df_fp_train = df_fp_train[df_fp_train['ID'].isin(df_desc_train['ID'])]\n",
    "\n",
    "df_emb_test = df_emb_test[df_emb_test['ID'].isin(df_desc_test['ID'])]\n",
    "df_emb_train = df_emb_train[df_emb_train['ID'].isin(df_desc_train['ID'])]\n",
    "\n",
    "df_atomic_test = df_atomic_test[df_atomic_test['ID'].isin(df_desc_test['ID'])]\n",
    "df_atomic_train = df_atomic_train[df_atomic_train['ID'].isin(df_desc_train['ID'])]\n",
    "\n",
    "print(df_desc_train.shape)\n",
    "print(df_desc_test.shape)\n",
    "print(df_fp_train.shape)\n",
    "print(df_fp_test.shape)\n",
    "print(df_emb_train.shape)\n",
    "print(df_emb_test.shape)\n",
    "print(df_atomic_train.shape)\n",
    "print(df_atomic_test.shape)\n",
    "\n",
    "def scale_features(df_train, df_test):\n",
    "    scaler = StandardScaler()\n",
    "    train_features = df_train.drop(columns=['ID', 'SMILES', target_column])\n",
    "    test_features = df_test.drop(columns=['ID', 'SMILES', target_column])\n",
    "    scaler.fit(train_features)\n",
    "    train_scaled = pd.DataFrame(scaler.transform(train_features), columns=train_features.columns, index=df_train.index)\n",
    "    test_scaled = pd.DataFrame(scaler.transform(test_features), columns=test_features.columns, index=df_test.index)\n",
    "    df_train_scaled = pd.concat([df_train[['ID', 'SMILES', target_column]], train_scaled], axis=1)\n",
    "    df_test_scaled = pd.concat([df_test[['ID', 'SMILES', target_column]], test_scaled], axis=1)\n",
    "    return df_train_scaled, df_test_scaled\n",
    "\n",
    "target_column = 'Permeability'\n",
    "df_desc_train, df_desc_test = scale_features(df_desc_train, df_desc_test)\n",
    "df_fp_train, df_fp_test = scale_features(df_fp_train, df_fp_test)\n",
    "df_emb_train, df_emb_test = scale_features(df_emb_train, df_emb_test)\n",
    "df_atomic_train, df_atomic_test = scale_features(df_atomic_train, df_atomic_test)\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Data Processing completed')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print(df_desc_train.shape)\n",
    "print(df_desc_test.shape)\n",
    "print(df_fp_train.shape)\n",
    "print(df_fp_test.shape)\n",
    "print(df_emb_train.shape)\n",
    "print(df_emb_test.shape)\n",
    "print(df_atomic_train.shape)\n",
    "print(df_atomic_test.shape)\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print(df_desc_train)\n",
    "print(df_desc_test)\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print(df_fp_train)\n",
    "print(df_fp_test)\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print(df_emb_train)\n",
    "print(df_emb_test)\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print(df_atomic_train)\n",
    "print(df_atomic_test)\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9289416",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Stage 1: Training weak learners with 15-fold CV\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataframe pairs:   0%|                                                                 | 0/4 [00:00<?, ?it/s]\n",
      "Training models:   0%|                                                                           | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56930\n",
      "[LightGBM] [Info] Number of data points in the train set: 5196, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.747798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007088 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56947\n",
      "[LightGBM] [Info] Number of data points in the train set: 5196, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.741033\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007219 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56944\n",
      "[LightGBM] [Info] Number of data points in the train set: 5196, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.743723\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56952\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.742838\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007620 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56950\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.744857\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007325 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56945\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.744324\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007404 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56950\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.738795\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008526 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56938\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.742697\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008436 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56952\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.746284\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007785 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56941\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.747981\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007373 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56951\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.744388\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56940\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.744043\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56940\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.738129\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009262 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56948\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.742162\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56946\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.734536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  10%|██████▋                                                            | 1/10 [00:11<01:39, 11.11s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  20%|█████████████▍                                                     | 2/10 [00:33<02:20, 17.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  30%|███████████████████▊                                              | 3/10 [06:59<21:42, 186.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  40%|██████████████████████████▍                                       | 4/10 [08:55<15:50, 158.34s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  50%|█████████████████████████████████                                 | 5/10 [09:06<08:46, 105.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  60%|████████████████████████████████████████▏                          | 6/10 [09:14<04:48, 72.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  70%|██████████████████████████████████████████████▉                    | 7/10 [09:16<02:27, 49.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  80%|█████████████████████████████████████████████████████▌             | 8/10 [10:01<01:35, 47.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  90%|████████████████████████████████████████████████████████████▎      | 9/10 [10:45<00:46, 46.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models: 100%|██████████████████████████████████████████████████████████████████| 10/10 [11:02<00:00, 66.25s/it]\u001b[A\n",
      "Processing dataframe pairs:  25%|██████████████                                          | 1/4 [11:02<33:07, 662.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:   0%|                                                                           | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041929 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4201\n",
      "[LightGBM] [Info] Number of data points in the train set: 5196, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.747798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041894 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4212\n",
      "[LightGBM] [Info] Number of data points in the train set: 5196, number of used features: 1107\n",
      "[LightGBM] [Info] Start training from score -5.741033\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041869 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4209\n",
      "[LightGBM] [Info] Number of data points in the train set: 5196, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.743723\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040729 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4211\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 1107\n",
      "[LightGBM] [Info] Start training from score -5.742838\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041960 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4207\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.744857\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041650 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4194\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 1104\n",
      "[LightGBM] [Info] Start training from score -5.744324\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059933 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4203\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.738795\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042365 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4205\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.742697\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041618 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4202\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.746284\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041210 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4208\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.747981\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041141 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4204\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.744388\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040645 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4207\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.744043\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041647 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4212\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 1107\n",
      "[LightGBM] [Info] Start training from score -5.738129\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040691 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4193\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 1104\n",
      "[LightGBM] [Info] Start training from score -5.742162\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040981 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4204\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 1107\n",
      "[LightGBM] [Info] Start training from score -5.734536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  10%|██████▋                                                            | 1/10 [00:16<02:27, 16.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  20%|█████████████▍                                                     | 2/10 [00:25<01:38, 12.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  30%|████████████████████                                               | 3/10 [02:14<06:35, 56.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  40%|██████████████████████████▊                                        | 4/10 [03:42<06:53, 68.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  50%|█████████████████████████████████▌                                 | 5/10 [03:54<04:01, 48.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  60%|████████████████████████████████████████▏                          | 6/10 [04:03<02:19, 34.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  70%|██████████████████████████████████████████████▉                    | 7/10 [04:05<01:12, 24.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  80%|█████████████████████████████████████████████████████▌             | 8/10 [06:37<02:10, 65.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  90%|████████████████████████████████████████████████████████████▎      | 9/10 [07:42<01:04, 64.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models: 100%|██████████████████████████████████████████████████████████████████| 10/10 [07:47<00:00, 46.74s/it]\u001b[A\n",
      "Processing dataframe pairs:  50%|████████████████████████████                            | 2/4 [18:49<18:15, 547.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:   0%|                                                                           | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5196, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.747798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017100 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5196, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.741033\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016983 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5196, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.743723\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.742838\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.744857\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017255 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.744324\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.738795\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017027 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.742697\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016955 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.746284\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017113 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.747981\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.744388\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017168 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.744043\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.738129\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017327 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.742162\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017292 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.734536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  10%|██████▋                                                            | 1/10 [00:11<01:43, 11.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  20%|█████████████▍                                                     | 2/10 [01:41<07:42, 57.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  30%|██████████████████▉                                            | 3/10 [38:11<2:00:17, 1031.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  40%|█████████████████████████▌                                      | 4/10 [46:17<1:21:36, 816.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  50%|█████████████████████████████████                                 | 5/10 [46:53<44:33, 534.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  60%|███████████████████████████████████████▌                          | 6/10 [47:13<23:58, 359.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  70%|██████████████████████████████████████████████▏                   | 7/10 [47:15<12:08, 242.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  80%|████████████████████████████████████████████████████▊             | 8/10 [48:51<06:32, 196.11s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  90%|███████████████████████████████████████████████████████████▍      | 9/10 [50:13<02:40, 160.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models: 100%|█████████████████████████████████████████████████████████████████| 10/10 [51:38<00:00, 309.86s/it]\u001b[A\n",
      "Processing dataframe pairs:  75%|███████████████████████████████████████▊             | 3/4 [1:10:28<28:32, 1712.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:   0%|                                                                           | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.105328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5196, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.747798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5196, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.741033\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000765 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5196, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.743723\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001080 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.742838\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000735 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.744857\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 151\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.744324\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000741 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.738795\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000738 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.742697\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000787 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.746284\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000737 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.747981\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000743 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.744388\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000744 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.744043\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.738129\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.742162\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.734536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  10%|██████▋                                                            | 1/10 [00:07<01:08,  7.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  20%|█████████████▍                                                     | 2/10 [00:13<00:51,  6.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  30%|████████████████████                                               | 3/10 [00:15<00:31,  4.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  40%|██████████████████████████▊                                        | 4/10 [00:16<00:18,  3.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  50%|█████████████████████████████████▌                                 | 5/10 [00:19<00:14,  2.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  60%|████████████████████████████████████████▏                          | 6/10 [00:23<00:13,  3.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  70%|██████████████████████████████████████████████▉                    | 7/10 [00:24<00:07,  2.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  80%|█████████████████████████████████████████████████████▌             | 8/10 [00:36<00:11,  5.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models: 100%|██████████████████████████████████████████████████████████████████| 10/10 [00:59<00:00,  5.99s/it]\u001b[A\n",
      "Processing dataframe pairs: 100%|█████████████████████████████████████████████████████| 4/4 [1:11:28<00:00, 1072.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8\n",
      "Model training done 9\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dimensions of meta_features_train: (5568, 40)\n",
      "Dimensions of meta_features_test: (1392, 40)\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Stage 1 completed\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Stage 2: Training meta-learners\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063239 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10137\n",
      "[LightGBM] [Info] Number of data points in the train set: 5196, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.747798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001473 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10135\n",
      "[LightGBM] [Info] Number of data points in the train set: 5196, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.741033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001656 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10136\n",
      "[LightGBM] [Info] Number of data points in the train set: 5196, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.743723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001678 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10137\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.742838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001650 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10140\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.744857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001577 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10138\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.744324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001472 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10137\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.738795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001646 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10136\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.742697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001611 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10139\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.746284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001630 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10143\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.747981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001642 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10134\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.744388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002522 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10139\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.744043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001648 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10142\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.738129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001661 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10132\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.742162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001674 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10139\n",
      "[LightGBM] [Info] Number of data points in the train set: 5197, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.734536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor Evaluation completed: Test R2 score: 0.6957, Test RMSE: 0.4388\n",
      "DecisionTreeRegressor Evaluation completed: Test R2 score: 0.6716, Test RMSE: 0.4558\n",
      "RandomForestRegressor Evaluation completed: Test R2 score: 0.6930, Test RMSE: 0.4408\n",
      "GradientBoostingRegressor Evaluation completed: Test R2 score: 0.6963, Test RMSE: 0.4384\n",
      "AdaBoostRegressor Evaluation completed: Test R2 score: 0.6515, Test RMSE: 0.4696\n",
      "XGBRegressor Evaluation completed: Test R2 score: 0.6944, Test RMSE: 0.4398\n",
      "ExtraTreesRegressor Evaluation completed: Test R2 score: 0.6930, Test RMSE: 0.4408\n",
      "LinearRegression Evaluation completed: Test R2 score: 0.6942, Test RMSE: 0.4399\n",
      "KNeighborsRegressor Evaluation completed: Test R2 score: 0.6799, Test RMSE: 0.4501\n",
      "SVR Evaluation completed: Test R2 score: 0.6939, Test RMSE: 0.4401\n",
      "MLPRegressor Evaluation completed: Test R2 score: 0.6964, Test RMSE: 0.4383\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Stacked Ensemble Training and Evaluation complete\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train MSE (15 fold CV)</th>\n",
       "      <th>Train MAE (15 fold CV)</th>\n",
       "      <th>Train RMSE (15 fold CV)</th>\n",
       "      <th>Train R2 (15 fold CV)</th>\n",
       "      <th>Train PCC (15 fold CV)</th>\n",
       "      <th>Train SCC (15 fold CV)</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Test MAE</th>\n",
       "      <th>Test RMSE</th>\n",
       "      <th>Test R2</th>\n",
       "      <th>Test PCC</th>\n",
       "      <th>Test SCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <td>0.124803</td>\n",
       "      <td>0.256939</td>\n",
       "      <td>0.353275</td>\n",
       "      <td>0.799612</td>\n",
       "      <td>0.894216</td>\n",
       "      <td>0.877238</td>\n",
       "      <td>0.192570</td>\n",
       "      <td>0.307265</td>\n",
       "      <td>0.438829</td>\n",
       "      <td>0.695699</td>\n",
       "      <td>0.834195</td>\n",
       "      <td>0.819372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>0.267856</td>\n",
       "      <td>0.376856</td>\n",
       "      <td>0.517548</td>\n",
       "      <td>0.569923</td>\n",
       "      <td>0.789705</td>\n",
       "      <td>0.766357</td>\n",
       "      <td>0.207799</td>\n",
       "      <td>0.321427</td>\n",
       "      <td>0.455849</td>\n",
       "      <td>0.671635</td>\n",
       "      <td>0.820121</td>\n",
       "      <td>0.803596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>0.126177</td>\n",
       "      <td>0.258095</td>\n",
       "      <td>0.355214</td>\n",
       "      <td>0.797407</td>\n",
       "      <td>0.893001</td>\n",
       "      <td>0.875599</td>\n",
       "      <td>0.194302</td>\n",
       "      <td>0.308937</td>\n",
       "      <td>0.440797</td>\n",
       "      <td>0.692963</td>\n",
       "      <td>0.832664</td>\n",
       "      <td>0.815785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>0.124109</td>\n",
       "      <td>0.255552</td>\n",
       "      <td>0.352291</td>\n",
       "      <td>0.800726</td>\n",
       "      <td>0.894840</td>\n",
       "      <td>0.877708</td>\n",
       "      <td>0.192199</td>\n",
       "      <td>0.306568</td>\n",
       "      <td>0.438405</td>\n",
       "      <td>0.696286</td>\n",
       "      <td>0.834582</td>\n",
       "      <td>0.819575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostRegressor</th>\n",
       "      <td>0.173815</td>\n",
       "      <td>0.322050</td>\n",
       "      <td>0.416911</td>\n",
       "      <td>0.720918</td>\n",
       "      <td>0.861063</td>\n",
       "      <td>0.855866</td>\n",
       "      <td>0.220510</td>\n",
       "      <td>0.352285</td>\n",
       "      <td>0.469585</td>\n",
       "      <td>0.651549</td>\n",
       "      <td>0.816796</td>\n",
       "      <td>0.812278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBRegressor</th>\n",
       "      <td>0.140338</td>\n",
       "      <td>0.272763</td>\n",
       "      <td>0.374618</td>\n",
       "      <td>0.774668</td>\n",
       "      <td>0.880829</td>\n",
       "      <td>0.863864</td>\n",
       "      <td>0.193417</td>\n",
       "      <td>0.309127</td>\n",
       "      <td>0.439793</td>\n",
       "      <td>0.694360</td>\n",
       "      <td>0.833550</td>\n",
       "      <td>0.817321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesRegressor</th>\n",
       "      <td>0.124208</td>\n",
       "      <td>0.255444</td>\n",
       "      <td>0.352431</td>\n",
       "      <td>0.800568</td>\n",
       "      <td>0.894773</td>\n",
       "      <td>0.876582</td>\n",
       "      <td>0.194272</td>\n",
       "      <td>0.309753</td>\n",
       "      <td>0.440763</td>\n",
       "      <td>0.693010</td>\n",
       "      <td>0.832764</td>\n",
       "      <td>0.815438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>0.124418</td>\n",
       "      <td>0.254880</td>\n",
       "      <td>0.352730</td>\n",
       "      <td>0.800231</td>\n",
       "      <td>0.894559</td>\n",
       "      <td>0.877908</td>\n",
       "      <td>0.193488</td>\n",
       "      <td>0.307760</td>\n",
       "      <td>0.439873</td>\n",
       "      <td>0.694248</td>\n",
       "      <td>0.833375</td>\n",
       "      <td>0.818256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsRegressor</th>\n",
       "      <td>0.147932</td>\n",
       "      <td>0.281681</td>\n",
       "      <td>0.384619</td>\n",
       "      <td>0.762476</td>\n",
       "      <td>0.873291</td>\n",
       "      <td>0.853887</td>\n",
       "      <td>0.202575</td>\n",
       "      <td>0.321204</td>\n",
       "      <td>0.450083</td>\n",
       "      <td>0.679890</td>\n",
       "      <td>0.825404</td>\n",
       "      <td>0.807349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>0.132723</td>\n",
       "      <td>0.260151</td>\n",
       "      <td>0.364312</td>\n",
       "      <td>0.786895</td>\n",
       "      <td>0.887303</td>\n",
       "      <td>0.875771</td>\n",
       "      <td>0.193713</td>\n",
       "      <td>0.305304</td>\n",
       "      <td>0.440128</td>\n",
       "      <td>0.693893</td>\n",
       "      <td>0.834128</td>\n",
       "      <td>0.817935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPRegressor</th>\n",
       "      <td>0.129892</td>\n",
       "      <td>0.261928</td>\n",
       "      <td>0.360406</td>\n",
       "      <td>0.791441</td>\n",
       "      <td>0.889836</td>\n",
       "      <td>0.871743</td>\n",
       "      <td>0.192121</td>\n",
       "      <td>0.307213</td>\n",
       "      <td>0.438317</td>\n",
       "      <td>0.696408</td>\n",
       "      <td>0.835034</td>\n",
       "      <td>0.820566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Train MSE (15 fold CV)  Train MAE (15 fold CV)  \\\n",
       "LGBMRegressor                            0.124803                0.256939   \n",
       "DecisionTreeRegressor                    0.267856                0.376856   \n",
       "RandomForestRegressor                    0.126177                0.258095   \n",
       "GradientBoostingRegressor                0.124109                0.255552   \n",
       "AdaBoostRegressor                        0.173815                0.322050   \n",
       "XGBRegressor                             0.140338                0.272763   \n",
       "ExtraTreesRegressor                      0.124208                0.255444   \n",
       "LinearRegression                         0.124418                0.254880   \n",
       "KNeighborsRegressor                      0.147932                0.281681   \n",
       "SVR                                      0.132723                0.260151   \n",
       "MLPRegressor                             0.129892                0.261928   \n",
       "\n",
       "                           Train RMSE (15 fold CV)  Train R2 (15 fold CV)  \\\n",
       "LGBMRegressor                             0.353275               0.799612   \n",
       "DecisionTreeRegressor                     0.517548               0.569923   \n",
       "RandomForestRegressor                     0.355214               0.797407   \n",
       "GradientBoostingRegressor                 0.352291               0.800726   \n",
       "AdaBoostRegressor                         0.416911               0.720918   \n",
       "XGBRegressor                              0.374618               0.774668   \n",
       "ExtraTreesRegressor                       0.352431               0.800568   \n",
       "LinearRegression                          0.352730               0.800231   \n",
       "KNeighborsRegressor                       0.384619               0.762476   \n",
       "SVR                                       0.364312               0.786895   \n",
       "MLPRegressor                              0.360406               0.791441   \n",
       "\n",
       "                           Train PCC (15 fold CV)  Train SCC (15 fold CV)  \\\n",
       "LGBMRegressor                            0.894216                0.877238   \n",
       "DecisionTreeRegressor                    0.789705                0.766357   \n",
       "RandomForestRegressor                    0.893001                0.875599   \n",
       "GradientBoostingRegressor                0.894840                0.877708   \n",
       "AdaBoostRegressor                        0.861063                0.855866   \n",
       "XGBRegressor                             0.880829                0.863864   \n",
       "ExtraTreesRegressor                      0.894773                0.876582   \n",
       "LinearRegression                         0.894559                0.877908   \n",
       "KNeighborsRegressor                      0.873291                0.853887   \n",
       "SVR                                      0.887303                0.875771   \n",
       "MLPRegressor                             0.889836                0.871743   \n",
       "\n",
       "                           Test MSE  Test MAE  Test RMSE   Test R2  Test PCC  \\\n",
       "LGBMRegressor              0.192570  0.307265   0.438829  0.695699  0.834195   \n",
       "DecisionTreeRegressor      0.207799  0.321427   0.455849  0.671635  0.820121   \n",
       "RandomForestRegressor      0.194302  0.308937   0.440797  0.692963  0.832664   \n",
       "GradientBoostingRegressor  0.192199  0.306568   0.438405  0.696286  0.834582   \n",
       "AdaBoostRegressor          0.220510  0.352285   0.469585  0.651549  0.816796   \n",
       "XGBRegressor               0.193417  0.309127   0.439793  0.694360  0.833550   \n",
       "ExtraTreesRegressor        0.194272  0.309753   0.440763  0.693010  0.832764   \n",
       "LinearRegression           0.193488  0.307760   0.439873  0.694248  0.833375   \n",
       "KNeighborsRegressor        0.202575  0.321204   0.450083  0.679890  0.825404   \n",
       "SVR                        0.193713  0.305304   0.440128  0.693893  0.834128   \n",
       "MLPRegressor               0.192121  0.307213   0.438317  0.696408  0.835034   \n",
       "\n",
       "                           Test SCC  \n",
       "LGBMRegressor              0.819372  \n",
       "DecisionTreeRegressor      0.803596  \n",
       "RandomForestRegressor      0.815785  \n",
       "GradientBoostingRegressor  0.819575  \n",
       "AdaBoostRegressor          0.812278  \n",
       "XGBRegressor               0.817321  \n",
       "ExtraTreesRegressor        0.815438  \n",
       "LinearRegression           0.818256  \n",
       "KNeighborsRegressor        0.807349  \n",
       "SVR                        0.817935  \n",
       "MLPRegressor               0.820566  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_weak = [\n",
    "    lgb.LGBMRegressor(objective='regression', metric='rmse', boosting_type='gbdt', num_leaves=31, learning_rate=0.05, random_state=101),\n",
    "    RandomForestRegressor(n_jobs=-1, random_state=101),\n",
    "    GradientBoostingRegressor(random_state=101),\n",
    "    AdaBoostRegressor(random_state=101),\n",
    "    xgb.XGBRegressor(random_state=101, eval_metric='rmse'),\n",
    "    ExtraTreesRegressor(n_jobs=-1, n_estimators=100, random_state=101),\n",
    "    KNeighborsRegressor(),\n",
    "    SVR(),\n",
    "    MLPRegressor(random_state=101, max_iter=500),\n",
    "    DecisionTreeRegressor(random_state=101),\n",
    "\n",
    "]\n",
    "\n",
    "models_meta = [\n",
    "    lgb.LGBMRegressor(objective='regression', metric='rmse', boosting_type='gbdt', num_leaves=31, learning_rate=0.05, random_state=101),\n",
    "    DecisionTreeRegressor(random_state=101),\n",
    "    RandomForestRegressor(n_jobs=-1, random_state=101),\n",
    "    GradientBoostingRegressor(random_state=101),\n",
    "    AdaBoostRegressor(random_state=101),\n",
    "    xgb.XGBRegressor(random_state=101, eval_metric='rmse'),\n",
    "    ExtraTreesRegressor(n_jobs=-1, n_estimators=100, random_state=101),\n",
    "    LinearRegression(),\n",
    "    KNeighborsRegressor(),\n",
    "    SVR(),\n",
    "    MLPRegressor(random_state=101, max_iter=500)\n",
    "]\n",
    "\n",
    "dataframes = [(df_desc_train, df_desc_test), (df_fp_train, df_fp_test), (df_emb_train, df_emb_test), (df_atomic_train, df_atomic_test)]\n",
    "\n",
    "meta_features_train = []\n",
    "meta_features_test = []\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Stage 1: Training weak learners with 15-fold CV')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "\n",
    "# Stage 1: Train weak learners with 15-fold cross-validation\n",
    "for df_train, df_test in tqdm(dataframes, desc=\"Processing dataframe pairs\"):\n",
    "    X_weak = df_train.drop(columns=['ID', 'SMILES', target_column])\n",
    "    y_weak = df_train[target_column]\n",
    "    X_eval = df_test.drop(columns=['ID', 'SMILES', target_column])\n",
    "    y_eval = df_test[target_column]\n",
    "\n",
    "    kf = KFold(n_splits=15, shuffle=True, random_state=101)\n",
    "\n",
    "    fold_meta_features_train = np.zeros((X_weak.shape[0], len(models_weak)))\n",
    "    fold_meta_features_test = np.zeros((X_eval.shape[0], len(models_weak)))\n",
    "\n",
    "    for i, model in tqdm(enumerate(models_weak), desc=\"Training models\", total=len(models_weak)):\n",
    "        fold_predictions = np.zeros(X_weak.shape[0])\n",
    "        test_predictions_folds = []\n",
    "\n",
    "        for train_index, val_index in kf.split(X_weak):\n",
    "            X_train, X_val = X_weak.iloc[train_index], X_weak.iloc[val_index]\n",
    "            y_train, y_val = y_weak.iloc[train_index], y_weak.iloc[val_index]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            fold_predictions[val_index] = np.clip(model.predict(X_val), -10, -3.9)\n",
    "            test_predictions_fold = np.clip(model.predict(X_eval), -10, -3.9)\n",
    "            test_predictions_folds.append(test_predictions_fold)\n",
    "\n",
    "        fold_meta_features_train[:, i] = fold_predictions\n",
    "        fold_meta_features_test[:, i] = np.mean(test_predictions_folds, axis=0)\n",
    "        print(f'Model training done {i}')\n",
    "\n",
    "    meta_features_train.append(fold_meta_features_train)\n",
    "    meta_features_test.append(fold_meta_features_test)\n",
    "\n",
    "meta_features_train = np.hstack(meta_features_train)\n",
    "meta_features_test = np.hstack(meta_features_test)\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print(\"Dimensions of meta_features_train:\", meta_features_train.shape)\n",
    "print(\"Dimensions of meta_features_test:\", meta_features_test.shape)\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Stage 1 completed')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Stage 2: Training meta-learners')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "\n",
    "\n",
    "results = {}\n",
    "predictions = []\n",
    "\n",
    "kf = KFold(n_splits=15, shuffle=True, random_state=101)\n",
    "\n",
    "for model in models_meta:\n",
    "    model_name = model.__class__.__name__\n",
    "    predictions_train = []\n",
    "    actual_y_train = []\n",
    "    test_predictions_folds = []\n",
    "\n",
    "    for train_index, val_index in kf.split(meta_features_train):\n",
    "        X_fold_train, X_fold_val = meta_features_train[train_index], meta_features_train[val_index]\n",
    "        y_fold_train, y_fold_val = y_weak.iloc[train_index], y_weak.iloc[val_index]\n",
    "\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "        y_pred_fold = model.predict(X_fold_val)\n",
    "        y_pred_fold = np.clip(y_pred_fold, -10, -3.9)\n",
    "\n",
    "        predictions_train.extend(y_pred_fold)\n",
    "        actual_y_train.extend(y_fold_val)\n",
    "\n",
    "        test_predictions_fold = model.predict(meta_features_test)\n",
    "        test_predictions_fold = np.clip(test_predictions_fold, -10, -3.9)\n",
    "        test_predictions_folds.append(test_predictions_fold)\n",
    "\n",
    "    predictions_test_mean = np.mean(test_predictions_folds, axis=0)\n",
    "    predictions_test_std = np.std(test_predictions_folds, axis=0)\n",
    "\n",
    "    mse_train = mean_squared_error(actual_y_train, predictions_train)\n",
    "    mae_train = mean_absolute_error(actual_y_train, predictions_train)\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    r2_train = r2_score(actual_y_train, predictions_train)\n",
    "    pearson_train, _ = pearsonr(actual_y_train, predictions_train)\n",
    "    spearman_train, _ = spearmanr(actual_y_train, predictions_train)\n",
    "\n",
    "    mse_test = mean_squared_error(y_eval, predictions_test_mean)\n",
    "    mae_test = mean_absolute_error(y_eval, predictions_test_mean)\n",
    "    rmse_test = np.sqrt(mse_test)\n",
    "    r2_test = r2_score(y_eval, predictions_test_mean)\n",
    "    pearson_test, _ = pearsonr(y_eval, predictions_test_mean)\n",
    "    spearman_test, _ = spearmanr(y_eval, predictions_test_mean)\n",
    "\n",
    "    print(f'{model_name} Evaluation completed: Test R2 score: {r2_test:.4f}, Test RMSE: {rmse_test:.4f}')\n",
    "\n",
    "    predictions.append({\n",
    "        'Model': model_name,\n",
    "        'Y Train pred': predictions_train,\n",
    "        'Y Train actual': actual_y_train,\n",
    "        'Y Test actual': y_eval,\n",
    "        'Test Predictions folds': test_predictions_folds,\n",
    "        'Test Predictions Mean': predictions_test_mean,\n",
    "        'Test Predictions Std': predictions_test_std,\n",
    "    })\n",
    "\n",
    "    results[model_name] = {\n",
    "        'Train MSE (15 fold CV)': mse_train,\n",
    "        'Train MAE (15 fold CV)': mae_train,\n",
    "        'Train RMSE (15 fold CV)': rmse_train,\n",
    "        'Train R2 (15 fold CV)': r2_train,\n",
    "        'Train PCC (15 fold CV)': pearson_train,\n",
    "        'Train SCC (15 fold CV)': spearman_train,\n",
    "        'Test MSE': mse_test,\n",
    "        'Test MAE': mae_test,\n",
    "        'Test RMSE': rmse_test,\n",
    "        'Test R2': r2_test,\n",
    "        'Test PCC': pearson_test,\n",
    "        'Test SCC': spearman_test,\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Stacked Ensemble Training and Evaluation complete')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2982898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('/home/users/akshay/PCPpred/PAMPA/Results/Stacked_architecture/Results_15_fold_stacked_prediction_LVR_scaled.csv')\n",
    "predictions_df.to_csv('/home/users/akshay/PCPpred/PAMPA/Results/Stacked_architecture/Prediction_data_Meta_learners_15_fold_stacked_prediction_LVR_scaled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed96e5fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Stage 1: Training weak learners with 5-fold CV\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataframe pairs:   0%|                                                                 | 0/4 [00:00<?, ?it/s]\n",
      "Training models:   0%|                                                                           | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56831\n",
      "[LightGBM] [Info] Number of data points in the train set: 4454, number of used features: 260\n",
      "[LightGBM] [Info] Start training from score -5.747051\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007085 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56849\n",
      "[LightGBM] [Info] Number of data points in the train set: 4454, number of used features: 260\n",
      "[LightGBM] [Info] Start training from score -5.747251\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007428 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56836\n",
      "[LightGBM] [Info] Number of data points in the train set: 4454, number of used features: 261\n",
      "[LightGBM] [Info] Start training from score -5.741644\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007087 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56849\n",
      "[LightGBM] [Info] Number of data points in the train set: 4455, number of used features: 261\n",
      "[LightGBM] [Info] Start training from score -5.751882\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56834\n",
      "[LightGBM] [Info] Number of data points in the train set: 4455, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.726702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  10%|██████▋                                                            | 1/10 [00:03<00:33,  3.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  20%|█████████████▍                                                     | 2/10 [00:10<00:42,  5.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  30%|████████████████████                                               | 3/10 [02:02<06:20, 54.32s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  40%|██████████████████████████▊                                        | 4/10 [02:36<04:36, 46.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  50%|█████████████████████████████████▌                                 | 5/10 [02:39<02:33, 30.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  60%|████████████████████████████████████████▏                          | 6/10 [02:42<01:24, 21.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  70%|██████████████████████████████████████████████▉                    | 7/10 [02:43<00:43, 14.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  80%|█████████████████████████████████████████████████████▌             | 8/10 [02:56<00:28, 14.08s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  90%|████████████████████████████████████████████████████████████▎      | 9/10 [03:07<00:13, 13.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models: 100%|██████████████████████████████████████████████████████████████████| 10/10 [03:11<00:00, 19.19s/it]\u001b[A\n",
      "Processing dataframe pairs:  25%|██████████████                                          | 1/4 [03:11<09:35, 191.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:   0%|                                                                           | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041948 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4161\n",
      "[LightGBM] [Info] Number of data points in the train set: 4454, number of used features: 1098\n",
      "[LightGBM] [Info] Start training from score -5.747051\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041298 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4165\n",
      "[LightGBM] [Info] Number of data points in the train set: 4454, number of used features: 1100\n",
      "[LightGBM] [Info] Start training from score -5.747251\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041860 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4165\n",
      "[LightGBM] [Info] Number of data points in the train set: 4454, number of used features: 1101\n",
      "[LightGBM] [Info] Start training from score -5.741644\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040958 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4172\n",
      "[LightGBM] [Info] Number of data points in the train set: 4455, number of used features: 1102\n",
      "[LightGBM] [Info] Start training from score -5.751882\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040490 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4144\n",
      "[LightGBM] [Info] Number of data points in the train set: 4455, number of used features: 1096\n",
      "[LightGBM] [Info] Start training from score -5.726702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  10%|██████▋                                                            | 1/10 [00:05<00:47,  5.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  20%|█████████████▍                                                     | 2/10 [00:08<00:31,  3.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  30%|████████████████████                                               | 3/10 [00:39<01:54, 16.32s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  40%|██████████████████████████▊                                        | 4/10 [01:04<01:58, 19.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  50%|█████████████████████████████████▌                                 | 5/10 [01:08<01:10, 14.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  60%|████████████████████████████████████████▏                          | 6/10 [01:10<00:40, 10.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  70%|██████████████████████████████████████████████▉                    | 7/10 [01:11<00:21,  7.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  80%|█████████████████████████████████████████████████████▌             | 8/10 [02:00<00:40, 20.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  90%|████████████████████████████████████████████████████████████▎      | 9/10 [02:23<00:21, 21.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models: 100%|██████████████████████████████████████████████████████████████████| 10/10 [02:24<00:00, 14.46s/it]\u001b[A\n",
      "Processing dataframe pairs:  50%|████████████████████████████                            | 2/4 [05:36<05:28, 164.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:   0%|                                                                           | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017068 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 4454, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.747051\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015529 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 4454, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.747251\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 4454, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.741644\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016363 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 4455, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.751882\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016066 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 4455, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.726702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  10%|██████▋                                                            | 1/10 [00:03<00:34,  3.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  20%|█████████████▍                                                     | 2/10 [00:28<02:10, 16.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  30%|███████████████████▊                                              | 3/10 [10:44<33:50, 290.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  40%|██████████████████████████▍                                       | 4/10 [13:03<23:02, 230.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  50%|█████████████████████████████████                                 | 5/10 [13:15<12:38, 151.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  60%|███████████████████████████████████████▌                          | 6/10 [13:21<06:48, 102.08s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  70%|██████████████████████████████████████████████▉                    | 7/10 [13:22<03:26, 68.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  80%|█████████████████████████████████████████████████████▌             | 8/10 [13:51<01:52, 56.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  90%|████████████████████████████████████████████████████████████▎      | 9/10 [14:12<00:45, 45.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models: 100%|██████████████████████████████████████████████████████████████████| 10/10 [14:36<00:00, 87.65s/it]\u001b[A\n",
      "Processing dataframe pairs:  75%|██████████████████████████████████████████              | 3/4 [20:13<08:09, 489.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:   0%|                                                                           | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001312 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 150\n",
      "[LightGBM] [Info] Number of data points in the train set: 4454, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.747051\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 148\n",
      "[LightGBM] [Info] Number of data points in the train set: 4454, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.747251\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001064 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 150\n",
      "[LightGBM] [Info] Number of data points in the train set: 4454, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.741644\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 151\n",
      "[LightGBM] [Info] Number of data points in the train set: 4455, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.751882\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001071 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 150\n",
      "[LightGBM] [Info] Number of data points in the train set: 4455, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.726702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  10%|██████▋                                                            | 1/10 [00:02<00:22,  2.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  20%|█████████████▍                                                     | 2/10 [00:04<00:17,  2.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  30%|████████████████████                                               | 3/10 [00:05<00:10,  1.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  40%|██████████████████████████▊                                        | 4/10 [00:05<00:05,  1.01it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  50%|█████████████████████████████████▌                                 | 5/10 [00:06<00:04,  1.08it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  60%|████████████████████████████████████████▏                          | 6/10 [00:07<00:04,  1.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  70%|██████████████████████████████████████████████▉                    | 7/10 [00:07<00:02,  1.23it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  80%|█████████████████████████████████████████████████████▌             | 8/10 [00:11<00:03,  1.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models: 100%|██████████████████████████████████████████████████████████████████| 10/10 [00:18<00:00,  1.80s/it]\u001b[A\n",
      "Processing dataframe pairs: 100%|████████████████████████████████████████████████████████| 4/4 [20:31<00:00, 307.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8\n",
      "Model training done 9\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dimensions of meta_features_train: (5568, 40)\n",
      "Dimensions of meta_features_test: (1392, 40)\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Stage 1 completed\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Stage 2: Training meta-learners with 5-fold CV\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061491 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9967\n",
      "[LightGBM] [Info] Number of data points in the train set: 4454, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.747051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9966\n",
      "[LightGBM] [Info] Number of data points in the train set: 4454, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.747251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001479 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9966\n",
      "[LightGBM] [Info] Number of data points in the train set: 4454, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.741644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001515 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9979\n",
      "[LightGBM] [Info] Number of data points in the train set: 4455, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.751882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001522 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9961\n",
      "[LightGBM] [Info] Number of data points in the train set: 4455, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.726702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor Evaluation completed: Test R2 score: 0.6926, Test RMSE: 0.4410\n",
      "DecisionTreeRegressor Evaluation completed: Test R2 score: 0.6544, Test RMSE: 0.4676\n",
      "RandomForestRegressor Evaluation completed: Test R2 score: 0.6888, Test RMSE: 0.4438\n",
      "GradientBoostingRegressor Evaluation completed: Test R2 score: 0.6947, Test RMSE: 0.4395\n",
      "AdaBoostRegressor Evaluation completed: Test R2 score: 0.6464, Test RMSE: 0.4730\n",
      "XGBRegressor Evaluation completed: Test R2 score: 0.6804, Test RMSE: 0.4497\n",
      "ExtraTreesRegressor Evaluation completed: Test R2 score: 0.6909, Test RMSE: 0.4423\n",
      "LinearRegression Evaluation completed: Test R2 score: 0.6935, Test RMSE: 0.4404\n",
      "KNeighborsRegressor Evaluation completed: Test R2 score: 0.6793, Test RMSE: 0.4505\n",
      "SVR Evaluation completed: Test R2 score: 0.6933, Test RMSE: 0.4406\n",
      "MLPRegressor Evaluation completed: Test R2 score: 0.6953, Test RMSE: 0.4391\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Stacked Ensemble Training and Evaluation complete\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train MSE (5 fold CV)</th>\n",
       "      <th>Train MAE (5 fold CV)</th>\n",
       "      <th>Train RMSE (5 fold CV)</th>\n",
       "      <th>Train R2 (5 fold CV)</th>\n",
       "      <th>Train PCC (5 fold CV)</th>\n",
       "      <th>Train SCC (5 fold CV)</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Test MAE</th>\n",
       "      <th>Test RMSE</th>\n",
       "      <th>Test R2</th>\n",
       "      <th>Test PCC</th>\n",
       "      <th>Test SCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <td>0.127861</td>\n",
       "      <td>0.259890</td>\n",
       "      <td>0.357577</td>\n",
       "      <td>0.794702</td>\n",
       "      <td>0.891473</td>\n",
       "      <td>0.876314</td>\n",
       "      <td>0.194517</td>\n",
       "      <td>0.309135</td>\n",
       "      <td>0.441041</td>\n",
       "      <td>0.692623</td>\n",
       "      <td>0.832415</td>\n",
       "      <td>0.818280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>0.264635</td>\n",
       "      <td>0.373133</td>\n",
       "      <td>0.514427</td>\n",
       "      <td>0.575094</td>\n",
       "      <td>0.790962</td>\n",
       "      <td>0.773510</td>\n",
       "      <td>0.218677</td>\n",
       "      <td>0.331339</td>\n",
       "      <td>0.467629</td>\n",
       "      <td>0.654446</td>\n",
       "      <td>0.810925</td>\n",
       "      <td>0.793323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>0.129418</td>\n",
       "      <td>0.261589</td>\n",
       "      <td>0.359747</td>\n",
       "      <td>0.792203</td>\n",
       "      <td>0.890144</td>\n",
       "      <td>0.873851</td>\n",
       "      <td>0.196953</td>\n",
       "      <td>0.309978</td>\n",
       "      <td>0.443793</td>\n",
       "      <td>0.688774</td>\n",
       "      <td>0.830284</td>\n",
       "      <td>0.814919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>0.128346</td>\n",
       "      <td>0.260018</td>\n",
       "      <td>0.358254</td>\n",
       "      <td>0.793924</td>\n",
       "      <td>0.891025</td>\n",
       "      <td>0.875486</td>\n",
       "      <td>0.193185</td>\n",
       "      <td>0.308821</td>\n",
       "      <td>0.439529</td>\n",
       "      <td>0.694727</td>\n",
       "      <td>0.833655</td>\n",
       "      <td>0.818067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostRegressor</th>\n",
       "      <td>0.173250</td>\n",
       "      <td>0.321010</td>\n",
       "      <td>0.416233</td>\n",
       "      <td>0.721825</td>\n",
       "      <td>0.859417</td>\n",
       "      <td>0.847435</td>\n",
       "      <td>0.223738</td>\n",
       "      <td>0.356501</td>\n",
       "      <td>0.473009</td>\n",
       "      <td>0.646448</td>\n",
       "      <td>0.811120</td>\n",
       "      <td>0.805100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBRegressor</th>\n",
       "      <td>0.146996</td>\n",
       "      <td>0.279729</td>\n",
       "      <td>0.383400</td>\n",
       "      <td>0.763980</td>\n",
       "      <td>0.875091</td>\n",
       "      <td>0.858649</td>\n",
       "      <td>0.202248</td>\n",
       "      <td>0.314479</td>\n",
       "      <td>0.449720</td>\n",
       "      <td>0.680406</td>\n",
       "      <td>0.825609</td>\n",
       "      <td>0.810231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesRegressor</th>\n",
       "      <td>0.128417</td>\n",
       "      <td>0.260089</td>\n",
       "      <td>0.358353</td>\n",
       "      <td>0.793810</td>\n",
       "      <td>0.891065</td>\n",
       "      <td>0.875282</td>\n",
       "      <td>0.195636</td>\n",
       "      <td>0.310127</td>\n",
       "      <td>0.442308</td>\n",
       "      <td>0.690854</td>\n",
       "      <td>0.831489</td>\n",
       "      <td>0.816471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>0.125450</td>\n",
       "      <td>0.256851</td>\n",
       "      <td>0.354189</td>\n",
       "      <td>0.798574</td>\n",
       "      <td>0.893633</td>\n",
       "      <td>0.876812</td>\n",
       "      <td>0.193970</td>\n",
       "      <td>0.309375</td>\n",
       "      <td>0.440420</td>\n",
       "      <td>0.693488</td>\n",
       "      <td>0.832908</td>\n",
       "      <td>0.817046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsRegressor</th>\n",
       "      <td>0.153300</td>\n",
       "      <td>0.285699</td>\n",
       "      <td>0.391536</td>\n",
       "      <td>0.753856</td>\n",
       "      <td>0.868534</td>\n",
       "      <td>0.850334</td>\n",
       "      <td>0.202936</td>\n",
       "      <td>0.321619</td>\n",
       "      <td>0.450485</td>\n",
       "      <td>0.679318</td>\n",
       "      <td>0.824906</td>\n",
       "      <td>0.810051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>0.135357</td>\n",
       "      <td>0.263448</td>\n",
       "      <td>0.367909</td>\n",
       "      <td>0.782667</td>\n",
       "      <td>0.884809</td>\n",
       "      <td>0.873899</td>\n",
       "      <td>0.194086</td>\n",
       "      <td>0.306082</td>\n",
       "      <td>0.440552</td>\n",
       "      <td>0.693303</td>\n",
       "      <td>0.833898</td>\n",
       "      <td>0.819824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPRegressor</th>\n",
       "      <td>0.131252</td>\n",
       "      <td>0.262776</td>\n",
       "      <td>0.362288</td>\n",
       "      <td>0.789257</td>\n",
       "      <td>0.888991</td>\n",
       "      <td>0.870468</td>\n",
       "      <td>0.192847</td>\n",
       "      <td>0.307050</td>\n",
       "      <td>0.439143</td>\n",
       "      <td>0.695262</td>\n",
       "      <td>0.835068</td>\n",
       "      <td>0.820507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Train MSE (5 fold CV)  Train MAE (5 fold CV)  \\\n",
       "LGBMRegressor                           0.127861               0.259890   \n",
       "DecisionTreeRegressor                   0.264635               0.373133   \n",
       "RandomForestRegressor                   0.129418               0.261589   \n",
       "GradientBoostingRegressor               0.128346               0.260018   \n",
       "AdaBoostRegressor                       0.173250               0.321010   \n",
       "XGBRegressor                            0.146996               0.279729   \n",
       "ExtraTreesRegressor                     0.128417               0.260089   \n",
       "LinearRegression                        0.125450               0.256851   \n",
       "KNeighborsRegressor                     0.153300               0.285699   \n",
       "SVR                                     0.135357               0.263448   \n",
       "MLPRegressor                            0.131252               0.262776   \n",
       "\n",
       "                           Train RMSE (5 fold CV)  Train R2 (5 fold CV)  \\\n",
       "LGBMRegressor                            0.357577              0.794702   \n",
       "DecisionTreeRegressor                    0.514427              0.575094   \n",
       "RandomForestRegressor                    0.359747              0.792203   \n",
       "GradientBoostingRegressor                0.358254              0.793924   \n",
       "AdaBoostRegressor                        0.416233              0.721825   \n",
       "XGBRegressor                             0.383400              0.763980   \n",
       "ExtraTreesRegressor                      0.358353              0.793810   \n",
       "LinearRegression                         0.354189              0.798574   \n",
       "KNeighborsRegressor                      0.391536              0.753856   \n",
       "SVR                                      0.367909              0.782667   \n",
       "MLPRegressor                             0.362288              0.789257   \n",
       "\n",
       "                           Train PCC (5 fold CV)  Train SCC (5 fold CV)  \\\n",
       "LGBMRegressor                           0.891473               0.876314   \n",
       "DecisionTreeRegressor                   0.790962               0.773510   \n",
       "RandomForestRegressor                   0.890144               0.873851   \n",
       "GradientBoostingRegressor               0.891025               0.875486   \n",
       "AdaBoostRegressor                       0.859417               0.847435   \n",
       "XGBRegressor                            0.875091               0.858649   \n",
       "ExtraTreesRegressor                     0.891065               0.875282   \n",
       "LinearRegression                        0.893633               0.876812   \n",
       "KNeighborsRegressor                     0.868534               0.850334   \n",
       "SVR                                     0.884809               0.873899   \n",
       "MLPRegressor                            0.888991               0.870468   \n",
       "\n",
       "                           Test MSE  Test MAE  Test RMSE   Test R2  Test PCC  \\\n",
       "LGBMRegressor              0.194517  0.309135   0.441041  0.692623  0.832415   \n",
       "DecisionTreeRegressor      0.218677  0.331339   0.467629  0.654446  0.810925   \n",
       "RandomForestRegressor      0.196953  0.309978   0.443793  0.688774  0.830284   \n",
       "GradientBoostingRegressor  0.193185  0.308821   0.439529  0.694727  0.833655   \n",
       "AdaBoostRegressor          0.223738  0.356501   0.473009  0.646448  0.811120   \n",
       "XGBRegressor               0.202248  0.314479   0.449720  0.680406  0.825609   \n",
       "ExtraTreesRegressor        0.195636  0.310127   0.442308  0.690854  0.831489   \n",
       "LinearRegression           0.193970  0.309375   0.440420  0.693488  0.832908   \n",
       "KNeighborsRegressor        0.202936  0.321619   0.450485  0.679318  0.824906   \n",
       "SVR                        0.194086  0.306082   0.440552  0.693303  0.833898   \n",
       "MLPRegressor               0.192847  0.307050   0.439143  0.695262  0.835068   \n",
       "\n",
       "                           Test SCC  \n",
       "LGBMRegressor              0.818280  \n",
       "DecisionTreeRegressor      0.793323  \n",
       "RandomForestRegressor      0.814919  \n",
       "GradientBoostingRegressor  0.818067  \n",
       "AdaBoostRegressor          0.805100  \n",
       "XGBRegressor               0.810231  \n",
       "ExtraTreesRegressor        0.816471  \n",
       "LinearRegression           0.817046  \n",
       "KNeighborsRegressor        0.810051  \n",
       "SVR                        0.819824  \n",
       "MLPRegressor               0.820507  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_weak = [\n",
    "    lgb.LGBMRegressor(objective='regression', metric='rmse', boosting_type='gbdt', num_leaves=31, learning_rate=0.05, random_state=101),\n",
    "    RandomForestRegressor(n_jobs=-1, random_state=101),\n",
    "    GradientBoostingRegressor(random_state=101),\n",
    "    AdaBoostRegressor(random_state=101),\n",
    "    xgb.XGBRegressor(random_state=101, eval_metric='rmse'),\n",
    "    ExtraTreesRegressor(n_jobs=-1, n_estimators=100, random_state=101),\n",
    "    KNeighborsRegressor(),\n",
    "    SVR(),\n",
    "    MLPRegressor(random_state=101, max_iter=500),\n",
    "    DecisionTreeRegressor(random_state=101)\n",
    "]\n",
    "\n",
    "models_meta = [\n",
    "    lgb.LGBMRegressor(objective='regression', metric='rmse', boosting_type='gbdt', num_leaves=31, learning_rate=0.05, random_state=101),\n",
    "    DecisionTreeRegressor(random_state=101),\n",
    "    RandomForestRegressor(n_jobs=-1, random_state=101),\n",
    "    GradientBoostingRegressor(random_state=101),\n",
    "    AdaBoostRegressor(random_state=101),\n",
    "    xgb.XGBRegressor(random_state=101, eval_metric='rmse'),\n",
    "    ExtraTreesRegressor(n_jobs=-1, n_estimators=100, random_state=101),\n",
    "    LinearRegression(),\n",
    "    KNeighborsRegressor(),\n",
    "    SVR(),\n",
    "    MLPRegressor(random_state=101, max_iter=500)\n",
    "]\n",
    "target_column = 'Permeability'\n",
    "dataframes = [(df_desc_train, df_desc_test), (df_fp_train, df_fp_test), (df_emb_train, df_emb_test), (df_atomic_train, df_atomic_test)]\n",
    "\n",
    "meta_features_train = []\n",
    "meta_features_test = []\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Stage 1: Training weak learners with 5-fold CV')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "\n",
    "# Stage 1: Train weak learners with 5-fold cross-validation\n",
    "for df_train, df_test in tqdm(dataframes, desc=\"Processing dataframe pairs\"):\n",
    "    X_weak = df_train.drop(columns=['ID', 'SMILES', target_column])\n",
    "    y_weak = df_train[target_column]\n",
    "    X_eval = df_test.drop(columns=['ID', 'SMILES', target_column])\n",
    "    y_eval = df_test[target_column]\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=101)\n",
    "\n",
    "    fold_meta_features_train = np.zeros((X_weak.shape[0], len(models_weak)))\n",
    "    fold_meta_features_test = np.zeros((X_eval.shape[0], len(models_weak)))\n",
    "\n",
    "    for i, model in tqdm(enumerate(models_weak), desc=\"Training models\", total=len(models_weak)):\n",
    "        fold_predictions = np.zeros(X_weak.shape[0])\n",
    "        test_predictions_folds = []\n",
    "\n",
    "        for train_index, val_index in kf.split(X_weak):\n",
    "            X_train, X_val = X_weak.iloc[train_index], X_weak.iloc[val_index]\n",
    "            y_train, y_val = y_weak.iloc[train_index], y_weak.iloc[val_index]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            fold_predictions[val_index] = np.clip(model.predict(X_val), -10, -3.9)\n",
    "            test_predictions_fold = np.clip(model.predict(X_eval), -10, -3.9)\n",
    "            test_predictions_folds.append(test_predictions_fold)\n",
    "\n",
    "        fold_meta_features_train[:, i] = fold_predictions\n",
    "        fold_meta_features_test[:, i] = np.mean(test_predictions_folds, axis=0)\n",
    "        print(f'Model training done {i}')\n",
    "\n",
    "    meta_features_train.append(fold_meta_features_train)\n",
    "    meta_features_test.append(fold_meta_features_test)\n",
    "\n",
    "meta_features_train = np.hstack(meta_features_train)\n",
    "meta_features_test = np.hstack(meta_features_test)\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print(\"Dimensions of meta_features_train:\", meta_features_train.shape)\n",
    "print(\"Dimensions of meta_features_test:\", meta_features_test.shape)\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Stage 1 completed')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Stage 2: Training meta-learners with 5-fold CV')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "\n",
    "\n",
    "results = {}\n",
    "predictions = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=101)\n",
    "\n",
    "for model in models_meta:\n",
    "    model_name = model.__class__.__name__\n",
    "    predictions_train = []\n",
    "    actual_y_train = []\n",
    "    test_predictions_folds = []\n",
    "\n",
    "    for train_index, val_index in kf.split(meta_features_train):\n",
    "        X_fold_train, X_fold_val = meta_features_train[train_index], meta_features_train[val_index]\n",
    "        y_fold_train, y_fold_val = y_weak.iloc[train_index], y_weak.iloc[val_index]\n",
    "\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "        y_pred_fold = model.predict(X_fold_val)\n",
    "        y_pred_fold = np.clip(y_pred_fold, -10, -3.9)\n",
    "\n",
    "        predictions_train.extend(y_pred_fold)\n",
    "        actual_y_train.extend(y_fold_val)\n",
    "\n",
    "        test_predictions_fold = model.predict(meta_features_test)\n",
    "        test_predictions_fold = np.clip(test_predictions_fold, -10, -3.9)\n",
    "        test_predictions_folds.append(test_predictions_fold)\n",
    "\n",
    "    predictions_test_mean = np.mean(test_predictions_folds, axis=0)\n",
    "    predictions_test_std = np.std(test_predictions_folds, axis=0)\n",
    "\n",
    "    mse_train = mean_squared_error(actual_y_train, predictions_train)\n",
    "    mae_train = mean_absolute_error(actual_y_train, predictions_train)\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    r2_train = r2_score(actual_y_train, predictions_train)\n",
    "    pearson_train, _ = pearsonr(actual_y_train, predictions_train)\n",
    "    spearman_train, _ = spearmanr(actual_y_train, predictions_train)\n",
    "\n",
    "    \n",
    "    mse_test = mean_squared_error(y_eval, predictions_test_mean)\n",
    "    mae_test = mean_absolute_error(y_eval, predictions_test_mean)\n",
    "    rmse_test = np.sqrt(mse_test)\n",
    "    r2_test = r2_score(y_eval, predictions_test_mean)\n",
    "    pearson_test, _ = pearsonr(y_eval, predictions_test_mean)\n",
    "    spearman_test, _ = spearmanr(y_eval, predictions_test_mean)\n",
    "\n",
    "    print(f'{model_name} Evaluation completed: Test R2 score: {r2_test:.4f}, Test RMSE: {rmse_test:.4f}')\n",
    "\n",
    "    predictions.append({\n",
    "        'Model': model_name,\n",
    "        'Y Train pred': predictions_train,\n",
    "        'Y Train actual': actual_y_train,\n",
    "        'Y Test actual': y_eval,\n",
    "        'Test Predictions folds': test_predictions_folds,\n",
    "        'Test Predictions Mean': predictions_test_mean,\n",
    "        'Test Predictions Std': predictions_test_std,\n",
    "    })\n",
    "\n",
    "    results[model_name] = {\n",
    "        'Train MSE (5 fold CV)': mse_train,\n",
    "        'Train MAE (5 fold CV)': mae_train,\n",
    "        'Train RMSE (5 fold CV)': rmse_train,\n",
    "        'Train R2 (5 fold CV)': r2_train,\n",
    "        'Train PCC (5 fold CV)': pearson_train,\n",
    "        'Train SCC (5 fold CV)': spearman_train,\n",
    "        'Test MSE': mse_test,\n",
    "        'Test MAE': mae_test,\n",
    "        'Test RMSE': rmse_test,\n",
    "        'Test R2': r2_test,\n",
    "        'Test PCC': pearson_test,\n",
    "        'Test SCC': spearman_test,\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Stacked Ensemble Training and Evaluation complete')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cb84e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('/home/users/akshay/PCPpred/PAMPA/Results/Stacked_architecture/Results_5_fold_stacked_prediction_LVR_scaled.csv')\n",
    "predictions_df.to_csv('/home/users/akshay/PCPpred/PAMPA/Results/Stacked_architecture/Prediction_data_Meta_learners_5_fold_stacked_prediction_LVR_scaled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bb97d0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Stage 1: Training weak learners with 10-fold CV\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataframe pairs:   0%|                                                                 | 0/4 [00:00<?, ?it/s]\n",
      "Training models:   0%|                                                                           | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007584 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56912\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.747671\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56937\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.741826\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007441 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56933\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.740654\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008175 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56935\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.749020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008335 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56925\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.737478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007423 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56928\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.747212\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007919 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56933\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.750687\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008044 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56914\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.743084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007540 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56925\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.737254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007556 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56933\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  10%|██████▋                                                            | 1/10 [00:07<01:09,  7.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  20%|█████████████▍                                                     | 2/10 [00:21<01:31, 11.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  30%|███████████████████▊                                              | 3/10 [04:30<13:58, 119.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  40%|██████████████████████████▍                                       | 4/10 [05:45<10:12, 102.08s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  50%|█████████████████████████████████▌                                 | 5/10 [05:52<05:39, 68.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  60%|████████████████████████████████████████▏                          | 6/10 [05:58<03:06, 46.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  70%|██████████████████████████████████████████████▉                    | 7/10 [05:59<01:35, 31.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  80%|█████████████████████████████████████████████████████▌             | 8/10 [06:29<01:02, 31.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  90%|████████████████████████████████████████████████████████████▎      | 9/10 [06:54<00:29, 29.20s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models: 100%|██████████████████████████████████████████████████████████████████| 10/10 [07:05<00:00, 42.51s/it]\u001b[A\n",
      "Processing dataframe pairs:  25%|██████████████                                          | 1/4 [07:05<21:15, 425.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:   0%|                                                                           | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4196\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.747671\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042139 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4207\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.741826\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041590 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4200\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.740654\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4188\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1103\n",
      "[LightGBM] [Info] Start training from score -5.749020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041688 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4196\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.737478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041533 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4197\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.747212\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042365 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4200\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.750687\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041867 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4199\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.743084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042790 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4192\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 1104\n",
      "[LightGBM] [Info] Start training from score -5.737254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042687 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4189\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  10%|██████▋                                                            | 1/10 [00:10<01:35, 10.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  20%|█████████████▍                                                     | 2/10 [00:17<01:05,  8.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  30%|████████████████████                                               | 3/10 [01:26<04:14, 36.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  40%|██████████████████████████▊                                        | 4/10 [02:23<04:26, 44.34s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  50%|█████████████████████████████████▌                                 | 5/10 [02:31<02:35, 31.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  60%|████████████████████████████████████████▏                          | 6/10 [02:36<01:29, 22.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  70%|██████████████████████████████████████████████▉                    | 7/10 [02:38<00:46, 15.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  80%|█████████████████████████████████████████████████████▌             | 8/10 [04:19<01:25, 42.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  90%|████████████████████████████████████████████████████████████▎      | 9/10 [05:11<00:45, 45.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models: 100%|██████████████████████████████████████████████████████████████████| 10/10 [05:14<00:00, 31.47s/it]\u001b[A\n",
      "Processing dataframe pairs:  50%|████████████████████████████                            | 2/4 [12:19<12:00, 360.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:   0%|                                                                           | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052019 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.747671\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017737 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.741826\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017391 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.740654\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018109 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.749020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017490 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.737478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.747212\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017522 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.750687\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.743084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017691 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.737254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017702 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  10%|██████▋                                                            | 1/10 [00:08<01:13,  8.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  20%|█████████████▍                                                     | 2/10 [01:02<04:41, 35.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  30%|███████████████████▏                                            | 3/10 [24:22<1:16:47, 658.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  40%|██████████████████████████▍                                       | 4/10 [29:34<52:10, 521.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  50%|█████████████████████████████████                                 | 5/10 [29:58<28:31, 342.20s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  60%|███████████████████████████████████████▌                          | 6/10 [30:11<15:21, 230.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  70%|██████████████████████████████████████████████▏                   | 7/10 [30:12<07:46, 155.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  80%|████████████████████████████████████████████████████▊             | 8/10 [31:14<04:11, 125.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  90%|███████████████████████████████████████████████████████████▍      | 9/10 [32:00<01:40, 100.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models: 100%|█████████████████████████████████████████████████████████████████| 10/10 [32:55<00:00, 197.55s/it]\u001b[A\n",
      "Processing dataframe pairs:  75%|█████████████████████████████████████████▎             | 3/4 [45:15<18:17, 1097.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:   0%|                                                                           | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.747671\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.741826\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000728 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 151\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.740654\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000744 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 150\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.749020\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000746 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.737478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000739 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.747212\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.750687\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.743084\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000749 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.737254\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000762 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 151\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  10%|██████▋                                                            | 1/10 [00:05<00:45,  5.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  20%|█████████████▍                                                     | 2/10 [00:08<00:34,  4.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  30%|████████████████████                                               | 3/10 [00:10<00:20,  2.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  40%|██████████████████████████▊                                        | 4/10 [00:10<00:12,  2.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  50%|█████████████████████████████████▌                                 | 5/10 [00:12<00:09,  1.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  60%|████████████████████████████████████████▏                          | 6/10 [00:15<00:09,  2.32s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  70%|██████████████████████████████████████████████▉                    | 7/10 [00:16<00:05,  1.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  80%|█████████████████████████████████████████████████████▌             | 8/10 [00:23<00:07,  3.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models: 100%|██████████████████████████████████████████████████████████████████| 10/10 [00:39<00:00,  3.95s/it]\u001b[A\n",
      "Processing dataframe pairs: 100%|████████████████████████████████████████████████████████| 4/4 [45:54<00:00, 688.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8\n",
      "Model training done 9\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dimensions of meta_features_train: (5568, 40)\n",
      "Dimensions of meta_features_test: (1392, 40)\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Stage 1 completed\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Stage 2: Training meta-learners with 10-fold CV\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082880 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10102\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.747671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001634 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10101\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.741826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001578 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10095\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.740654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001560 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10097\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.749020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001415 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10093\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.737478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001613 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10092\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.747212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001415 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10094\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.750687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001645 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10097\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.743084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001667 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10091\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.737254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001623 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10097\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor Evaluation completed: Test R2 score: 0.6957, Test RMSE: 0.4388\n",
      "DecisionTreeRegressor Evaluation completed: Test R2 score: 0.6612, Test RMSE: 0.4631\n",
      "RandomForestRegressor Evaluation completed: Test R2 score: 0.6907, Test RMSE: 0.4425\n",
      "GradientBoostingRegressor Evaluation completed: Test R2 score: 0.6965, Test RMSE: 0.4382\n",
      "AdaBoostRegressor Evaluation completed: Test R2 score: 0.6657, Test RMSE: 0.4600\n",
      "XGBRegressor Evaluation completed: Test R2 score: 0.6870, Test RMSE: 0.4451\n",
      "ExtraTreesRegressor Evaluation completed: Test R2 score: 0.6928, Test RMSE: 0.4409\n",
      "LinearRegression Evaluation completed: Test R2 score: 0.6941, Test RMSE: 0.4400\n",
      "KNeighborsRegressor Evaluation completed: Test R2 score: 0.6817, Test RMSE: 0.4488\n",
      "SVR Evaluation completed: Test R2 score: 0.6933, Test RMSE: 0.4406\n",
      "MLPRegressor Evaluation completed: Test R2 score: 0.6903, Test RMSE: 0.4427\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Stacked Ensemble Training and Evaluation complete\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train MSE (10 fold CV)</th>\n",
       "      <th>Train MAE (10 fold CV)</th>\n",
       "      <th>Train RMSE (10 fold CV)</th>\n",
       "      <th>Train R2 (10 fold CV)</th>\n",
       "      <th>Train PCC (10 fold CV)</th>\n",
       "      <th>Train SCC (10 fold CV)</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Test MAE</th>\n",
       "      <th>Test RMSE</th>\n",
       "      <th>Test R2</th>\n",
       "      <th>Test PCC</th>\n",
       "      <th>Test SCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <td>0.124649</td>\n",
       "      <td>0.257625</td>\n",
       "      <td>0.353057</td>\n",
       "      <td>0.799860</td>\n",
       "      <td>0.894369</td>\n",
       "      <td>0.877403</td>\n",
       "      <td>0.192554</td>\n",
       "      <td>0.307291</td>\n",
       "      <td>0.438810</td>\n",
       "      <td>0.695725</td>\n",
       "      <td>0.834227</td>\n",
       "      <td>0.820651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>0.265595</td>\n",
       "      <td>0.376011</td>\n",
       "      <td>0.515359</td>\n",
       "      <td>0.573552</td>\n",
       "      <td>0.789093</td>\n",
       "      <td>0.764616</td>\n",
       "      <td>0.214418</td>\n",
       "      <td>0.323827</td>\n",
       "      <td>0.463053</td>\n",
       "      <td>0.661175</td>\n",
       "      <td>0.815084</td>\n",
       "      <td>0.801343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>0.126932</td>\n",
       "      <td>0.258329</td>\n",
       "      <td>0.356275</td>\n",
       "      <td>0.796194</td>\n",
       "      <td>0.892322</td>\n",
       "      <td>0.875365</td>\n",
       "      <td>0.195763</td>\n",
       "      <td>0.309286</td>\n",
       "      <td>0.442452</td>\n",
       "      <td>0.690653</td>\n",
       "      <td>0.831321</td>\n",
       "      <td>0.815909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>0.124790</td>\n",
       "      <td>0.256373</td>\n",
       "      <td>0.353256</td>\n",
       "      <td>0.799633</td>\n",
       "      <td>0.894225</td>\n",
       "      <td>0.876869</td>\n",
       "      <td>0.192059</td>\n",
       "      <td>0.308014</td>\n",
       "      <td>0.438245</td>\n",
       "      <td>0.696507</td>\n",
       "      <td>0.834725</td>\n",
       "      <td>0.820596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostRegressor</th>\n",
       "      <td>0.168182</td>\n",
       "      <td>0.315297</td>\n",
       "      <td>0.410101</td>\n",
       "      <td>0.729961</td>\n",
       "      <td>0.864573</td>\n",
       "      <td>0.852041</td>\n",
       "      <td>0.211558</td>\n",
       "      <td>0.345272</td>\n",
       "      <td>0.459955</td>\n",
       "      <td>0.665694</td>\n",
       "      <td>0.823699</td>\n",
       "      <td>0.812900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBRegressor</th>\n",
       "      <td>0.140547</td>\n",
       "      <td>0.273822</td>\n",
       "      <td>0.374896</td>\n",
       "      <td>0.774333</td>\n",
       "      <td>0.880612</td>\n",
       "      <td>0.862136</td>\n",
       "      <td>0.198070</td>\n",
       "      <td>0.311171</td>\n",
       "      <td>0.445051</td>\n",
       "      <td>0.687008</td>\n",
       "      <td>0.829228</td>\n",
       "      <td>0.815823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesRegressor</th>\n",
       "      <td>0.125389</td>\n",
       "      <td>0.256109</td>\n",
       "      <td>0.354103</td>\n",
       "      <td>0.798672</td>\n",
       "      <td>0.893708</td>\n",
       "      <td>0.876617</td>\n",
       "      <td>0.194397</td>\n",
       "      <td>0.309130</td>\n",
       "      <td>0.440905</td>\n",
       "      <td>0.692812</td>\n",
       "      <td>0.832603</td>\n",
       "      <td>0.816472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>0.124789</td>\n",
       "      <td>0.255568</td>\n",
       "      <td>0.353255</td>\n",
       "      <td>0.799634</td>\n",
       "      <td>0.894226</td>\n",
       "      <td>0.877550</td>\n",
       "      <td>0.193594</td>\n",
       "      <td>0.308208</td>\n",
       "      <td>0.439993</td>\n",
       "      <td>0.694081</td>\n",
       "      <td>0.833284</td>\n",
       "      <td>0.817977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsRegressor</th>\n",
       "      <td>0.151814</td>\n",
       "      <td>0.285390</td>\n",
       "      <td>0.389633</td>\n",
       "      <td>0.756243</td>\n",
       "      <td>0.869643</td>\n",
       "      <td>0.850880</td>\n",
       "      <td>0.201411</td>\n",
       "      <td>0.321141</td>\n",
       "      <td>0.448789</td>\n",
       "      <td>0.681728</td>\n",
       "      <td>0.826085</td>\n",
       "      <td>0.804292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>0.133759</td>\n",
       "      <td>0.261469</td>\n",
       "      <td>0.365730</td>\n",
       "      <td>0.785233</td>\n",
       "      <td>0.886403</td>\n",
       "      <td>0.875806</td>\n",
       "      <td>0.194117</td>\n",
       "      <td>0.305681</td>\n",
       "      <td>0.440587</td>\n",
       "      <td>0.693255</td>\n",
       "      <td>0.833838</td>\n",
       "      <td>0.819023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPRegressor</th>\n",
       "      <td>0.140764</td>\n",
       "      <td>0.273675</td>\n",
       "      <td>0.375185</td>\n",
       "      <td>0.773986</td>\n",
       "      <td>0.883286</td>\n",
       "      <td>0.864447</td>\n",
       "      <td>0.195986</td>\n",
       "      <td>0.309455</td>\n",
       "      <td>0.442703</td>\n",
       "      <td>0.690301</td>\n",
       "      <td>0.835183</td>\n",
       "      <td>0.820859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Train MSE (10 fold CV)  Train MAE (10 fold CV)  \\\n",
       "LGBMRegressor                            0.124649                0.257625   \n",
       "DecisionTreeRegressor                    0.265595                0.376011   \n",
       "RandomForestRegressor                    0.126932                0.258329   \n",
       "GradientBoostingRegressor                0.124790                0.256373   \n",
       "AdaBoostRegressor                        0.168182                0.315297   \n",
       "XGBRegressor                             0.140547                0.273822   \n",
       "ExtraTreesRegressor                      0.125389                0.256109   \n",
       "LinearRegression                         0.124789                0.255568   \n",
       "KNeighborsRegressor                      0.151814                0.285390   \n",
       "SVR                                      0.133759                0.261469   \n",
       "MLPRegressor                             0.140764                0.273675   \n",
       "\n",
       "                           Train RMSE (10 fold CV)  Train R2 (10 fold CV)  \\\n",
       "LGBMRegressor                             0.353057               0.799860   \n",
       "DecisionTreeRegressor                     0.515359               0.573552   \n",
       "RandomForestRegressor                     0.356275               0.796194   \n",
       "GradientBoostingRegressor                 0.353256               0.799633   \n",
       "AdaBoostRegressor                         0.410101               0.729961   \n",
       "XGBRegressor                              0.374896               0.774333   \n",
       "ExtraTreesRegressor                       0.354103               0.798672   \n",
       "LinearRegression                          0.353255               0.799634   \n",
       "KNeighborsRegressor                       0.389633               0.756243   \n",
       "SVR                                       0.365730               0.785233   \n",
       "MLPRegressor                              0.375185               0.773986   \n",
       "\n",
       "                           Train PCC (10 fold CV)  Train SCC (10 fold CV)  \\\n",
       "LGBMRegressor                            0.894369                0.877403   \n",
       "DecisionTreeRegressor                    0.789093                0.764616   \n",
       "RandomForestRegressor                    0.892322                0.875365   \n",
       "GradientBoostingRegressor                0.894225                0.876869   \n",
       "AdaBoostRegressor                        0.864573                0.852041   \n",
       "XGBRegressor                             0.880612                0.862136   \n",
       "ExtraTreesRegressor                      0.893708                0.876617   \n",
       "LinearRegression                         0.894226                0.877550   \n",
       "KNeighborsRegressor                      0.869643                0.850880   \n",
       "SVR                                      0.886403                0.875806   \n",
       "MLPRegressor                             0.883286                0.864447   \n",
       "\n",
       "                           Test MSE  Test MAE  Test RMSE   Test R2  Test PCC  \\\n",
       "LGBMRegressor              0.192554  0.307291   0.438810  0.695725  0.834227   \n",
       "DecisionTreeRegressor      0.214418  0.323827   0.463053  0.661175  0.815084   \n",
       "RandomForestRegressor      0.195763  0.309286   0.442452  0.690653  0.831321   \n",
       "GradientBoostingRegressor  0.192059  0.308014   0.438245  0.696507  0.834725   \n",
       "AdaBoostRegressor          0.211558  0.345272   0.459955  0.665694  0.823699   \n",
       "XGBRegressor               0.198070  0.311171   0.445051  0.687008  0.829228   \n",
       "ExtraTreesRegressor        0.194397  0.309130   0.440905  0.692812  0.832603   \n",
       "LinearRegression           0.193594  0.308208   0.439993  0.694081  0.833284   \n",
       "KNeighborsRegressor        0.201411  0.321141   0.448789  0.681728  0.826085   \n",
       "SVR                        0.194117  0.305681   0.440587  0.693255  0.833838   \n",
       "MLPRegressor               0.195986  0.309455   0.442703  0.690301  0.835183   \n",
       "\n",
       "                           Test SCC  \n",
       "LGBMRegressor              0.820651  \n",
       "DecisionTreeRegressor      0.801343  \n",
       "RandomForestRegressor      0.815909  \n",
       "GradientBoostingRegressor  0.820596  \n",
       "AdaBoostRegressor          0.812900  \n",
       "XGBRegressor               0.815823  \n",
       "ExtraTreesRegressor        0.816472  \n",
       "LinearRegression           0.817977  \n",
       "KNeighborsRegressor        0.804292  \n",
       "SVR                        0.819023  \n",
       "MLPRegressor               0.820859  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_weak = [\n",
    "    lgb.LGBMRegressor(objective='regression', metric='rmse', boosting_type='gbdt', num_leaves=31, learning_rate=0.05, random_state=101),\n",
    "    RandomForestRegressor(n_jobs=-1, random_state=101),\n",
    "    GradientBoostingRegressor(random_state=101),\n",
    "    AdaBoostRegressor(random_state=101),\n",
    "    xgb.XGBRegressor(random_state=101, eval_metric='rmse'),\n",
    "    ExtraTreesRegressor(n_jobs=-1, n_estimators=100, random_state=101),\n",
    "    KNeighborsRegressor(),\n",
    "    SVR(),\n",
    "    MLPRegressor(random_state=101, max_iter=500),\n",
    "    DecisionTreeRegressor(random_state=101)\n",
    "]\n",
    "\n",
    "models_meta = [\n",
    "    lgb.LGBMRegressor(objective='regression', metric='rmse', boosting_type='gbdt', num_leaves=31, learning_rate=0.05, random_state=101),\n",
    "    DecisionTreeRegressor(random_state=101),\n",
    "    RandomForestRegressor(n_jobs=-1, random_state=101),\n",
    "    GradientBoostingRegressor(random_state=101),\n",
    "    AdaBoostRegressor(random_state=101),\n",
    "    xgb.XGBRegressor(random_state=101, eval_metric='rmse'),\n",
    "    ExtraTreesRegressor(n_jobs=-1, n_estimators=100, random_state=101),\n",
    "    LinearRegression(),\n",
    "    KNeighborsRegressor(),\n",
    "    SVR(),\n",
    "    MLPRegressor(random_state=101, max_iter=500)\n",
    "]\n",
    "target_column = 'Permeability'\n",
    "dataframes = [(df_desc_train, df_desc_test), (df_fp_train, df_fp_test), (df_emb_train, df_emb_test), (df_atomic_train, df_atomic_test)]\n",
    "\n",
    "meta_features_train = []\n",
    "meta_features_test = []\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Stage 1: Training weak learners with 10-fold CV')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "\n",
    "# Stage 1: Train weak learners with 10-fold cross-validation\n",
    "for df_train, df_test in tqdm(dataframes, desc=\"Processing dataframe pairs\"):\n",
    "    X_weak = df_train.drop(columns=['ID', 'SMILES', target_column])\n",
    "    y_weak = df_train[target_column]\n",
    "    X_eval = df_test.drop(columns=['ID', 'SMILES', target_column])\n",
    "    y_eval = df_test[target_column]\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=101)\n",
    "\n",
    "    fold_meta_features_train = np.zeros((X_weak.shape[0], len(models_weak)))\n",
    "    fold_meta_features_test = np.zeros((X_eval.shape[0], len(models_weak)))\n",
    "\n",
    "    for i, model in tqdm(enumerate(models_weak), desc=\"Training models\", total=len(models_weak)):\n",
    "        fold_predictions = np.zeros(X_weak.shape[0])\n",
    "        test_predictions_folds = []\n",
    "\n",
    "        for train_index, val_index in kf.split(X_weak):\n",
    "            X_train, X_val = X_weak.iloc[train_index], X_weak.iloc[val_index]\n",
    "            y_train, y_val = y_weak.iloc[train_index], y_weak.iloc[val_index]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            fold_predictions[val_index] = np.clip(model.predict(X_val), -10, -3.9)\n",
    "            test_predictions_fold = np.clip(model.predict(X_eval), -10, -3.9)\n",
    "            test_predictions_folds.append(test_predictions_fold)\n",
    "\n",
    "        fold_meta_features_train[:, i] = fold_predictions\n",
    "        fold_meta_features_test[:, i] = np.mean(test_predictions_folds, axis=0)\n",
    "        print(f'Model training done {i}')\n",
    "\n",
    "    meta_features_train.append(fold_meta_features_train)\n",
    "    meta_features_test.append(fold_meta_features_test)\n",
    "\n",
    "meta_features_train = np.hstack(meta_features_train)\n",
    "meta_features_test = np.hstack(meta_features_test)\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print(\"Dimensions of meta_features_train:\", meta_features_train.shape)\n",
    "print(\"Dimensions of meta_features_test:\", meta_features_test.shape)\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Stage 1 completed')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Stage 2: Training meta-learners with 10-fold CV')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "\n",
    "\n",
    "results = {}\n",
    "predictions = []\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=101)\n",
    "\n",
    "for model in models_meta:\n",
    "    model_name = model.__class__.__name__\n",
    "    predictions_train = []\n",
    "    actual_y_train = []\n",
    "    test_predictions_folds = []\n",
    "\n",
    "    for train_index, val_index in kf.split(meta_features_train):\n",
    "        X_fold_train, X_fold_val = meta_features_train[train_index], meta_features_train[val_index]\n",
    "        y_fold_train, y_fold_val = y_weak.iloc[train_index], y_weak.iloc[val_index]\n",
    "\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "        y_pred_fold = model.predict(X_fold_val)\n",
    "        y_pred_fold = np.clip(y_pred_fold, -10, -3.9)\n",
    "\n",
    "        predictions_train.extend(y_pred_fold)\n",
    "        actual_y_train.extend(y_fold_val)\n",
    "\n",
    "        test_predictions_fold = model.predict(meta_features_test)\n",
    "        test_predictions_fold = np.clip(test_predictions_fold, -10, -3.9)\n",
    "        test_predictions_folds.append(test_predictions_fold)\n",
    "\n",
    "    predictions_test_mean = np.mean(test_predictions_folds, axis=0)\n",
    "    predictions_test_std = np.std(test_predictions_folds, axis=0)\n",
    "\n",
    "    mse_train = mean_squared_error(actual_y_train, predictions_train)\n",
    "    mae_train = mean_absolute_error(actual_y_train, predictions_train)\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    r2_train = r2_score(actual_y_train, predictions_train)\n",
    "    pearson_train, _ = pearsonr(actual_y_train, predictions_train)\n",
    "    spearman_train, _ = spearmanr(actual_y_train, predictions_train)\n",
    "\n",
    "    \n",
    "    mse_test = mean_squared_error(y_eval, predictions_test_mean)\n",
    "    mae_test = mean_absolute_error(y_eval, predictions_test_mean)\n",
    "    rmse_test = np.sqrt(mse_test)\n",
    "    r2_test = r2_score(y_eval, predictions_test_mean)\n",
    "    pearson_test, _ = pearsonr(y_eval, predictions_test_mean)\n",
    "    spearman_test, _ = spearmanr(y_eval, predictions_test_mean)\n",
    "\n",
    "    print(f'{model_name} Evaluation completed: Test R2 score: {r2_test:.4f}, Test RMSE: {rmse_test:.4f}')\n",
    "\n",
    "    predictions.append({\n",
    "        'Model': model_name,\n",
    "        'Y Train pred': predictions_train,\n",
    "        'Y Train actual': actual_y_train,\n",
    "        'Y Test actual': y_eval,\n",
    "        'Test Predictions folds': test_predictions_folds,\n",
    "        'Test Predictions Mean': predictions_test_mean,\n",
    "        'Test Predictions Std': predictions_test_std,\n",
    "    })\n",
    "\n",
    "    results[model_name] = {\n",
    "        'Train MSE (10 fold CV)': mse_train,\n",
    "        'Train MAE (10 fold CV)': mae_train,\n",
    "        'Train RMSE (10 fold CV)': rmse_train,\n",
    "        'Train R2 (10 fold CV)': r2_train,\n",
    "        'Train PCC (10 fold CV)': pearson_train,\n",
    "        'Train SCC (10 fold CV)': spearman_train,\n",
    "        'Test MSE': mse_test,\n",
    "        'Test MAE': mae_test,\n",
    "        'Test RMSE': rmse_test,\n",
    "        'Test R2': r2_test,\n",
    "        'Test PCC': pearson_test,\n",
    "        'Test SCC': spearman_test,\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Stacked Ensemble Training and Evaluation complete')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce7170f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('/home/users/akshay/PCPpred/PAMPA/Results/Stacked_architecture/Results_10_fold_stacked_prediction_LVR_scaled.csv')\n",
    "predictions_df.to_csv('/home/users/akshay/PCPpred/PAMPA/Results/Stacked_architecture/Prediction_data_Meta_learners_10_fold_stacked_prediction_LVR_scaled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ff697ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Stage 1: Training weak learners with 20-fold CV\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataframe pairs:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Training models:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 56958\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.747281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56944\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.743197\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007794 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56956\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.740467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007893 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56953\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.744446\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007838 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56954\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.740965\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008751 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56954\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.742240\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56952\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.748642\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008141 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56955\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.743284\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007900 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56955\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.739611\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56950\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.740964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007745 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56951\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.744793\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56962\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.745041\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007805 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56958\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.747061\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56947\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.746083\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56956\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.740864\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008094 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56944\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.745182\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008162 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56947\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.739569\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008357 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56951\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.740887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007510 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56958\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.741020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007881 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56953\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.736521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  10%|█         | 1/10 [00:18<02:45, 18.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  20%|██        | 2/10 [00:47<03:17, 24.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  30%|███       | 3/10 [09:32<29:30, 252.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  40%|████      | 4/10 [12:09<21:31, 215.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  50%|█████     | 5/10 [12:26<11:58, 143.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  60%|██████    | 6/10 [12:37<06:34, 98.61s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  70%|███████   | 7/10 [12:40<03:21, 67.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  80%|████████  | 8/10 [13:38<02:09, 64.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  90%|█████████ | 9/10 [14:37<01:02, 62.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models: 100%|██████████| 10/10 [14:59<00:00, 89.99s/it]\u001b[A\n",
      "Processing dataframe pairs:  25%|██▌       | 1/4 [14:59<44:59, 899.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041889 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4209\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 1107\n",
      "[LightGBM] [Info] Start training from score -5.747281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041175 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4204\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.743197\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041739 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4211\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.740467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040563 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4214\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 1107\n",
      "[LightGBM] [Info] Start training from score -5.744446\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041124 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4214\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 1107\n",
      "[LightGBM] [Info] Start training from score -5.740965\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042216 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4212\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 1107\n",
      "[LightGBM] [Info] Start training from score -5.742240\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4213\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 1107\n",
      "[LightGBM] [Info] Start training from score -5.748642\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042727 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4199\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.743284\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4212\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 1107\n",
      "[LightGBM] [Info] Start training from score -5.739611\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041741 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4206\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.740964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4207\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 1107\n",
      "[LightGBM] [Info] Start training from score -5.744793\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041460 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4206\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.745041\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042527 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4211\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.747061\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4211\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 1107\n",
      "[LightGBM] [Info] Start training from score -5.746083\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040783 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4210\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 1107\n",
      "[LightGBM] [Info] Start training from score -5.740864\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040940 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4209\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.745182\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4212\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 1107\n",
      "[LightGBM] [Info] Start training from score -5.739569\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042848 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4207\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.740887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043139 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4202\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.741020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041661 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4205\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 1107\n",
      "[LightGBM] [Info] Start training from score -5.736521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  10%|█         | 1/10 [00:22<03:20, 22.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  20%|██        | 2/10 [00:34<02:11, 16.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  30%|███       | 3/10 [03:03<08:58, 76.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  40%|████      | 4/10 [04:59<09:13, 92.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  50%|█████     | 5/10 [05:14<05:22, 64.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  60%|██████    | 6/10 [05:26<03:07, 46.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  70%|███████   | 7/10 [05:29<01:37, 32.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  80%|████████  | 8/10 [08:55<02:55, 87.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  90%|█████████ | 9/10 [10:30<01:29, 89.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models: 100%|██████████| 10/10 [10:36<00:00, 63.67s/it]\u001b[A\n",
      "Processing dataframe pairs:  50%|█████     | 2/4 [25:36<24:50, 745.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073951 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.747281\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.743197\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.740467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017261 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.744446\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017100 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.740965\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017188 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.742240\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.748642\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017321 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.743284\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017029 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.739611\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017184 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.740964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.744793\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.745041\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017111 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.747061\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017330 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.746083\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017553 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.740864\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017006 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.745182\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016852 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.739569\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.740887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016681 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.741020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016965 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.736521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  10%|█         | 1/10 [00:15<02:16, 15.20s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  20%|██        | 2/10 [02:16<10:20, 77.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  30%|███       | 3/10 [51:55<2:43:36, 1402.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  40%|████      | 4/10 [1:02:57<1:51:00, 1110.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  50%|█████     | 5/10 [1:03:46<1:00:37, 727.43s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  60%|██████    | 6/10 [1:04:14<32:38, 489.64s/it]  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  70%|███████   | 7/10 [1:04:16<16:31, 330.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  80%|████████  | 8/10 [1:06:24<08:51, 265.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  90%|█████████ | 9/10 [1:08:04<03:33, 213.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models: 100%|██████████| 10/10 [1:10:00<00:00, 420.09s/it]\u001b[A\n",
      "Processing dataframe pairs:  75%|███████▌  | 3/4 [1:35:37<38:43, 2323.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.747281\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000746 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.743197\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000755 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.740467\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.744446\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000735 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.740965\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000753 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.742240\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000729 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 151\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.748642\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.743284\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.739611\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.740964\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000744 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.744793\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.745041\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.747061\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.746083\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.740864\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.745182\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.739569\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.740887\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.741020\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000739 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.736521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  10%|█         | 1/10 [00:10<01:38, 10.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  20%|██        | 2/10 [00:18<01:12,  9.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  30%|███       | 3/10 [00:21<00:43,  6.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  40%|████      | 4/10 [00:23<00:26,  4.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  50%|█████     | 5/10 [00:26<00:20,  4.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  60%|██████    | 6/10 [00:32<00:18,  4.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  70%|███████   | 7/10 [00:33<00:10,  3.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  80%|████████  | 8/10 [00:49<00:14,  7.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  90%|█████████ | 9/10 [01:19<00:14, 14.67s/it]\u001b[A\n",
      "Training models: 100%|██████████| 10/10 [01:19<00:00,  7.99s/it]\u001b[A\n",
      "Processing dataframe pairs: 100%|██████████| 4/4 [1:36:57<00:00, 1454.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8\n",
      "Model training done 9\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dimensions of meta_features_train: (5568, 40)\n",
      "Dimensions of meta_features_test: (1392, 40)\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Stage 1 completed\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Stage 2: Training meta-learners with 20-fold CV\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001962 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10160\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.747281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001613 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10163\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.743197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001606 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10160\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.740467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001660 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10161\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.744446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001612 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10159\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.740965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001555 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10160\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.742240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001646 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10161\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.748642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001571 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10160\n",
      "[LightGBM] [Info] Number of data points in the train set: 5289, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.743284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001635 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10161\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.739611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001604 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10160\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.740964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001557 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10158\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.744793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10159\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.745041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001591 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10163\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.747061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001609 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10160\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.746083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001562 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10163\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.740864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001570 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10158\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.745182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001584 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10161\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.739569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10162\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.740887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001608 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10163\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.741020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001568 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10164\n",
      "[LightGBM] [Info] Number of data points in the train set: 5290, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.736521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor Evaluation completed: Test R2 score: 0.6972, Test RMSE: 0.4377\n",
      "DecisionTreeRegressor Evaluation completed: Test R2 score: 0.6674, Test RMSE: 0.4588\n",
      "RandomForestRegressor Evaluation completed: Test R2 score: 0.6926, Test RMSE: 0.4411\n",
      "GradientBoostingRegressor Evaluation completed: Test R2 score: 0.6952, Test RMSE: 0.4392\n",
      "AdaBoostRegressor Evaluation completed: Test R2 score: 0.6557, Test RMSE: 0.4668\n",
      "XGBRegressor Evaluation completed: Test R2 score: 0.6931, Test RMSE: 0.4407\n",
      "ExtraTreesRegressor Evaluation completed: Test R2 score: 0.6942, Test RMSE: 0.4399\n",
      "LinearRegression Evaluation completed: Test R2 score: 0.6962, Test RMSE: 0.4385\n",
      "KNeighborsRegressor Evaluation completed: Test R2 score: 0.6772, Test RMSE: 0.4520\n",
      "SVR Evaluation completed: Test R2 score: 0.6925, Test RMSE: 0.4411\n",
      "MLPRegressor Evaluation completed: Test R2 score: 0.6987, Test RMSE: 0.4367\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Stacked Ensemble Training and Evaluation complete\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train MSE (20 fold CV)</th>\n",
       "      <th>Train MAE (20 fold CV)</th>\n",
       "      <th>Train RMSE (20 fold CV)</th>\n",
       "      <th>Train R2 (20 fold CV)</th>\n",
       "      <th>Train PCC (20 fold CV)</th>\n",
       "      <th>Train SCC (20 fold CV)</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Test MAE</th>\n",
       "      <th>Test RMSE</th>\n",
       "      <th>Test R2</th>\n",
       "      <th>Test PCC</th>\n",
       "      <th>Test SCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <td>0.123625</td>\n",
       "      <td>0.255786</td>\n",
       "      <td>0.351603</td>\n",
       "      <td>0.801504</td>\n",
       "      <td>0.895277</td>\n",
       "      <td>0.878916</td>\n",
       "      <td>0.191610</td>\n",
       "      <td>0.307490</td>\n",
       "      <td>0.437732</td>\n",
       "      <td>0.697217</td>\n",
       "      <td>0.835119</td>\n",
       "      <td>0.820597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>0.255956</td>\n",
       "      <td>0.370629</td>\n",
       "      <td>0.505921</td>\n",
       "      <td>0.589030</td>\n",
       "      <td>0.798817</td>\n",
       "      <td>0.770858</td>\n",
       "      <td>0.210502</td>\n",
       "      <td>0.323504</td>\n",
       "      <td>0.458805</td>\n",
       "      <td>0.667363</td>\n",
       "      <td>0.818505</td>\n",
       "      <td>0.798929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>0.125116</td>\n",
       "      <td>0.256122</td>\n",
       "      <td>0.353718</td>\n",
       "      <td>0.799109</td>\n",
       "      <td>0.893964</td>\n",
       "      <td>0.877420</td>\n",
       "      <td>0.194562</td>\n",
       "      <td>0.308341</td>\n",
       "      <td>0.441091</td>\n",
       "      <td>0.692552</td>\n",
       "      <td>0.832448</td>\n",
       "      <td>0.815556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>0.124204</td>\n",
       "      <td>0.256301</td>\n",
       "      <td>0.352426</td>\n",
       "      <td>0.800574</td>\n",
       "      <td>0.894748</td>\n",
       "      <td>0.877182</td>\n",
       "      <td>0.192875</td>\n",
       "      <td>0.307301</td>\n",
       "      <td>0.439175</td>\n",
       "      <td>0.695218</td>\n",
       "      <td>0.833927</td>\n",
       "      <td>0.819573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostRegressor</th>\n",
       "      <td>0.165833</td>\n",
       "      <td>0.311031</td>\n",
       "      <td>0.407225</td>\n",
       "      <td>0.733734</td>\n",
       "      <td>0.866184</td>\n",
       "      <td>0.858277</td>\n",
       "      <td>0.217909</td>\n",
       "      <td>0.347540</td>\n",
       "      <td>0.466807</td>\n",
       "      <td>0.655659</td>\n",
       "      <td>0.816905</td>\n",
       "      <td>0.811944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBRegressor</th>\n",
       "      <td>0.142237</td>\n",
       "      <td>0.274186</td>\n",
       "      <td>0.377143</td>\n",
       "      <td>0.771620</td>\n",
       "      <td>0.878973</td>\n",
       "      <td>0.862483</td>\n",
       "      <td>0.194220</td>\n",
       "      <td>0.307843</td>\n",
       "      <td>0.440704</td>\n",
       "      <td>0.693093</td>\n",
       "      <td>0.832948</td>\n",
       "      <td>0.818835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesRegressor</th>\n",
       "      <td>0.123368</td>\n",
       "      <td>0.254954</td>\n",
       "      <td>0.351238</td>\n",
       "      <td>0.801916</td>\n",
       "      <td>0.895523</td>\n",
       "      <td>0.877273</td>\n",
       "      <td>0.193505</td>\n",
       "      <td>0.308350</td>\n",
       "      <td>0.439892</td>\n",
       "      <td>0.694222</td>\n",
       "      <td>0.833482</td>\n",
       "      <td>0.816034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>0.123873</td>\n",
       "      <td>0.254337</td>\n",
       "      <td>0.351956</td>\n",
       "      <td>0.801106</td>\n",
       "      <td>0.895048</td>\n",
       "      <td>0.878397</td>\n",
       "      <td>0.192259</td>\n",
       "      <td>0.307641</td>\n",
       "      <td>0.438474</td>\n",
       "      <td>0.696190</td>\n",
       "      <td>0.834527</td>\n",
       "      <td>0.818674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsRegressor</th>\n",
       "      <td>0.148924</td>\n",
       "      <td>0.280575</td>\n",
       "      <td>0.385906</td>\n",
       "      <td>0.760884</td>\n",
       "      <td>0.872412</td>\n",
       "      <td>0.852664</td>\n",
       "      <td>0.204262</td>\n",
       "      <td>0.317270</td>\n",
       "      <td>0.451954</td>\n",
       "      <td>0.677223</td>\n",
       "      <td>0.824058</td>\n",
       "      <td>0.809096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>0.131766</td>\n",
       "      <td>0.259583</td>\n",
       "      <td>0.362996</td>\n",
       "      <td>0.788433</td>\n",
       "      <td>0.888171</td>\n",
       "      <td>0.877363</td>\n",
       "      <td>0.194604</td>\n",
       "      <td>0.305052</td>\n",
       "      <td>0.441139</td>\n",
       "      <td>0.692485</td>\n",
       "      <td>0.833372</td>\n",
       "      <td>0.818657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPRegressor</th>\n",
       "      <td>0.133672</td>\n",
       "      <td>0.266273</td>\n",
       "      <td>0.365612</td>\n",
       "      <td>0.785372</td>\n",
       "      <td>0.886431</td>\n",
       "      <td>0.866990</td>\n",
       "      <td>0.190683</td>\n",
       "      <td>0.306709</td>\n",
       "      <td>0.436673</td>\n",
       "      <td>0.698681</td>\n",
       "      <td>0.836082</td>\n",
       "      <td>0.821108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Train MSE (20 fold CV)  Train MAE (20 fold CV)  \\\n",
       "LGBMRegressor                            0.123625                0.255786   \n",
       "DecisionTreeRegressor                    0.255956                0.370629   \n",
       "RandomForestRegressor                    0.125116                0.256122   \n",
       "GradientBoostingRegressor                0.124204                0.256301   \n",
       "AdaBoostRegressor                        0.165833                0.311031   \n",
       "XGBRegressor                             0.142237                0.274186   \n",
       "ExtraTreesRegressor                      0.123368                0.254954   \n",
       "LinearRegression                         0.123873                0.254337   \n",
       "KNeighborsRegressor                      0.148924                0.280575   \n",
       "SVR                                      0.131766                0.259583   \n",
       "MLPRegressor                             0.133672                0.266273   \n",
       "\n",
       "                           Train RMSE (20 fold CV)  Train R2 (20 fold CV)  \\\n",
       "LGBMRegressor                             0.351603               0.801504   \n",
       "DecisionTreeRegressor                     0.505921               0.589030   \n",
       "RandomForestRegressor                     0.353718               0.799109   \n",
       "GradientBoostingRegressor                 0.352426               0.800574   \n",
       "AdaBoostRegressor                         0.407225               0.733734   \n",
       "XGBRegressor                              0.377143               0.771620   \n",
       "ExtraTreesRegressor                       0.351238               0.801916   \n",
       "LinearRegression                          0.351956               0.801106   \n",
       "KNeighborsRegressor                       0.385906               0.760884   \n",
       "SVR                                       0.362996               0.788433   \n",
       "MLPRegressor                              0.365612               0.785372   \n",
       "\n",
       "                           Train PCC (20 fold CV)  Train SCC (20 fold CV)  \\\n",
       "LGBMRegressor                            0.895277                0.878916   \n",
       "DecisionTreeRegressor                    0.798817                0.770858   \n",
       "RandomForestRegressor                    0.893964                0.877420   \n",
       "GradientBoostingRegressor                0.894748                0.877182   \n",
       "AdaBoostRegressor                        0.866184                0.858277   \n",
       "XGBRegressor                             0.878973                0.862483   \n",
       "ExtraTreesRegressor                      0.895523                0.877273   \n",
       "LinearRegression                         0.895048                0.878397   \n",
       "KNeighborsRegressor                      0.872412                0.852664   \n",
       "SVR                                      0.888171                0.877363   \n",
       "MLPRegressor                             0.886431                0.866990   \n",
       "\n",
       "                           Test MSE  Test MAE  Test RMSE   Test R2  Test PCC  \\\n",
       "LGBMRegressor              0.191610  0.307490   0.437732  0.697217  0.835119   \n",
       "DecisionTreeRegressor      0.210502  0.323504   0.458805  0.667363  0.818505   \n",
       "RandomForestRegressor      0.194562  0.308341   0.441091  0.692552  0.832448   \n",
       "GradientBoostingRegressor  0.192875  0.307301   0.439175  0.695218  0.833927   \n",
       "AdaBoostRegressor          0.217909  0.347540   0.466807  0.655659  0.816905   \n",
       "XGBRegressor               0.194220  0.307843   0.440704  0.693093  0.832948   \n",
       "ExtraTreesRegressor        0.193505  0.308350   0.439892  0.694222  0.833482   \n",
       "LinearRegression           0.192259  0.307641   0.438474  0.696190  0.834527   \n",
       "KNeighborsRegressor        0.204262  0.317270   0.451954  0.677223  0.824058   \n",
       "SVR                        0.194604  0.305052   0.441139  0.692485  0.833372   \n",
       "MLPRegressor               0.190683  0.306709   0.436673  0.698681  0.836082   \n",
       "\n",
       "                           Test SCC  \n",
       "LGBMRegressor              0.820597  \n",
       "DecisionTreeRegressor      0.798929  \n",
       "RandomForestRegressor      0.815556  \n",
       "GradientBoostingRegressor  0.819573  \n",
       "AdaBoostRegressor          0.811944  \n",
       "XGBRegressor               0.818835  \n",
       "ExtraTreesRegressor        0.816034  \n",
       "LinearRegression           0.818674  \n",
       "KNeighborsRegressor        0.809096  \n",
       "SVR                        0.818657  \n",
       "MLPRegressor               0.821108  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_weak = [\n",
    "    lgb.LGBMRegressor(objective='regression', metric='rmse', boosting_type='gbdt', num_leaves=31, learning_rate=0.05, random_state=101),\n",
    "    RandomForestRegressor(n_jobs=-1, random_state=101),\n",
    "    GradientBoostingRegressor(random_state=101),\n",
    "    AdaBoostRegressor(random_state=101),\n",
    "    xgb.XGBRegressor(random_state=101, eval_metric='rmse'),\n",
    "    ExtraTreesRegressor(n_jobs=-1, n_estimators=100, random_state=101),\n",
    "    KNeighborsRegressor(),\n",
    "    SVR(),\n",
    "    MLPRegressor(random_state=101, max_iter=500),\n",
    "    DecisionTreeRegressor(random_state=101)\n",
    "]\n",
    "\n",
    "models_meta = [\n",
    "    lgb.LGBMRegressor(objective='regression', metric='rmse', boosting_type='gbdt', num_leaves=31, learning_rate=0.05, random_state=101),\n",
    "    DecisionTreeRegressor(random_state=101),\n",
    "    RandomForestRegressor(n_jobs=-1, random_state=101),\n",
    "    GradientBoostingRegressor(random_state=101),\n",
    "    AdaBoostRegressor(random_state=101),\n",
    "    xgb.XGBRegressor(random_state=101, eval_metric='rmse'),\n",
    "    ExtraTreesRegressor(n_jobs=-1, n_estimators=100, random_state=101),\n",
    "    LinearRegression(),\n",
    "    KNeighborsRegressor(),\n",
    "    SVR(),\n",
    "    MLPRegressor(random_state=101, max_iter=500)\n",
    "]\n",
    "target_column = 'Permeability'\n",
    "dataframes = [(df_desc_train, df_desc_test), (df_fp_train, df_fp_test), (df_emb_train, df_emb_test), (df_atomic_train, df_atomic_test)]\n",
    "\n",
    "meta_features_train = []\n",
    "meta_features_test = []\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Stage 1: Training weak learners with 20-fold CV')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "\n",
    "# Stage 1: Train weak learners with 20-fold cross-validation\n",
    "for df_train, df_test in tqdm(dataframes, desc=\"Processing dataframe pairs\"):\n",
    "    X_weak = df_train.drop(columns=['ID', 'SMILES', target_column])\n",
    "    y_weak = df_train[target_column]\n",
    "    X_eval = df_test.drop(columns=['ID', 'SMILES', target_column])\n",
    "    y_eval = df_test[target_column]\n",
    "    kf = KFold(n_splits=20, shuffle=True, random_state=101)\n",
    "\n",
    "    fold_meta_features_train = np.zeros((X_weak.shape[0], len(models_weak)))\n",
    "    fold_meta_features_test = np.zeros((X_eval.shape[0], len(models_weak)))\n",
    "\n",
    "    for i, model in tqdm(enumerate(models_weak), desc=\"Training models\", total=len(models_weak)):\n",
    "        fold_predictions = np.zeros(X_weak.shape[0])\n",
    "        test_predictions_folds = []\n",
    "\n",
    "        for train_index, val_index in kf.split(X_weak):\n",
    "            X_train, X_val = X_weak.iloc[train_index], X_weak.iloc[val_index]\n",
    "            y_train, y_val = y_weak.iloc[train_index], y_weak.iloc[val_index]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            fold_predictions[val_index] = np.clip(model.predict(X_val), -10, -3.9)\n",
    "            test_predictions_fold = np.clip(model.predict(X_eval), -10, -3.9)\n",
    "            test_predictions_folds.append(test_predictions_fold)\n",
    "\n",
    "        fold_meta_features_train[:, i] = fold_predictions\n",
    "        fold_meta_features_test[:, i] = np.mean(test_predictions_folds, axis=0)\n",
    "        print(f'Model training done {i}')\n",
    "\n",
    "    meta_features_train.append(fold_meta_features_train)\n",
    "    meta_features_test.append(fold_meta_features_test)\n",
    "\n",
    "meta_features_train = np.hstack(meta_features_train)\n",
    "meta_features_test = np.hstack(meta_features_test)\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print(\"Dimensions of meta_features_train:\", meta_features_train.shape)\n",
    "print(\"Dimensions of meta_features_test:\", meta_features_test.shape)\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Stage 1 completed')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Stage 2: Training meta-learners with 20-fold CV')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "\n",
    "\n",
    "results = {}\n",
    "predictions = []\n",
    "\n",
    "kf = KFold(n_splits=20, shuffle=True, random_state=101)\n",
    "\n",
    "for model in models_meta:\n",
    "    model_name = model.__class__.__name__\n",
    "    predictions_train = []\n",
    "    actual_y_train = []\n",
    "    test_predictions_folds = []\n",
    "\n",
    "    for train_index, val_index in kf.split(meta_features_train):\n",
    "        X_fold_train, X_fold_val = meta_features_train[train_index], meta_features_train[val_index]\n",
    "        y_fold_train, y_fold_val = y_weak.iloc[train_index], y_weak.iloc[val_index]\n",
    "\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "        y_pred_fold = model.predict(X_fold_val)\n",
    "        y_pred_fold = np.clip(y_pred_fold, -10, -3.9)\n",
    "\n",
    "        predictions_train.extend(y_pred_fold)\n",
    "        actual_y_train.extend(y_fold_val)\n",
    "\n",
    "        test_predictions_fold = model.predict(meta_features_test)\n",
    "        test_predictions_fold = np.clip(test_predictions_fold, -10, -3.9)\n",
    "        test_predictions_folds.append(test_predictions_fold)\n",
    "\n",
    "    predictions_test_mean = np.mean(test_predictions_folds, axis=0)\n",
    "    predictions_test_std = np.std(test_predictions_folds, axis=0)\n",
    "\n",
    "    mse_train = mean_squared_error(actual_y_train, predictions_train)\n",
    "    mae_train = mean_absolute_error(actual_y_train, predictions_train)\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    r2_train = r2_score(actual_y_train, predictions_train)\n",
    "    pearson_train, _ = pearsonr(actual_y_train, predictions_train)\n",
    "    spearman_train, _ = spearmanr(actual_y_train, predictions_train)\n",
    "\n",
    "    \n",
    "    mse_test = mean_squared_error(y_eval, predictions_test_mean)\n",
    "    mae_test = mean_absolute_error(y_eval, predictions_test_mean)\n",
    "    rmse_test = np.sqrt(mse_test)\n",
    "    r2_test = r2_score(y_eval, predictions_test_mean)\n",
    "    pearson_test, _ = pearsonr(y_eval, predictions_test_mean)\n",
    "    spearman_test, _ = spearmanr(y_eval, predictions_test_mean)\n",
    "\n",
    "    print(f'{model_name} Evaluation completed: Test R2 score: {r2_test:.4f}, Test RMSE: {rmse_test:.4f}')\n",
    "\n",
    "    predictions.append({\n",
    "        'Model': model_name,\n",
    "        'Y Train pred': predictions_train,\n",
    "        'Y Train actual': actual_y_train,\n",
    "        'Y Test actual': y_eval,\n",
    "        'Test Predictions folds': test_predictions_folds,\n",
    "        'Test Predictions Mean': predictions_test_mean,\n",
    "        'Test Predictions Std': predictions_test_std,\n",
    "\n",
    "    })\n",
    "\n",
    "    results[model_name] = {\n",
    "        'Train MSE (20 fold CV)': mse_train,\n",
    "        'Train MAE (20 fold CV)': mae_train,\n",
    "        'Train RMSE (20 fold CV)': rmse_train,\n",
    "        'Train R2 (20 fold CV)': r2_train,\n",
    "        'Train PCC (20 fold CV)': pearson_train,\n",
    "        'Train SCC (20 fold CV)': spearman_train,\n",
    "        'Test MSE': mse_test,\n",
    "        'Test MAE': mae_test,\n",
    "        'Test RMSE': rmse_test,\n",
    "        'Test R2': r2_test,\n",
    "        'Test PCC': pearson_test,\n",
    "        'Test SCC': spearman_test,\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Stacked Ensemble Training and Evaluation complete')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfd5b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('/home/users/akshay/PCPpred/PAMPA/Results/Stacked_architecture/Results_20_fold_stacked_prediction_LVR_scaled.csv')\n",
    "predictions_df.to_csv('/home/users/akshay/PCPpred/PAMPA/Results/Stacked_architecture/Prediction_data_Meta_learners_20_fold_stacked_prediction_LVR_scaled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd447289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c2590-b3e1-4948-a1a0-860337558212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b754a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2667770/3964313023.py:24: DtypeWarning: Columns (1275,1277,1280,1285,1292,1298,1304,1354,1356,1359,1364,1371,1377,1383,1579,1580,1581,1583,1584,1590,1595,1596,1597) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_desc_train = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Descriptors/Train_2d_3d_all_descriptors.csv')\n",
      "/tmp/ipykernel_2667770/3964313023.py:33: DtypeWarning: Columns (1275,1277,1292,1298,1354,1356,1371,1377,1579,1580,1581,1583,1584,1595,1596,1597) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_desc_test = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Descriptors/Test_2d_3d_all_descriptors.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Data Loading completed\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Data Processing completed\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "(5568, 271)\n",
      "(1392, 271)\n",
      "(5568, 1128)\n",
      "(1392, 1128)\n",
      "(5568, 758)\n",
      "(1392, 758)\n",
      "(5568, 12)\n",
      "(1392, 12)\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "4765     3  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.000000   \n",
      "4772     4  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.100000   \n",
      "4766     6  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.300000   \n",
      "4767     7  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.300000   \n",
      "4585    11  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.470000   \n",
      "...    ...                                                ...           ...   \n",
      "607   8512  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](C(...     -6.640000   \n",
      "726   8514  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -5.929053   \n",
      "772   8516  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.199359   \n",
      "5468  8517  CC[C@@H]1c2nnc(o2)[C@H](Cc2ccccc2)NC(=O)CNC(=O...     -6.780000   \n",
      "5397  8518  NCCCC[C@@H]1NC(=O)[C@H](Cc2ccccc2)NC(=O)[C@H](...     -5.505000   \n",
      "\n",
      "      MinEStateIndex       qed        SPS  FpDensityMorgan1  BCUT2D_MWHI  \\\n",
      "4765       -1.126245  0.215270  26.000000          0.568627    16.254973   \n",
      "4772       -1.126245  0.215270  26.000000          0.568627    16.254973   \n",
      "4766       -1.126245  0.215270  26.000000          0.568627    16.254973   \n",
      "4767       -1.126245  0.215270  26.000000          0.568627    16.254973   \n",
      "4585       -1.057909  0.233167  25.673077          0.576923    16.465390   \n",
      "...              ...       ...        ...               ...          ...   \n",
      "607        -1.208204  0.194882  26.621951          0.378049    16.190089   \n",
      "726        -1.044795  0.274599  29.051282          0.564103    16.183951   \n",
      "772        -1.052167  0.264032  28.118421          0.552632    16.183876   \n",
      "5468       -0.845231  0.429454  25.300000          0.925000    16.366728   \n",
      "5397       -1.583104  0.118757  23.086957          0.630435    16.257835   \n",
      "\n",
      "        AvgIpc  BalabanJ_x  ...      RNCS    LOBMAX    LOBMIN   MOMI-XY  \\\n",
      "4765  3.524800    2.105859  ...  4.059403  1.229311  1.116345  1.445284   \n",
      "4772  3.524800    2.105859  ...  2.430700  1.301900  1.013546  1.331807   \n",
      "4766  3.524800    2.105859  ...  2.379031  1.318982  1.318982  1.416716   \n",
      "4767  3.524800    2.105859  ...  2.392510  1.171389  1.057402  1.316650   \n",
      "4585  3.555194    2.111795  ...  1.876242  1.482892  1.461695  1.319991   \n",
      "...        ...         ...  ...       ...       ...       ...       ...   \n",
      "607   3.466151    4.234460  ...  0.625185  1.354079  1.354079  1.385853   \n",
      "726   4.111830    1.340817  ...  1.277461  1.176213  1.033796  1.547430   \n",
      "772   4.065966    1.511740  ...  0.778508  1.247251  1.204229  1.549481   \n",
      "5468  3.609789    1.643135  ...  0.000000  1.360702  1.170716  1.386263   \n",
      "5397  3.023866    2.051939  ...  0.919946  1.936361  1.704628  1.082969   \n",
      "\n",
      "      geomShape     RDF55p       P1m        Dv       L3p        Ki  \n",
      "4765   0.832252  35.580478  0.532075  1.216361  2.697051  0.361574  \n",
      "4772   0.905135  38.960678  0.562820  0.913940  3.825470  0.285387  \n",
      "4766   0.765706  32.692290  0.579772  1.189531  2.451943  0.374559  \n",
      "4767   0.859830  40.512780  0.517578  0.988930  3.888022  0.269237  \n",
      "4585   0.833525  44.236785  0.627460  0.992478  2.726084  0.375665  \n",
      "...         ...        ...       ...       ...       ...       ...  \n",
      "607    0.782432  76.013922  0.618173  1.272322  3.465821  0.407764  \n",
      "726    0.829074  68.967821  0.566761  1.033432  2.674361  0.411379  \n",
      "772    0.904516  48.286030  0.563706  1.120753  2.906278  0.407451  \n",
      "5468   0.920917  22.088272  0.570890  1.076762  2.195044  0.395831  \n",
      "5397   0.901215  34.586426  0.705175  0.989737  2.917641  0.539314  \n",
      "\n",
      "[5568 rows x 271 columns]\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "1192     8  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -7.300000   \n",
      "1145    15  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -6.520000   \n",
      "841     17  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -7.290000   \n",
      "1146    18  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.750000   \n",
      "755     19  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@H]...     -6.950000   \n",
      "...    ...                                                ...           ...   \n",
      "1147  8475  CC(C)C[C@@H]1NC(=O)[C@@H](CC(C)C)N(C)C(=O)[C@H...     -5.960000   \n",
      "174   8483  CC(C)C[C@@H]1NC(=O)[C@H](C)N(C)C(=O)[C@H](CC2C...     -6.009141   \n",
      "1193  8487  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -6.495000   \n",
      "122   8513  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](CC...     -8.200000   \n",
      "201   8515  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.566211   \n",
      "\n",
      "      MinEStateIndex       qed        SPS  FpDensityMorgan1  BCUT2D_MWHI  \\\n",
      "1192       -1.126245  0.215270  26.000000          0.568627    16.254973   \n",
      "1145       -1.057909  0.233167  25.673077          0.576923    16.465390   \n",
      "841        -1.090876  0.292940  28.206897          0.517241    16.465422   \n",
      "1146       -1.057909  0.233167  25.673077          0.576923    16.465390   \n",
      "755        -1.109218  0.224862  26.932203          0.508475    16.465419   \n",
      "...              ...       ...        ...               ...          ...   \n",
      "1147       -1.125961  0.246302  25.942308          0.615385    16.254985   \n",
      "174        -1.049404  0.271350  28.911392          0.518987    16.183972   \n",
      "1193       -1.126245  0.215270  26.000000          0.568627    16.254973   \n",
      "122        -1.456022  0.132042  25.724138          0.413793    16.191299   \n",
      "201        -1.050097  0.279880  28.333333          0.586667    16.183920   \n",
      "\n",
      "        AvgIpc  BalabanJ_x  ...      RNCS    LOBMAX    LOBMIN   MOMI-XY  \\\n",
      "1192  3.524800    2.105859  ...  2.419467  1.150893  1.015666  1.562053   \n",
      "1145  3.555194    2.111795  ...  1.758568  1.242915  1.241792  1.325082   \n",
      "841   3.969702    1.549820  ...  1.571209  1.199839  1.008213  1.565738   \n",
      "1146  3.555194    2.111795  ...  1.928541  1.208494  1.074378  1.526981   \n",
      "755   3.862771    1.798149  ...  1.689117  1.206602  1.156762  1.543384   \n",
      "...        ...         ...  ...       ...       ...       ...       ...   \n",
      "1147  3.547997    2.146757  ...  2.299245  1.067795  1.010859  1.466799   \n",
      "174   4.118337    1.352048  ...  1.384929  1.235104  1.200599  1.633772   \n",
      "1193  3.524800    2.105859  ...  2.405988  1.084460  1.007445  1.542899   \n",
      "122   1.621178    4.382718  ...  1.176240  1.101137  1.084997  1.412328   \n",
      "201   4.060393    1.513753  ...  0.688547  1.111835  1.004241  1.721602   \n",
      "\n",
      "      geomShape     RDF55p       P1m        Dv       L3p        Ki  \n",
      "1192   0.745882  39.414328  0.557076  1.128459  1.562507  0.408108  \n",
      "1145   0.787200  39.934832  0.560880  1.215016  3.482162  0.351511  \n",
      "841    0.690480  48.632791  0.567719  1.036948  1.689512  0.421941  \n",
      "1146   0.771384  40.759570  0.551536  0.991921  2.430374  0.378168  \n",
      "755    0.757646  46.517665  0.572634  1.042262  1.866171  0.415839  \n",
      "...         ...        ...       ...       ...       ...       ...  \n",
      "1147   0.714699  46.871562  0.497007  1.109375  3.498353  0.315953  \n",
      "174    0.859816  57.763793  0.494014  1.233429  4.228185  0.368291  \n",
      "1193   0.818768  42.734317  0.477615  0.977114  2.536672  0.355842  \n",
      "122    0.491025  81.795576  0.526458  1.361869  6.133363  0.314560  \n",
      "201    0.832274  50.640449  0.526102  1.047451  2.011146  0.432148  \n",
      "\n",
      "[1392 rows x 271 columns]\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "4765     3  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.000000   \n",
      "4772     4  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.100000   \n",
      "4766     6  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.300000   \n",
      "4767     7  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.300000   \n",
      "4585    11  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.470000   \n",
      "...    ...                                                ...           ...   \n",
      "607   8512  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](C(...     -6.640000   \n",
      "726   8514  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -5.929053   \n",
      "772   8516  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.199359   \n",
      "5468  8517  CC[C@@H]1c2nnc(o2)[C@H](Cc2ccccc2)NC(=O)CNC(=O...     -6.780000   \n",
      "5397  8518  NCCCC[C@@H]1NC(=O)[C@H](Cc2ccccc2)NC(=O)[C@H](...     -5.505000   \n",
      "\n",
      "      Morgan_fp_2  Morgan_fp_5  Morgan_fp_13  Morgan_fp_19  Morgan_fp_42  \\\n",
      "4765            0            1             0             1             0   \n",
      "4772            0            1             0             1             0   \n",
      "4766            0            1             0             1             0   \n",
      "4767            0            1             0             1             0   \n",
      "4585            0            1             0             1             0   \n",
      "...           ...          ...           ...           ...           ...   \n",
      "607             0            1             0             1             0   \n",
      "726             1            1             0             0             0   \n",
      "772             1            1             0             0             0   \n",
      "5468            0            1             0             0             0   \n",
      "5397            0            0             0             0             0   \n",
      "\n",
      "      Morgan_fp_84  Morgan_fp_94  ...  SubFPC2  SubFPC100  SubFPC101  \\\n",
      "4765             0             0  ...      7.0        0.0        0.0   \n",
      "4772             0             0  ...      7.0        0.0        0.0   \n",
      "4766             0             0  ...      7.0        0.0        0.0   \n",
      "4767             0             0  ...      7.0        0.0        0.0   \n",
      "4585             0             0  ...      7.0        0.0        0.0   \n",
      "...            ...           ...  ...      ...        ...        ...   \n",
      "607              0             0  ...      7.0        0.0        0.0   \n",
      "726              0             0  ...     22.0        0.0        0.0   \n",
      "772              0             0  ...     18.0        0.0        0.0   \n",
      "5468             0             0  ...      5.0        0.0        0.0   \n",
      "5397             0             0  ...      6.0        0.0        0.0   \n",
      "\n",
      "      SubFPC127  SubFPC135  SubFPC137  SubFPC143  SubFPC182  SubFPC275  \\\n",
      "4765        6.0        0.0        0.0        0.0        0.0        6.0   \n",
      "4772        6.0        0.0        0.0        0.0        0.0        6.0   \n",
      "4766        6.0        0.0        0.0        0.0        0.0        6.0   \n",
      "4767        6.0        0.0        0.0        0.0        0.0        6.0   \n",
      "4585        6.0        0.0        0.0        0.0        0.0        6.0   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "607        11.0        0.0        0.0        0.0        0.0       11.0   \n",
      "726         6.0        0.0        0.0        0.0        0.0       12.0   \n",
      "772         6.0        0.0        0.0        0.0        0.0       12.0   \n",
      "5468        2.0        0.0        0.0        0.0        1.0        8.0   \n",
      "5397        5.0        0.0        0.0        0.0        0.0        5.0   \n",
      "\n",
      "      SubFPC302  \n",
      "4765       10.0  \n",
      "4772       10.0  \n",
      "4766       10.0  \n",
      "4767       10.0  \n",
      "4585       11.0  \n",
      "...         ...  \n",
      "607        14.0  \n",
      "726         9.0  \n",
      "772        10.0  \n",
      "5468        5.0  \n",
      "5397       11.0  \n",
      "\n",
      "[5568 rows x 1128 columns]\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "1192     8  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -7.300000   \n",
      "1145    15  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -6.520000   \n",
      "841     17  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -7.290000   \n",
      "1146    18  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.750000   \n",
      "755     19  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@H]...     -6.950000   \n",
      "...    ...                                                ...           ...   \n",
      "1147  8475  CC(C)C[C@@H]1NC(=O)[C@@H](CC(C)C)N(C)C(=O)[C@H...     -5.960000   \n",
      "174   8483  CC(C)C[C@@H]1NC(=O)[C@H](C)N(C)C(=O)[C@H](CC2C...     -6.009141   \n",
      "1193  8487  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -6.495000   \n",
      "122   8513  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](CC...     -8.200000   \n",
      "201   8515  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.566211   \n",
      "\n",
      "      Morgan_fp_2  Morgan_fp_5  Morgan_fp_13  Morgan_fp_19  Morgan_fp_42  \\\n",
      "1192            0            1             0             1             0   \n",
      "1145            0            1             0             1             0   \n",
      "841             0            1             0             1             0   \n",
      "1146            0            1             0             1             0   \n",
      "755             0            1             0             1             0   \n",
      "...           ...          ...           ...           ...           ...   \n",
      "1147            0            1             0             1             0   \n",
      "174             1            1             0             0             0   \n",
      "1193            0            1             0             1             0   \n",
      "122             0            1             0             1             0   \n",
      "201             1            1             0             0             0   \n",
      "\n",
      "      Morgan_fp_84  Morgan_fp_94  ...  SubFPC2  SubFPC100  SubFPC101  \\\n",
      "1192             0             0  ...      7.0        0.0        0.0   \n",
      "1145             0             0  ...      7.0        0.0        0.0   \n",
      "841              0             0  ...     10.0        0.0        0.0   \n",
      "1146             0             0  ...      7.0        0.0        0.0   \n",
      "755              0             0  ...      9.0        0.0        0.0   \n",
      "...            ...           ...  ...      ...        ...        ...   \n",
      "1147             0             0  ...      7.0        0.0        0.0   \n",
      "174              0             0  ...     21.0        0.0        0.0   \n",
      "1193             0             0  ...      7.0        0.0        0.0   \n",
      "122              0             0  ...     10.0        0.0        0.0   \n",
      "201              0             0  ...     17.0        0.0        0.0   \n",
      "\n",
      "      SubFPC127  SubFPC135  SubFPC137  SubFPC143  SubFPC182  SubFPC275  \\\n",
      "1192        6.0        0.0        0.0        0.0        0.0        6.0   \n",
      "1145        6.0        0.0        0.0        0.0        0.0        6.0   \n",
      "841         7.0        0.0        0.0        0.0        0.0        7.0   \n",
      "1146        6.0        0.0        0.0        0.0        0.0        6.0   \n",
      "755         7.0        0.0        0.0        0.0        0.0        7.0   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "1147        6.0        0.0        0.0        0.0        0.0        6.0   \n",
      "174         6.0        0.0        0.0        0.0        0.0       12.0   \n",
      "1193        6.0        0.0        0.0        0.0        0.0        6.0   \n",
      "122        11.0        0.0        0.0        0.0        0.0       11.0   \n",
      "201         6.0        0.0        0.0        0.0        0.0       12.0   \n",
      "\n",
      "      SubFPC302  \n",
      "1192       10.0  \n",
      "1145       11.0  \n",
      "841         9.0  \n",
      "1146       11.0  \n",
      "755        11.0  \n",
      "...         ...  \n",
      "1147       10.0  \n",
      "174         9.0  \n",
      "1193       10.0  \n",
      "122        18.0  \n",
      "201         9.0  \n",
      "\n",
      "[1392 rows x 1128 columns]\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "4765     3  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.000000   \n",
      "4772     4  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.100000   \n",
      "4766     6  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.300000   \n",
      "4767     7  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.300000   \n",
      "4585    11  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.470000   \n",
      "...    ...                                                ...           ...   \n",
      "607   8512  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](C(...     -6.640000   \n",
      "726   8514  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -5.929053   \n",
      "772   8516  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.199359   \n",
      "5468  8517  CC[C@@H]1c2nnc(o2)[C@H](Cc2ccccc2)NC(=O)CNC(=O...     -6.780000   \n",
      "5397  8518  NCCCC[C@@H]1NC(=O)[C@H](Cc2ccccc2)NC(=O)[C@H](...     -5.505000   \n",
      "\n",
      "      x_fine_emb_MFXL0  x_fine_emb_MFXL1  x_fine_emb_MFXL2  x_fine_emb_MFXL3  \\\n",
      "4765         -0.337456          1.695567          1.677923          0.162478   \n",
      "4772         -0.693720          1.671361          1.743715         -0.021658   \n",
      "4766         -0.611395          1.743946          1.904406          0.110867   \n",
      "4767         -0.250263          1.665128          1.810053          0.183712   \n",
      "4585         -0.332478          1.717748          1.224448         -0.192105   \n",
      "...                ...               ...               ...               ...   \n",
      "607          -0.253131          1.356235          2.147836         -0.730332   \n",
      "726           0.665720          1.008713          1.577817         -0.446476   \n",
      "772           0.556295          1.102336          1.611429         -0.437126   \n",
      "5468         -0.473899          1.651141          1.904790         -0.159932   \n",
      "5397         -0.181520          1.522243          1.929046          0.574847   \n",
      "\n",
      "      x_fine_emb_MFXL4  x_fine_emb_MFXL5  x_fine_emb_MFXL6  ...  \\\n",
      "4765          1.364697          0.580695          0.724169  ...   \n",
      "4772          1.516323          0.512279          0.604900  ...   \n",
      "4766          1.226735          0.470598          0.625896  ...   \n",
      "4767          1.260458          0.583332          0.751572  ...   \n",
      "4585          1.425188          0.506747          0.714092  ...   \n",
      "...                ...               ...               ...  ...   \n",
      "607           1.072611          0.129030         -0.000324  ...   \n",
      "726           0.650200         -0.126983          0.574037  ...   \n",
      "772           0.648019         -0.101036          0.535102  ...   \n",
      "5468          1.259037          0.160518          0.135909  ...   \n",
      "5397          0.116897          0.439762          0.441216  ...   \n",
      "\n",
      "      x_fine_emb_MFXL758  x_fine_emb_MFXL759  x_fine_emb_MFXL760  \\\n",
      "4765           -0.635967            0.820135           -0.942027   \n",
      "4772           -0.502921            0.976571           -0.753769   \n",
      "4766           -0.383834            0.798668           -0.694361   \n",
      "4767           -0.601673            0.719792           -0.860240   \n",
      "4585           -0.925237            1.054691           -1.066249   \n",
      "...                  ...                 ...                 ...   \n",
      "607            -0.582554            0.410025           -0.968044   \n",
      "726            -0.483579            0.453173           -0.217859   \n",
      "772            -0.436216            0.198191           -0.192394   \n",
      "5468           -0.710861            0.853048           -0.854692   \n",
      "5397           -0.226625            0.651272           -0.586757   \n",
      "\n",
      "      x_fine_emb_MFXL761  x_fine_emb_MFXL762  x_fine_emb_MFXL763  \\\n",
      "4765            0.165309           -0.076899           -0.386877   \n",
      "4772            0.165744           -0.038887           -0.817339   \n",
      "4766            0.281050           -0.103311           -0.557286   \n",
      "4767            0.094293           -0.081830           -0.332994   \n",
      "4585            0.048505            0.015418           -0.031599   \n",
      "...                  ...                 ...                 ...   \n",
      "607            -0.183033            0.740053           -0.381717   \n",
      "726            -0.083189            0.271193            0.184055   \n",
      "772             0.008898            0.275830            0.129249   \n",
      "5468            0.364419            0.550643           -0.105093   \n",
      "5397            0.176439            0.239423           -0.561742   \n",
      "\n",
      "      x_fine_emb_MFXL764  x_fine_emb_MFXL765  x_fine_emb_MFXL766  \\\n",
      "4765           -0.977795            1.060619            0.122603   \n",
      "4772           -0.666867            1.064216           -0.031611   \n",
      "4766           -0.933991            1.012367           -0.046721   \n",
      "4767           -1.010956            1.036676            0.138966   \n",
      "4585           -1.154511            0.699089           -0.047392   \n",
      "...                  ...                 ...                 ...   \n",
      "607            -0.659635            0.141509           -0.279061   \n",
      "726            -0.980991            0.152226           -0.026960   \n",
      "772            -1.167997            0.149711           -0.033849   \n",
      "5468           -1.025956            0.673330           -0.127334   \n",
      "5397           -0.868175            0.942459           -0.184120   \n",
      "\n",
      "      x_fine_emb_MFXL767  \n",
      "4765            0.295984  \n",
      "4772            0.442533  \n",
      "4766            0.303076  \n",
      "4767            0.161183  \n",
      "4585           -0.208382  \n",
      "...                  ...  \n",
      "607            -0.573766  \n",
      "726            -1.126979  \n",
      "772            -1.247071  \n",
      "5468           -0.165311  \n",
      "5397           -0.083694  \n",
      "\n",
      "[5568 rows x 758 columns]\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "1192     8  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -7.300000   \n",
      "1145    15  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -6.520000   \n",
      "841     17  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -7.290000   \n",
      "1146    18  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.750000   \n",
      "755     19  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@H]...     -6.950000   \n",
      "...    ...                                                ...           ...   \n",
      "1147  8475  CC(C)C[C@@H]1NC(=O)[C@@H](CC(C)C)N(C)C(=O)[C@H...     -5.960000   \n",
      "174   8483  CC(C)C[C@@H]1NC(=O)[C@H](C)N(C)C(=O)[C@H](CC2C...     -6.009141   \n",
      "1193  8487  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -6.495000   \n",
      "122   8513  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](CC...     -8.200000   \n",
      "201   8515  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.566211   \n",
      "\n",
      "      x_fine_emb_MFXL0  x_fine_emb_MFXL1  x_fine_emb_MFXL2  x_fine_emb_MFXL3  \\\n",
      "1192         -0.406239          1.693077          1.616310          0.136363   \n",
      "1145         -0.260269          1.739833          1.254690         -0.199432   \n",
      "841          -0.480061          1.673956          1.227621         -0.153543   \n",
      "1146         -0.424540          1.619960          1.112747         -0.148454   \n",
      "755          -0.402701          1.668258          1.159961         -0.089745   \n",
      "...                ...               ...               ...               ...   \n",
      "1147          0.109826          1.409296          1.827492         -0.063606   \n",
      "174           0.129572          1.463878          1.760533         -0.360613   \n",
      "1193         -0.375062          1.710263          1.622855          0.133052   \n",
      "122          -0.886350          0.999057          2.070368         -0.645724   \n",
      "201           0.566460          1.053462          1.630616         -0.436546   \n",
      "\n",
      "      x_fine_emb_MFXL4  x_fine_emb_MFXL5  x_fine_emb_MFXL6  ...  \\\n",
      "1192          1.396899          0.556379          0.715648  ...   \n",
      "1145          1.408004          0.493616          0.737641  ...   \n",
      "841           1.422423          0.532140          0.658469  ...   \n",
      "1146          1.394888          0.395110          0.639109  ...   \n",
      "755           1.482198          0.432872          0.692142  ...   \n",
      "...                ...               ...               ...  ...   \n",
      "1147          0.966044          0.412432          0.596994  ...   \n",
      "174           1.143565          0.265003          0.522915  ...   \n",
      "1193          1.426098          0.572014          0.739967  ...   \n",
      "122           0.983707          0.082509         -0.098325  ...   \n",
      "201           0.655992         -0.096901          0.492707  ...   \n",
      "\n",
      "      x_fine_emb_MFXL758  x_fine_emb_MFXL759  x_fine_emb_MFXL760  \\\n",
      "1192           -0.648148            0.891384           -0.890869   \n",
      "1145           -0.937227            1.045442           -1.043088   \n",
      "841            -1.238928            1.095139           -0.979132   \n",
      "1146           -1.022537            0.999551           -0.954643   \n",
      "755            -1.237151            1.029979           -0.965218   \n",
      "...                  ...                 ...                 ...   \n",
      "1147           -0.440023            0.891086           -0.556675   \n",
      "174            -0.409511            0.658839           -0.530521   \n",
      "1193           -0.691713            0.812630           -0.933072   \n",
      "122            -0.537097            0.696460           -0.961172   \n",
      "201            -0.362031            0.193457           -0.122189   \n",
      "\n",
      "      x_fine_emb_MFXL761  x_fine_emb_MFXL762  x_fine_emb_MFXL763  \\\n",
      "1192            0.177673           -0.048339           -0.377661   \n",
      "1145            0.064723            0.004101            0.034019   \n",
      "841             0.162785           -0.290237           -0.074921   \n",
      "1146            0.011778           -0.192340           -0.119020   \n",
      "755             0.093065           -0.265660           -0.138270   \n",
      "...                  ...                 ...                 ...   \n",
      "1147            0.301938            0.025583           -0.455801   \n",
      "174            -0.422093           -0.074831           -0.032116   \n",
      "1193            0.175445           -0.097936           -0.358790   \n",
      "122             0.305727            0.438187           -0.882162   \n",
      "201             0.064138            0.328782            0.099381   \n",
      "\n",
      "      x_fine_emb_MFXL764  x_fine_emb_MFXL765  x_fine_emb_MFXL766  \\\n",
      "1192           -1.012855            1.088217            0.080363   \n",
      "1145           -1.175928            0.673376           -0.054396   \n",
      "841            -0.676783            0.697224           -0.153161   \n",
      "1146           -1.037015            0.606396           -0.055217   \n",
      "755            -0.842036            0.740999           -0.151366   \n",
      "...                  ...                 ...                 ...   \n",
      "1147           -1.044930            0.907672            0.273455   \n",
      "174            -0.651280            0.410307            0.232910   \n",
      "1193           -0.972274            1.053713            0.127515   \n",
      "122             0.034320            0.589909            0.500930   \n",
      "201            -1.112577            0.163432           -0.091296   \n",
      "\n",
      "      x_fine_emb_MFXL767  \n",
      "1192            0.420160  \n",
      "1145           -0.236034  \n",
      "841             0.007492  \n",
      "1146           -0.065022  \n",
      "755            -0.059674  \n",
      "...                  ...  \n",
      "1147           -0.046504  \n",
      "174            -0.827267  \n",
      "1193            0.353476  \n",
      "122             1.018875  \n",
      "201            -1.274860  \n",
      "\n",
      "[1392 rows x 758 columns]\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "4765     3  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.000000   \n",
      "4772     4  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.100000   \n",
      "4766     6  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.300000   \n",
      "4767     7  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.300000   \n",
      "4585    11  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.470000   \n",
      "...    ...                                                ...           ...   \n",
      "607   8512  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](C(...     -6.640000   \n",
      "726   8514  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -5.929053   \n",
      "772   8516  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.199359   \n",
      "5468  8517  CC[C@@H]1c2nnc(o2)[C@H](Cc2ccccc2)NC(=O)CNC(=O...     -6.780000   \n",
      "5397  8518  NCCCC[C@@H]1NC(=O)[C@H](Cc2ccccc2)NC(=O)[C@H](...     -5.505000   \n",
      "\n",
      "      Degree_Cl  Degree_N  Degree_C  Degree_F  Degree_S  Degree_O  Aromatic  \\\n",
      "4765          0         3         3         0         0         1         6   \n",
      "4772          0         3         3         0         0         1         6   \n",
      "4766          0         3         3         0         0         1         6   \n",
      "4767          0         3         3         0         0         1         6   \n",
      "4585          0         3         3         0         0         2         6   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "607           0         3         3         0         0         1         0   \n",
      "726           0         3         3         0         0         1         5   \n",
      "772           0         3         3         0         0         1         5   \n",
      "5468          0         3         3         0         0         2        11   \n",
      "5397          0         2         3         0         0         1        12   \n",
      "\n",
      "      Overall_Formal_Charge  Is_Aromatic  \n",
      "4765                     64            1  \n",
      "4772                     64            1  \n",
      "4766                     64            1  \n",
      "4767                     64            1  \n",
      "4585                     64            1  \n",
      "...                     ...          ...  \n",
      "607                      98            0  \n",
      "726                      90            1  \n",
      "772                      90            1  \n",
      "5468                     56            1  \n",
      "5397                     75            1  \n",
      "\n",
      "[5568 rows x 12 columns]\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "1192     8  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -7.300000   \n",
      "1145    15  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -6.520000   \n",
      "841     17  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -7.290000   \n",
      "1146    18  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.750000   \n",
      "755     19  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@H]...     -6.950000   \n",
      "...    ...                                                ...           ...   \n",
      "1147  8475  CC(C)C[C@@H]1NC(=O)[C@@H](CC(C)C)N(C)C(=O)[C@H...     -5.960000   \n",
      "174   8483  CC(C)C[C@@H]1NC(=O)[C@H](C)N(C)C(=O)[C@H](CC2C...     -6.009141   \n",
      "1193  8487  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -6.495000   \n",
      "122   8513  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](CC...     -8.200000   \n",
      "201   8515  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.566211   \n",
      "\n",
      "      Degree_Cl  Degree_N  Degree_C  Degree_F  Degree_S  Degree_O  Aromatic  \\\n",
      "1192          0         3         3         0         0         1         6   \n",
      "1145          0         3         3         0         0         2         6   \n",
      "841           0         3         3         0         0         2         6   \n",
      "1146          0         3         3         0         0         2         6   \n",
      "755           0         3         3         0         0         2         6   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "1147          0         3         3         0         0         1         6   \n",
      "174           0         3         3         0         0         1         5   \n",
      "1193          0         3         3         0         0         1         6   \n",
      "122           0         3         3         0         0         1         0   \n",
      "201           0         3         3         0         0         1         5   \n",
      "\n",
      "      Overall_Formal_Charge  Is_Aromatic  \n",
      "1192                     64            1  \n",
      "1145                     64            1  \n",
      "841                      73            1  \n",
      "1146                     64            1  \n",
      "755                      73            1  \n",
      "...                     ...          ...  \n",
      "1147                     64            1  \n",
      "174                      90            1  \n",
      "1193                     64            1  \n",
      "122                     106            0  \n",
      "201                      90            1  \n",
      "\n",
      "[1392 rows x 12 columns]\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataframe pairs:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Training models:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008198 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56912\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.747671\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007835 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56937\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.741826\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56933\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.740654\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008593 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56935\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.749020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007551 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56925\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.737478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008718 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56928\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.747212\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56933\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.750687\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007543 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56914\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.743084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007528 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56925\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.737254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007630 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56933\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  10%|█         | 1/10 [00:07<01:09,  7.69s/it]\u001b[A\n",
      "Training models:  20%|██        | 2/10 [00:49<03:41, 27.70s/it]\u001b[A\n",
      "Training models:  30%|███       | 3/10 [04:57<14:58, 128.40s/it]\u001b[A\n",
      "Training models:  40%|████      | 4/10 [06:12<10:42, 107.10s/it]\u001b[A\n",
      "Training models:  50%|█████     | 5/10 [06:19<05:56, 71.30s/it] \u001b[A\n",
      "Training models:  60%|██████    | 6/10 [06:33<03:26, 51.66s/it]\u001b[A\n",
      "Training models:  70%|███████   | 7/10 [06:36<01:47, 35.79s/it]\u001b[A\n",
      "Training models:  80%|████████  | 8/10 [07:04<01:06, 33.25s/it]\u001b[A\n",
      "Training models:  90%|█████████ | 9/10 [07:28<00:30, 30.38s/it]\u001b[A\n",
      "Training models: 100%|██████████| 10/10 [07:39<00:00, 45.95s/it]\u001b[A\n",
      "Processing dataframe pairs:  25%|██▌       | 1/4 [07:39<22:58, 459.46s/it]\n",
      "Training models:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4196\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.747671\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041946 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4207\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.741826\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042760 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4200\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.740654\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4188\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1103\n",
      "[LightGBM] [Info] Start training from score -5.749020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042536 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4196\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.737478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4197\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.747212\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042375 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4200\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.750687\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043120 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4199\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.743084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042428 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4192\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 1104\n",
      "[LightGBM] [Info] Start training from score -5.737254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4189\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  10%|█         | 1/10 [00:10<01:35, 10.59s/it]\u001b[A\n",
      "Training models:  20%|██        | 2/10 [00:25<01:43, 12.91s/it]\u001b[A\n",
      "Training models:  30%|███       | 3/10 [01:36<04:36, 39.44s/it]\u001b[A\n",
      "Training models:  40%|████      | 4/10 [02:47<05:11, 51.89s/it]\u001b[A\n",
      "Training models:  50%|█████     | 5/10 [02:55<03:00, 36.12s/it]\u001b[A\n",
      "Training models:  60%|██████    | 6/10 [03:19<02:08, 32.02s/it]\u001b[A\n",
      "Training models:  70%|███████   | 7/10 [03:54<01:39, 33.08s/it]\u001b[A\n",
      "Training models:  80%|████████  | 8/10 [05:38<01:51, 55.62s/it]\u001b[A\n",
      "Training models:  90%|█████████ | 9/10 [06:31<00:54, 54.69s/it]\u001b[A\n",
      "Training models: 100%|██████████| 10/10 [06:34<00:00, 39.43s/it]\u001b[A\n",
      "Processing dataframe pairs:  50%|█████     | 2/4 [14:13<14:02, 421.12s/it]\n",
      "Training models:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016912 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.747671\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.741826\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016960 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.740654\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017245 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.749020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017162 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.737478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017039 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.747212\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017148 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.750687\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017171 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.743084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.737254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  10%|█         | 1/10 [00:07<01:11,  8.00s/it]\u001b[A\n",
      "Training models:  20%|██        | 2/10 [01:07<05:07, 38.49s/it]\u001b[A\n",
      "Training models:  30%|███       | 3/10 [24:32<1:17:17, 662.46s/it]\u001b[A\n",
      "Training models:  40%|████      | 4/10 [29:45<52:27, 524.55s/it]  \u001b[A\n",
      "Training models:  50%|█████     | 5/10 [30:10<28:41, 344.33s/it]\u001b[A\n",
      "Training models:  60%|██████    | 6/10 [30:53<16:07, 241.82s/it]\u001b[A\n",
      "Training models:  70%|███████   | 7/10 [31:21<08:36, 172.04s/it]\u001b[A\n",
      "Training models:  80%|████████  | 8/10 [32:26<04:35, 137.94s/it]\u001b[A\n",
      "Training models:  90%|█████████ | 9/10 [33:13<01:49, 109.49s/it]\u001b[A\n",
      "Training models: 100%|██████████| 10/10 [34:08<00:00, 204.82s/it][A\n",
      "Processing dataframe pairs:  75%|███████▌  | 3/4 [48:21<19:24, 1164.09s/it]\n",
      "Training models:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.747671\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.741826\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000752 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 151\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.740654\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 150\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.749020\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000737 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.737478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.747212\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.750687\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000741 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.743084\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.737254\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 151\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models:  10%|█         | 1/10 [00:05<00:47,  5.29s/it]\u001b[A\n",
      "Training models:  20%|██        | 2/10 [00:09<00:37,  4.64s/it]\u001b[A\n",
      "Training models:  30%|███       | 3/10 [00:11<00:22,  3.23s/it]\u001b[A\n",
      "Training models:  40%|████      | 4/10 [00:11<00:13,  2.21s/it]\u001b[A\n",
      "Training models:  50%|█████     | 5/10 [00:13<00:10,  2.04s/it]\u001b[A\n",
      "Training models:  60%|██████    | 6/10 [00:16<00:09,  2.48s/it]\u001b[A\n",
      "Training models:  70%|███████   | 7/10 [00:17<00:05,  1.84s/it]\u001b[A\n",
      "Training models:  80%|████████  | 8/10 [00:25<00:07,  3.75s/it]\u001b[A\n",
      "Training models: 100%|██████████| 10/10 [00:40<00:00,  4.09s/it][A\n",
      "Processing dataframe pairs: 100%|██████████| 4/4 [49:02<00:00, 735.73s/it] \n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dimensions of meta_features_train: (5568, 40)\n",
      "Dimensions of meta_features_test: (1392, 40)\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Stage 1 completed\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072987 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10102\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.747671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001654 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10101\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.741826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001715 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10095\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.740654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001641 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10097\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.749020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001687 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10093\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.737478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001659 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10092\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.747212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001678 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10094\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.750687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001637 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10097\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.743084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001711 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10091\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.737254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001674 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10097\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "\n",
    "os.makedirs('/home/users/akshay/PCPpred/PAMPA/pampa_models/', exist_ok=True)\n",
    "\n",
    "# 2D and 3D descriptors dataframes\n",
    "df_desc_train = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Descriptors/Train_2d_3d_all_descriptors.csv')\n",
    "df_train = df_desc_train.sort_values(by='ID')\n",
    "df_train = df_train.dropna()\n",
    "train = df_train.drop(['ID','SMILES'], axis=1)\n",
    "train = train.select_dtypes(include=['number'])\n",
    "train, _ = remove_low_variance_columns(train)\n",
    "selected_features_desc = features(train, \"Permeability\")\n",
    "joblib.dump(selected_features_desc, '/home/users/akshay/PCPpred/PAMPA/pampa_models/selected_features_descriptors.joblib')\n",
    "df_desc_train = pd.concat([df_train[['ID','SMILES','Permeability']], df_train[selected_features_desc]], axis=1)\n",
    "df_desc_test = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Descriptors/Test_2d_3d_all_descriptors.csv')\n",
    "df_desc_test = df_desc_test.sort_values(by='ID')\n",
    "df_desc_test = df_desc_test.dropna()\n",
    "df_desc_test = df_desc_test[df_desc_train.columns]\n",
    "\n",
    "# Fingerprints\n",
    "df_fp_train = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Fingerprints/Train/All_fingerprints_train.csv')\n",
    "df_train = df_fp_train.sort_values(by='ID')\n",
    "df_train = df_train.dropna()\n",
    "train = df_train.drop(['ID','SMILES'], axis=1)\n",
    "train = train.select_dtypes(include=['number'])\n",
    "train, _ = remove_low_variance_columns(train)\n",
    "selected_features_fp = features(train, \"Permeability\")\n",
    "joblib.dump(selected_features_fp, '/home/users/akshay/PCPpred/PAMPA/pampa_models/selected_features_fingerprints.joblib')\n",
    "df_fp_train = pd.concat([df_train[['ID','SMILES','Permeability']], df_train[selected_features_fp]], axis=1)\n",
    "df_fp_test = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Fingerprints/Test/All_fingerprints_test.csv')\n",
    "df_fp_test = df_fp_test.sort_values(by='ID')\n",
    "df_fp_test = df_fp_test.dropna()\n",
    "df_fp_test = df_fp_test[df_fp_train.columns]\n",
    "\n",
    "# Smiles Embeddings\n",
    "df_emb_train = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Embeddings/Train_MoLFormer-XL-both-10pct_model_1_fine_tuned_embeddings.csv')\n",
    "df_train = df_emb_train.sort_values(by='ID')\n",
    "df_train = df_train.dropna()\n",
    "train = df_train.drop(['ID','SMILES'], axis=1)\n",
    "train = train.select_dtypes(include=['number'])\n",
    "train, _ = remove_low_variance_columns(train)\n",
    "selected_features_emb = features(train, \"Permeability\")\n",
    "joblib.dump(selected_features_emb, '/home/users/akshay/PCPpred/PAMPA/pampa_models/selected_features_embeddings.joblib')\n",
    "df_emb_train = pd.concat([df_train[['ID','SMILES','Permeability']], df_train[selected_features_emb]], axis=1)\n",
    "df_emb_test = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Embeddings/Test_MoLFormer-XL-both-10pct_model_1_fine_tuned_embeddings.csv')\n",
    "df_emb_test = df_emb_test.sort_values(by='ID')\n",
    "df_emb_test = df_emb_test.dropna()\n",
    "df_emb_test = df_emb_test[df_emb_train.columns]\n",
    "\n",
    "# Atomic features\n",
    "df_atomic_train = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Atomic/Train_all_atomic_desc.csv')\n",
    "df_train = df_atomic_train.sort_values(by='ID')\n",
    "df_train = df_train.dropna()\n",
    "train = df_train.drop(['ID','SMILES'], axis=1)\n",
    "train = train.select_dtypes(include=['number'])\n",
    "train, _ = remove_low_variance_columns(train)\n",
    "selected_features_atomic = features(train, \"Permeability\")\n",
    "joblib.dump(selected_features_atomic, '/home/users/akshay/PCPpred/PAMPA/pampa_models/selected_features_atomic.joblib')\n",
    "df_atomic_train = pd.concat([df_train[['ID','SMILES','Permeability']], df_train[selected_features_atomic]], axis=1)\n",
    "df_atomic_test = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Atomic/Test_all_atomic_desc.csv')\n",
    "df_atomic_test = df_atomic_test.sort_values(by='ID')\n",
    "df_atomic_test = df_atomic_test.dropna()\n",
    "df_atomic_test = df_atomic_test[df_atomic_train.columns]\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Data Loading completed')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "\n",
    "# Filter dataframes to have consistent IDs\n",
    "df_fp_test = df_fp_test[df_fp_test['ID'].isin(df_desc_test['ID'])]\n",
    "df_fp_train = df_fp_train[df_fp_train['ID'].isin(df_desc_train['ID'])]\n",
    "df_emb_test = df_emb_test[df_emb_test['ID'].isin(df_desc_test['ID'])]\n",
    "df_emb_train = df_emb_train[df_emb_train['ID'].isin(df_desc_train['ID'])]\n",
    "df_atomic_test = df_atomic_test[df_atomic_test['ID'].isin(df_desc_test['ID'])]\n",
    "df_atomic_train = df_atomic_train[df_atomic_train['ID'].isin(df_desc_train['ID'])]\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Data Processing completed')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print(df_desc_train.shape)\n",
    "print(df_desc_test.shape)\n",
    "print(df_fp_train.shape)\n",
    "print(df_fp_test.shape)\n",
    "print(df_emb_train.shape)\n",
    "print(df_emb_test.shape)\n",
    "print(df_atomic_train.shape)\n",
    "print(df_atomic_test.shape)\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print(df_desc_train)\n",
    "print(df_desc_test)\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print(df_fp_train)\n",
    "print(df_fp_test)\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print(df_emb_train)\n",
    "print(df_emb_test)\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print(df_atomic_train)\n",
    "print(df_atomic_test)\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "\n",
    "target_column = 'Permeability'\n",
    "\n",
    "def scale_features(df_train, df_test, feature_type):\n",
    "    scaler = StandardScaler()\n",
    "    train_features = df_train.drop(columns=['ID', 'SMILES', target_column])\n",
    "    test_features = df_test.drop(columns=['ID', 'SMILES', target_column])\n",
    "    scaler.fit(train_features)\n",
    "    train_scaled = pd.DataFrame(scaler.transform(train_features), columns=train_features.columns, index=df_train.index)\n",
    "    test_scaled = pd.DataFrame(scaler.transform(test_features), columns=test_features.columns, index=df_test.index)\n",
    "    df_train_scaled = pd.concat([df_train[['ID', 'SMILES', target_column]], train_scaled], axis=1)\n",
    "    df_test_scaled = pd.concat([df_test[['ID', 'SMILES', target_column]], test_scaled], axis=1)\n",
    "    # Save the scaler\n",
    "    joblib.dump(scaler, f'/home/users/akshay/PCPpred/PAMPA/pampa_models/scaler_{feature_type}.joblib')\n",
    "    return df_train_scaled, df_test_scaled\n",
    "\n",
    "df_desc_train, df_desc_test = scale_features(df_desc_train, df_desc_test, 'Descriptor')\n",
    "df_fp_train, df_fp_test = scale_features(df_fp_train, df_fp_test, 'Fingerprints')\n",
    "df_emb_train, df_emb_test = scale_features(df_emb_train, df_emb_test, 'Embeddings')\n",
    "df_atomic_train, df_atomic_test = scale_features(df_atomic_train, df_atomic_test , 'Atomic')\n",
    "\n",
    "models_weak = [\n",
    "    lgb.LGBMRegressor(objective='regression', metric='rmse', boosting_type='gbdt', num_leaves=31, learning_rate=0.05, random_state=101),\n",
    "    RandomForestRegressor(n_jobs=-1, random_state=101),\n",
    "    GradientBoostingRegressor(random_state=101),\n",
    "    AdaBoostRegressor(random_state=101),\n",
    "    xgb.XGBRegressor(random_state=101),\n",
    "    ExtraTreesRegressor(n_jobs=-1, n_estimators=100, random_state=101),\n",
    "    KNeighborsRegressor(),\n",
    "    SVR(),\n",
    "    MLPRegressor(random_state=101, max_iter=500),\n",
    "    DecisionTreeRegressor(random_state=101),\n",
    "]\n",
    "\n",
    "models_meta = [\n",
    "    lgb.LGBMRegressor(objective='regression', metric='rmse', boosting_type='gbdt', num_leaves=31, learning_rate=0.05, random_state=101),\n",
    "    DecisionTreeRegressor(random_state=101),\n",
    "    RandomForestRegressor(n_jobs=-1, random_state=101),\n",
    "    GradientBoostingRegressor(random_state=101),\n",
    "    AdaBoostRegressor(random_state=101),\n",
    "    xgb.XGBRegressor(random_state=101),\n",
    "    ExtraTreesRegressor(n_jobs=-1, n_estimators=100, random_state=101),\n",
    "    LinearRegression(),\n",
    "    KNeighborsRegressor(),\n",
    "    SVR(),\n",
    "    MLPRegressor(random_state=101)\n",
    "]\n",
    "\n",
    "dataframes = [(df_desc_train, df_desc_test), (df_fp_train, df_fp_test), (df_emb_train, df_emb_test), (df_atomic_train, df_atomic_test)]\n",
    "data_names = ['descriptors', 'fingerprints', 'embeddings', 'atomic']\n",
    "\n",
    "meta_features_train = []\n",
    "meta_features_test = []\n",
    "\n",
    "# Stage 1: Train weak learners with 10-fold cross-validation\n",
    "for df_idx, (df_train, df_test) in enumerate(tqdm(dataframes, desc=\"Processing dataframe pairs\")):\n",
    "    X_weak = df_train.drop(columns=['ID', 'SMILES', target_column])\n",
    "    X_eval = df_test.drop(columns=['ID', 'SMILES', target_column])\n",
    "    y_weak = df_train[target_column]\n",
    "    y_eval = df_test[target_column]\n",
    "\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=101)\n",
    "\n",
    "    fold_meta_features_train = np.zeros((X_weak.shape[0], len(models_weak)))\n",
    "    fold_meta_features_test = np.zeros((X_eval.shape[0], len(models_weak)))\n",
    "\n",
    "    for i, model in tqdm(enumerate(models_weak), desc=\"Training models\", total=len(models_weak)):\n",
    "        fold_predictions = np.zeros(X_weak.shape[0])\n",
    "        test_predictions_folds = []\n",
    "\n",
    "        for fold_idx, (train_index, val_index) in enumerate(kf.split(X_weak)):\n",
    "            X_train, X_val = X_weak.iloc[train_index], X_weak.iloc[val_index]\n",
    "            y_train, y_val = y_weak.iloc[train_index], y_weak.iloc[val_index]\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            model_name = model.__class__.__name__\n",
    "            joblib.dump(model, f'/home/users/akshay/PCPpred/PAMPA/pampa_models/weak_{data_names[df_idx]}_{model_name}_fold_{fold_idx}.joblib')\n",
    "\n",
    "            fold_predictions[val_index] = np.clip(model.predict(X_val), -10, -3.9)\n",
    "\n",
    "            test_predictions_fold = np.clip(model.predict(X_eval), -10, -3.9)\n",
    "            test_predictions_folds.append(test_predictions_fold)\n",
    "\n",
    "        fold_meta_features_train[:, i] = fold_predictions\n",
    "        fold_meta_features_test[:, i] = np.mean(test_predictions_folds, axis=0)\n",
    "\n",
    "    meta_features_train.append(fold_meta_features_train)\n",
    "    meta_features_test.append(fold_meta_features_test)\n",
    "    \n",
    "    joblib.dump(fold_meta_features_train, f'/home/users/akshay/PCPpred/PAMPA/pampa_models/meta_features_train_{data_names[df_idx]}.joblib')\n",
    "    joblib.dump(fold_meta_features_test, f'/home/users/akshay/PCPpred/PAMPA/pampa_models/meta_features_test_{data_names[df_idx]}.joblib')\n",
    "\n",
    "meta_features_train = np.hstack(meta_features_train)\n",
    "meta_features_test = np.hstack(meta_features_test)\n",
    "\n",
    "joblib.dump(meta_features_train, '/home/users/akshay/PCPpred/PAMPA/pampa_models/meta_features_train_combined.joblib')\n",
    "joblib.dump(meta_features_test, '/home/users/akshay/PCPpred/PAMPA/pampa_models/meta_features_test_combined.joblib')\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print(\"Dimensions of meta_features_train:\", meta_features_train.shape)\n",
    "print(\"Dimensions of meta_features_test:\", meta_features_test.shape)\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Stage 1 completed')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "\n",
    "# Stage 2: Train the meta-learner using predictions from weak learners\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=101)\n",
    "results = {}\n",
    "predictions = []\n",
    "for model in models_meta:\n",
    "    model_name = model.__class__.__name__\n",
    "    predictions_train = []\n",
    "    actual_y_train = []\n",
    "    \n",
    "    test_predictions_folds = []\n",
    "\n",
    "    for fold_idx, (train_index, val_index) in enumerate(kf.split(meta_features_train)):\n",
    "        X_fold_train, X_fold_val = meta_features_train[train_index], meta_features_train[val_index]\n",
    "        y_fold_train, y_fold_val = y_weak.iloc[train_index], y_weak.iloc[val_index]\n",
    "        \n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "        \n",
    "        joblib.dump(model, f'/home/users/akshay/PCPpred/PAMPA/pampa_models/meta_{model_name}_fold_{fold_idx}.joblib')\n",
    "\n",
    "        y_pred_fold = model.predict(X_fold_val)\n",
    "        y_pred_fold = np.clip(y_pred_fold, -10, -3.9)\n",
    "        predictions_train.extend(y_pred_fold)\n",
    "        actual_y_train.extend(y_fold_val)\n",
    "\n",
    "        test_predictions_fold = model.predict(meta_features_test)\n",
    "        test_predictions_fold = np.clip(test_predictions_fold, -10, -3.9)\n",
    "        test_predictions_folds.append(test_predictions_fold)\n",
    "\n",
    "    # Metrics\n",
    "    predictions_test_mean = np.mean(test_predictions_folds, axis=0)\n",
    "    predictions_test_std = np.std(test_predictions_folds, axis=0)\n",
    "\n",
    "    mse_train = mean_squared_error(actual_y_train, predictions_train)\n",
    "    mae_train = mean_absolute_error(actual_y_train, predictions_train)\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    r2_train = r2_score(actual_y_train, predictions_train)\n",
    "    pearson_train, _ = pearsonr(actual_y_train, predictions_train)\n",
    "    spearman_train, _ = spearmanr(actual_y_train, predictions_train)\n",
    "\n",
    "    mse_test = mean_squared_error(y_eval, predictions_test_mean)\n",
    "    mae_test = mean_absolute_error(y_eval, predictions_test_mean)\n",
    "    rmse_test = np.sqrt(mse_test)\n",
    "    r2_test = r2_score(y_eval, predictions_test_mean)\n",
    "    pearson_test, _ = pearsonr(y_eval, predictions_test_mean)\n",
    "    spearman_test, _ = spearmanr(y_eval, predictions_test_mean)\n",
    "    \n",
    "\n",
    "    predictions.append({\n",
    "        'Model': model_name,\n",
    "        'Y Train pred': predictions_train,\n",
    "        'Y Test actual': y_eval,\n",
    "        'Test prediction folds': test_predictions_folds,\n",
    "        'Test Predictions Mean': predictions_test_mean,\n",
    "        'Test Predictions Std': predictions_test_mean,\n",
    "    })\n",
    "\n",
    "    results[model_name] = {\n",
    "        'Train MSE (10 fold CV)': mse_train,\n",
    "        'Train MAE (10 fold CV)': mae_train,\n",
    "        'Train RMSE (10 fold CV)': rmse_train,\n",
    "        'Train R2 (10 fold CV)': r2_train,\n",
    "        'Train PCC (10 fold CV)': pearson_train,\n",
    "        'Train SCC (10 fold CV)': spearman_train,\n",
    "        'Test MSE': mse_test,\n",
    "        'Test MAE': mae_test,\n",
    "        'Test RMSE': rmse_test,\n",
    "        'Test R2': r2_test,\n",
    "        'Test PCC': pearson_test,\n",
    "        'Test SCC': spearman_test,\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127dd4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429e947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ablations study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3063f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression  # LogisticRegression is not used for regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03263c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_low_variance_columns(df, threshold=0.005):\n",
    "    # df = df.drop(['ID','SMILES','Permeability'],axis=1)\n",
    "    variances = df.var()\n",
    "    \n",
    "    low_variance_columns = variances[variances < threshold].index.tolist()\n",
    "    \n",
    "    df_cleaned = df.drop(columns=low_variance_columns)\n",
    "    \n",
    "    return df_cleaned, low_variance_columns\n",
    "\n",
    "def features(df, target_column='Permeability', threshold=0.9):\n",
    "    correlation_matrix = df.corr()\n",
    "    \n",
    "    features_to_drop = set()\n",
    "    \n",
    "    for feature in correlation_matrix.columns:\n",
    "        if feature == target_column:\n",
    "            continue \n",
    "        target_corr = correlation_matrix[target_column][feature]\n",
    "        \n",
    "        for other_feature in correlation_matrix.columns:\n",
    "            if other_feature == feature or other_feature == target_column:\n",
    "                continue\n",
    "            \n",
    "            if abs(correlation_matrix[feature][other_feature]) > threshold:\n",
    "                other_target_corr = correlation_matrix[target_column][other_feature]\n",
    "\n",
    "                if abs(other_target_corr) < abs(target_corr):\n",
    "                    features_to_drop.add(other_feature)\n",
    "                else:\n",
    "                    features_to_drop.add(feature)\n",
    "    selected_features = [col for col in df.columns if col not in features_to_drop and col != target_column]\n",
    "    \n",
    "    return selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "792904f2-18b2-419d-9660-08f2edcadfe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524149/3482494709.py:2: DtypeWarning: Columns (1275,1277,1280,1285,1292,1298,1304,1354,1356,1359,1364,1371,1377,1383,1579,1580,1581,1583,1584,1590,1595,1596,1597) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_desc_train = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Descriptors/Train_2d_3d_all_descriptors.csv')\n",
      "/tmp/ipykernel_524149/3482494709.py:10: DtypeWarning: Columns (1275,1277,1292,1298,1354,1356,1371,1377,1579,1580,1581,1583,1584,1595,1596,1597) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_desc_test = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Descriptors/Test_2d_3d_all_descriptors.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Data Loading completed\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "(5568, 271)\n",
      "(1392, 271)\n",
      "(5568, 1128)\n",
      "(1392, 1128)\n",
      "(5568, 758)\n",
      "(1392, 758)\n",
      "(5568, 12)\n",
      "(1392, 12)\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Data Processing completed\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "(5568, 271)\n",
      "(1392, 271)\n",
      "(5568, 1128)\n",
      "(1392, 1128)\n",
      "(5568, 758)\n",
      "(1392, 758)\n",
      "(5568, 12)\n",
      "(1392, 12)\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "4765     3  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.000000   \n",
      "4772     4  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.100000   \n",
      "4766     6  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.300000   \n",
      "4767     7  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.300000   \n",
      "4585    11  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.470000   \n",
      "...    ...                                                ...           ...   \n",
      "607   8512  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](C(...     -6.640000   \n",
      "726   8514  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -5.929053   \n",
      "772   8516  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.199359   \n",
      "5468  8517  CC[C@@H]1c2nnc(o2)[C@H](Cc2ccccc2)NC(=O)CNC(=O...     -6.780000   \n",
      "5397  8518  NCCCC[C@@H]1NC(=O)[C@H](Cc2ccccc2)NC(=O)[C@H](...     -5.505000   \n",
      "\n",
      "      MinEStateIndex       qed       SPS  FpDensityMorgan1  BCUT2D_MWHI  \\\n",
      "4765        0.423845 -0.582130 -0.083445         -0.544377    -0.255732   \n",
      "4772        0.423845 -0.582130 -0.083445         -0.544377    -0.255732   \n",
      "4766        0.423845 -0.582130 -0.083445         -0.544377    -0.255732   \n",
      "4767        0.423845 -0.582130 -0.083445         -0.544377    -0.255732   \n",
      "4585        0.616720 -0.412259 -0.263969         -0.463275    -0.202503   \n",
      "...              ...       ...       ...               ...          ...   \n",
      "607         0.192516 -0.775650  0.259993         -2.407555    -0.272146   \n",
      "726         0.653737 -0.018994  1.601452         -0.588614    -0.273699   \n",
      "772         0.632929 -0.119292  1.086333         -0.700759    -0.273718   \n",
      "5468        1.217004  1.450851 -0.469980          2.939673    -0.227462   \n",
      "5397       -0.865635 -1.498214 -1.692007          0.059878    -0.255008   \n",
      "\n",
      "        AvgIpc  BalabanJ_x  ...      RNCS    LOBMAX    LOBMIN   MOMI-XY  \\\n",
      "4765 -0.023178    0.366371  ...  5.020426 -0.327012 -0.489038  0.266542   \n",
      "4772 -0.023178    0.366371  ...  2.590824  0.131149 -1.095841 -0.505283   \n",
      "4766 -0.023178    0.366371  ...  2.513747  0.238967  0.707090  0.072232   \n",
      "4767 -0.023178    0.366371  ...  2.533854 -0.692595 -0.836966 -0.608377   \n",
      "4585  0.013981    0.379551  ...  1.763717  1.273519  1.549498 -0.585647   \n",
      "...        ...         ...  ...       ...       ...       ...       ...   \n",
      "607  -0.094881    5.092978  ... -0.102534  0.460486  0.914258 -0.137682   \n",
      "726   0.694517   -1.332423  ...  0.870492 -0.662150 -0.976313  0.961296   \n",
      "772   0.638444   -0.952883  ...  0.126184 -0.213779  0.029722  0.975247   \n",
      "5468  0.080729   -0.661118  ... -1.035147  0.502289 -0.168095 -0.134893   \n",
      "5397 -0.635613    0.246641  ...  0.337173  4.135680  2.983486 -2.197778   \n",
      "\n",
      "      geomShape    RDF55p       P1m        Dv       L3p        Ki  \n",
      "4765   0.429391 -1.250047 -0.635846  0.724053 -0.517977 -0.255556  \n",
      "4772   1.091360 -1.030083 -0.203460 -2.226817  0.397570 -1.294787  \n",
      "4766  -0.175018 -1.437993  0.034947  0.462258 -0.716847 -0.078421  \n",
      "4767   0.679871 -0.929081 -0.839721 -1.495106  0.448322 -1.515073  \n",
      "4585   0.440959 -0.686745  0.705614 -1.460487 -0.494421 -0.063346  \n",
      "...         ...       ...       ...       ...       ...       ...  \n",
      "607   -0.023103  1.381124  0.575002  1.270101  0.105768  0.374513  \n",
      "726    0.400530  0.922606 -0.148038 -1.060872 -0.536387  0.423812  \n",
      "772    1.085739 -0.423244 -0.190998 -0.208842 -0.348220  0.370241  \n",
      "5468   1.234705 -2.128040 -0.089959 -0.638086 -0.925282  0.211735  \n",
      "5397   1.055760 -1.314734  1.798569 -1.487228 -0.339001  2.168920  \n",
      "\n",
      "[5568 rows x 271 columns]\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "1192     8  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -7.300000   \n",
      "1145    15  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -6.520000   \n",
      "841     17  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -7.290000   \n",
      "1146    18  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.750000   \n",
      "755     19  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@H]...     -6.950000   \n",
      "...    ...                                                ...           ...   \n",
      "1147  8475  CC(C)C[C@@H]1NC(=O)[C@@H](CC(C)C)N(C)C(=O)[C@H...     -5.960000   \n",
      "174   8483  CC(C)C[C@@H]1NC(=O)[C@H](C)N(C)C(=O)[C@H](CC2C...     -6.009141   \n",
      "1193  8487  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -6.495000   \n",
      "122   8513  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](CC...     -8.200000   \n",
      "201   8515  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.566211   \n",
      "\n",
      "      MinEStateIndex       qed       SPS  FpDensityMorgan1  BCUT2D_MWHI  \\\n",
      "1192        0.423845 -0.582130 -0.083445         -0.544377    -0.255732   \n",
      "1145        0.616720 -0.412259 -0.263969         -0.463275    -0.202503   \n",
      "841         0.523673  0.155095  1.135189         -1.046749    -0.202495   \n",
      "1146        0.616720 -0.412259 -0.263969         -0.463275    -0.202503   \n",
      "755         0.471903 -0.491085  0.431312         -1.132457    -0.202496   \n",
      "...              ...       ...       ...               ...          ...   \n",
      "1147        0.424645 -0.287590 -0.115302         -0.087259    -0.255729   \n",
      "174         0.640728 -0.049837  1.524206         -1.029680    -0.273694   \n",
      "1193        0.423845 -0.582130 -0.083445         -0.544377    -0.255732   \n",
      "122        -0.506947 -1.372119 -0.235774         -2.058103    -0.271840   \n",
      "201         0.638770  0.031128  1.205006         -0.368018    -0.273707   \n",
      "\n",
      "        AvgIpc  BalabanJ_x  ...      RNCS    LOBMAX    LOBMIN   MOMI-XY  \\\n",
      "1192 -0.023178    0.366371  ...  2.574068 -0.821963 -1.083327  1.060758   \n",
      "1145  0.013981    0.379551  ...  1.588178 -0.241147  0.251452 -0.551026   \n",
      "841   0.520753   -0.868326  ...  1.308687 -0.513032 -1.127321  1.085821   \n",
      "1146  0.013981    0.379551  ...  1.841734 -0.458404 -0.736761  0.822209   \n",
      "755   0.390021   -0.316906  ...  1.484576 -0.470343 -0.250467  0.933780   \n",
      "...        ...         ...  ...       ...       ...       ...       ...   \n",
      "1147  0.005182    0.457186  ...  2.394727 -1.346452 -1.111705  0.412881   \n",
      "174   0.702472   -1.307483  ...  1.030806 -0.290446  0.008297  1.548563   \n",
      "1193 -0.023178    0.366371  ...  2.553961 -1.241267 -1.131854  0.930482   \n",
      "122  -2.350518    5.422188  ...  0.719497 -1.136004 -0.674079  0.042385   \n",
      "201   0.631631   -0.948414  ... -0.008013 -1.068484 -1.150767  2.145945   \n",
      "\n",
      "      geomShape    RDF55p       P1m        Dv       L3p        Ki  \n",
      "1192  -0.355074 -1.000562 -0.284243 -0.133650 -1.438494  0.379207  \n",
      "1145   0.020201 -0.966691 -0.230741  0.710936  0.119026 -0.392815  \n",
      "841   -0.858263 -0.400679 -0.134555 -1.026565 -1.335448  0.567891  \n",
      "1146  -0.123442 -0.913022 -0.362147 -1.465924 -0.734346 -0.029197  \n",
      "755   -0.248220 -0.538319 -0.065438 -0.974718 -1.192115  0.484648  \n",
      "...         ...       ...       ...       ...       ...       ...  \n",
      "1147  -0.638297 -0.515289 -1.129021 -0.319862  0.132162 -0.877849  \n",
      "174    0.679744  0.193513 -1.171116  0.890599  0.724314 -0.163931  \n",
      "1193   0.306923 -0.784517 -1.401740 -1.610402 -0.648101 -0.333739  \n",
      "122   -2.669840  1.757360 -0.714835  2.143854  2.270087 -0.896843  \n",
      "201    0.429596 -0.270032 -0.719844 -0.924084 -1.074489  0.707120  \n",
      "\n",
      "[1392 rows x 271 columns]\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "4765     3  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.000000   \n",
      "4772     4  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.100000   \n",
      "4766     6  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.300000   \n",
      "4767     7  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.300000   \n",
      "4585    11  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.470000   \n",
      "...    ...                                                ...           ...   \n",
      "607   8512  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](C(...     -6.640000   \n",
      "726   8514  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -5.929053   \n",
      "772   8516  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.199359   \n",
      "5468  8517  CC[C@@H]1c2nnc(o2)[C@H](Cc2ccccc2)NC(=O)CNC(=O...     -6.780000   \n",
      "5397  8518  NCCCC[C@@H]1NC(=O)[C@H](Cc2ccccc2)NC(=O)[C@H](...     -5.505000   \n",
      "\n",
      "      Morgan_fp_2  Morgan_fp_5  Morgan_fp_13  Morgan_fp_19  Morgan_fp_42  \\\n",
      "4765    -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "4772    -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "4766    -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "4767    -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "4585    -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "...           ...          ...           ...           ...           ...   \n",
      "607     -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "726      2.326961     0.122258     -0.181197     -0.768672     -0.296163   \n",
      "772      2.326961     0.122258     -0.181197     -0.768672     -0.296163   \n",
      "5468    -0.429745     0.122258     -0.181197     -0.768672     -0.296163   \n",
      "5397    -0.429745    -8.179391     -0.181197     -0.768672     -0.296163   \n",
      "\n",
      "      Morgan_fp_84  Morgan_fp_94  ...   SubFPC2  SubFPC100  SubFPC101  \\\n",
      "4765     -0.373919     -0.406921  ... -0.639589   -0.50779  -0.605817   \n",
      "4772     -0.373919     -0.406921  ... -0.639589   -0.50779  -0.605817   \n",
      "4766     -0.373919     -0.406921  ... -0.639589   -0.50779  -0.605817   \n",
      "4767     -0.373919     -0.406921  ... -0.639589   -0.50779  -0.605817   \n",
      "4585     -0.373919     -0.406921  ... -0.639589   -0.50779  -0.605817   \n",
      "...            ...           ...  ...       ...        ...        ...   \n",
      "607      -0.373919     -0.406921  ... -0.639589   -0.50779  -0.605817   \n",
      "726      -0.373919     -0.406921  ...  6.363355   -0.50779  -0.605817   \n",
      "772      -0.373919     -0.406921  ...  4.495903   -0.50779  -0.605817   \n",
      "5468     -0.373919     -0.406921  ... -1.573315   -0.50779  -0.605817   \n",
      "5397     -0.373919     -0.406921  ... -1.106452   -0.50779  -0.605817   \n",
      "\n",
      "      SubFPC127  SubFPC135  SubFPC137  SubFPC143  SubFPC182  SubFPC275  \\\n",
      "4765  -0.208683  -0.060896  -0.040882   -0.05219  -0.093837  -0.943041   \n",
      "4772  -0.208683  -0.060896  -0.040882   -0.05219  -0.093837  -0.943041   \n",
      "4766  -0.208683  -0.060896  -0.040882   -0.05219  -0.093837  -0.943041   \n",
      "4767  -0.208683  -0.060896  -0.040882   -0.05219  -0.093837  -0.943041   \n",
      "4585  -0.208683  -0.060896  -0.040882   -0.05219  -0.093837  -0.943041   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "607    2.434504  -0.060896  -0.040882   -0.05219  -0.093837   1.818822   \n",
      "726   -0.208683  -0.060896  -0.040882   -0.05219  -0.093837   2.371194   \n",
      "772   -0.208683  -0.060896  -0.040882   -0.05219  -0.093837   2.371194   \n",
      "5468  -2.323232  -0.060896  -0.040882   -0.05219   9.581758   0.161704   \n",
      "5397  -0.737320  -0.060896  -0.040882   -0.05219  -0.093837  -1.495414   \n",
      "\n",
      "      SubFPC302  \n",
      "4765  -0.421116  \n",
      "4772  -0.421116  \n",
      "4766  -0.421116  \n",
      "4767  -0.421116  \n",
      "4585  -0.196025  \n",
      "...         ...  \n",
      "607    0.479248  \n",
      "726   -0.646207  \n",
      "772   -0.421116  \n",
      "5468  -1.546572  \n",
      "5397  -0.196025  \n",
      "\n",
      "[5568 rows x 1128 columns]\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "1192     8  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -7.300000   \n",
      "1145    15  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -6.520000   \n",
      "841     17  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -7.290000   \n",
      "1146    18  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.750000   \n",
      "755     19  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@H]...     -6.950000   \n",
      "...    ...                                                ...           ...   \n",
      "1147  8475  CC(C)C[C@@H]1NC(=O)[C@@H](CC(C)C)N(C)C(=O)[C@H...     -5.960000   \n",
      "174   8483  CC(C)C[C@@H]1NC(=O)[C@H](C)N(C)C(=O)[C@H](CC2C...     -6.009141   \n",
      "1193  8487  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -6.495000   \n",
      "122   8513  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](CC...     -8.200000   \n",
      "201   8515  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.566211   \n",
      "\n",
      "      Morgan_fp_2  Morgan_fp_5  Morgan_fp_13  Morgan_fp_19  Morgan_fp_42  \\\n",
      "1192    -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "1145    -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "841     -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "1146    -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "755     -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "...           ...          ...           ...           ...           ...   \n",
      "1147    -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "174      2.326961     0.122258     -0.181197     -0.768672     -0.296163   \n",
      "1193    -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "122     -0.429745     0.122258     -0.181197      1.300944     -0.296163   \n",
      "201      2.326961     0.122258     -0.181197     -0.768672     -0.296163   \n",
      "\n",
      "      Morgan_fp_84  Morgan_fp_94  ...   SubFPC2  SubFPC100  SubFPC101  \\\n",
      "1192     -0.373919     -0.406921  ... -0.639589   -0.50779  -0.605817   \n",
      "1145     -0.373919     -0.406921  ... -0.639589   -0.50779  -0.605817   \n",
      "841      -0.373919     -0.406921  ...  0.761000   -0.50779  -0.605817   \n",
      "1146     -0.373919     -0.406921  ... -0.639589   -0.50779  -0.605817   \n",
      "755      -0.373919     -0.406921  ...  0.294137   -0.50779  -0.605817   \n",
      "...            ...           ...  ...       ...        ...        ...   \n",
      "1147     -0.373919     -0.406921  ... -0.639589   -0.50779  -0.605817   \n",
      "174      -0.373919     -0.406921  ...  5.896492   -0.50779  -0.605817   \n",
      "1193     -0.373919     -0.406921  ... -0.639589   -0.50779  -0.605817   \n",
      "122      -0.373919     -0.406921  ...  0.761000   -0.50779  -0.605817   \n",
      "201      -0.373919     -0.406921  ...  4.029040   -0.50779  -0.605817   \n",
      "\n",
      "      SubFPC127  SubFPC135  SubFPC137  SubFPC143  SubFPC182  SubFPC275  \\\n",
      "1192  -0.208683  -0.060896  -0.040882   -0.05219  -0.093837  -0.943041   \n",
      "1145  -0.208683  -0.060896  -0.040882   -0.05219  -0.093837  -0.943041   \n",
      "841    0.319955  -0.060896  -0.040882   -0.05219  -0.093837  -0.390669   \n",
      "1146  -0.208683  -0.060896  -0.040882   -0.05219  -0.093837  -0.943041   \n",
      "755    0.319955  -0.060896  -0.040882   -0.05219  -0.093837  -0.390669   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "1147  -0.208683  -0.060896  -0.040882   -0.05219  -0.093837  -0.943041   \n",
      "174   -0.208683  -0.060896  -0.040882   -0.05219  -0.093837   2.371194   \n",
      "1193  -0.208683  -0.060896  -0.040882   -0.05219  -0.093837  -0.943041   \n",
      "122    2.434504  -0.060896  -0.040882   -0.05219  -0.093837   1.818822   \n",
      "201   -0.208683  -0.060896  -0.040882   -0.05219  -0.093837   2.371194   \n",
      "\n",
      "      SubFPC302  \n",
      "1192  -0.421116  \n",
      "1145  -0.196025  \n",
      "841   -0.646207  \n",
      "1146  -0.196025  \n",
      "755   -0.196025  \n",
      "...         ...  \n",
      "1147  -0.421116  \n",
      "174   -0.646207  \n",
      "1193  -0.421116  \n",
      "122    1.379613  \n",
      "201   -0.646207  \n",
      "\n",
      "[1392 rows x 1128 columns]\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "4765     3  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.000000   \n",
      "4772     4  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.100000   \n",
      "4766     6  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.300000   \n",
      "4767     7  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.300000   \n",
      "4585    11  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.470000   \n",
      "...    ...                                                ...           ...   \n",
      "607   8512  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](C(...     -6.640000   \n",
      "726   8514  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -5.929053   \n",
      "772   8516  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.199359   \n",
      "5468  8517  CC[C@@H]1c2nnc(o2)[C@H](Cc2ccccc2)NC(=O)CNC(=O...     -6.780000   \n",
      "5397  8518  NCCCC[C@@H]1NC(=O)[C@H](Cc2ccccc2)NC(=O)[C@H](...     -5.505000   \n",
      "\n",
      "      x_fine_emb_MFXL0  x_fine_emb_MFXL1  x_fine_emb_MFXL2  x_fine_emb_MFXL3  \\\n",
      "4765         -2.122422          1.147510         -0.627659          1.653762   \n",
      "4772         -3.267542          1.073391         -0.209054          0.443883   \n",
      "4766         -3.002931          1.295640          0.813363          1.314649   \n",
      "4767         -1.842162          1.054307          0.213031          1.793280   \n",
      "4585         -2.106422          1.215424         -3.512944         -0.676041   \n",
      "...                ...               ...               ...               ...   \n",
      "607          -1.851381          0.108503          2.362213         -4.212483   \n",
      "726           1.102033         -0.955575         -1.264599         -2.347400   \n",
      "772           0.750314         -0.668910         -1.050737         -2.285963   \n",
      "5468         -2.560984          1.011480          0.815804         -0.464647   \n",
      "5397         -1.621205          0.616807          0.970138          4.363248   \n",
      "\n",
      "      x_fine_emb_MFXL4  x_fine_emb_MFXL5  x_fine_emb_MFXL6  ...  \\\n",
      "4765          1.654852          2.011274          2.544794  ...   \n",
      "4772          2.186735          1.653986          1.932193  ...   \n",
      "4766          1.170899          1.436321          2.040036  ...   \n",
      "4767          1.289195          2.025044          2.685546  ...   \n",
      "4585          1.867048          1.625098          2.493037  ...   \n",
      "...                ...               ...               ...  ...   \n",
      "607           0.630250         -0.347428         -1.176430  ...   \n",
      "726          -0.851512         -1.684385          1.773667  ...   \n",
      "772          -0.859163         -1.548883          1.573688  ...   \n",
      "5468          1.284211         -0.182989         -0.476694  ...   \n",
      "5397         -2.722272          1.275290          1.091457  ...   \n",
      "\n",
      "      x_fine_emb_MFXL758  x_fine_emb_MFXL759  x_fine_emb_MFXL760  \\\n",
      "4765           -1.071306            1.868809           -1.581889   \n",
      "4772           -0.562022            2.539468           -0.814543   \n",
      "4766           -0.106171            1.776777           -0.572394   \n",
      "4767           -0.940032            1.438628           -1.248522   \n",
      "4585           -2.178598            2.874377           -2.088220   \n",
      "...                  ...                 ...                 ...   \n",
      "607            -0.866846            0.110616           -1.687932   \n",
      "726            -0.487984            0.295598            1.369837   \n",
      "772            -0.306684           -0.797541            1.473635   \n",
      "5468           -1.357991            2.009908           -1.225909   \n",
      "5397            0.495604            1.144872           -0.133799   \n",
      "\n",
      "      x_fine_emb_MFXL761  x_fine_emb_MFXL762  x_fine_emb_MFXL763  \\\n",
      "4765           -0.422563           -1.156847           -1.927335   \n",
      "4772           -0.419681           -0.967847           -3.831952   \n",
      "4766            0.344545           -1.288168           -2.681324   \n",
      "4767           -0.893246           -1.181363           -1.688922   \n",
      "4585           -1.196720           -0.697837           -0.355373   \n",
      "...                  ...                 ...                 ...   \n",
      "607            -2.731316            2.905098           -1.904500   \n",
      "726            -2.069568            0.573895            0.598811   \n",
      "772            -1.459228            0.596947            0.356315   \n",
      "5468            0.897100            1.963339           -0.680554   \n",
      "5397           -0.348794            0.415931           -2.701041   \n",
      "\n",
      "      x_fine_emb_MFXL764  x_fine_emb_MFXL765  x_fine_emb_MFXL766  \\\n",
      "4765            1.534425            2.238415            0.542137   \n",
      "4772            2.702819            2.253270           -0.404184   \n",
      "4766            1.699027            2.039126           -0.496907   \n",
      "4767            1.409813            2.139524            0.642545   \n",
      "4585            0.870366            0.745226           -0.501020   \n",
      "...                  ...                 ...                 ...   \n",
      "607             2.729993           -1.557689           -1.922636   \n",
      "726             1.522412           -1.513424           -0.375645   \n",
      "772             0.819689           -1.523811           -0.417917   \n",
      "5468            1.353444            0.638834           -0.991576   \n",
      "5397            1.946350            1.750392           -1.340038   \n",
      "\n",
      "      x_fine_emb_MFXL767  \n",
      "4765            2.905066  \n",
      "4772            3.304215  \n",
      "4766            2.924381  \n",
      "4767            2.537911  \n",
      "4585            1.531339  \n",
      "...                  ...  \n",
      "607             0.536152  \n",
      "726            -0.970615  \n",
      "772            -1.297706  \n",
      "5468            1.648650  \n",
      "5397            1.870947  \n",
      "\n",
      "[5568 rows x 758 columns]\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "1192     8  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -7.300000   \n",
      "1145    15  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -6.520000   \n",
      "841     17  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -7.290000   \n",
      "1146    18  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.750000   \n",
      "755     19  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@H]...     -6.950000   \n",
      "...    ...                                                ...           ...   \n",
      "1147  8475  CC(C)C[C@@H]1NC(=O)[C@@H](CC(C)C)N(C)C(=O)[C@H...     -5.960000   \n",
      "174   8483  CC(C)C[C@@H]1NC(=O)[C@H](C)N(C)C(=O)[C@H](CC2C...     -6.009141   \n",
      "1193  8487  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -6.495000   \n",
      "122   8513  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](CC...     -8.200000   \n",
      "201   8515  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.566211   \n",
      "\n",
      "      x_fine_emb_MFXL0  x_fine_emb_MFXL1  x_fine_emb_MFXL2  x_fine_emb_MFXL3  \\\n",
      "1192         -2.343506          1.139884         -1.019681          1.482168   \n",
      "1145         -1.874322          1.283046         -3.320531         -0.724183   \n",
      "841          -2.580788          1.081337         -3.492761         -0.422667   \n",
      "1146         -2.402330          0.916008         -4.223656         -0.389233   \n",
      "755          -2.332136          1.063892         -3.923251         -0.003484   \n",
      "...                ...               ...               ...               ...   \n",
      "1147         -0.684747          0.270972          0.323990          0.168266   \n",
      "174          -0.621277          0.438098         -0.102045         -1.783234   \n",
      "1193         -2.243296          1.192508         -0.978036          1.460417   \n",
      "122          -3.886704         -0.985142          1.869315         -3.656563   \n",
      "201           0.782988         -0.818559         -0.928657         -2.282151   \n",
      "\n",
      "      x_fine_emb_MFXL4  x_fine_emb_MFXL5  x_fine_emb_MFXL6  ...  \\\n",
      "1192          1.767813          1.884289          2.501027  ...   \n",
      "1145          1.806769          1.556525          2.613990  ...   \n",
      "841           1.857349          1.757706          2.207338  ...   \n",
      "1146          1.760759          1.042106          2.107901  ...   \n",
      "755           2.067028          1.239307          2.380293  ...   \n",
      "...                ...               ...               ...  ...   \n",
      "1147          0.256428          1.132567          1.891585  ...   \n",
      "174           0.879148          0.362656          1.511093  ...   \n",
      "1193          1.870237          1.965938          2.625938  ...   \n",
      "122           0.318389         -0.590369         -1.679794  ...   \n",
      "201          -0.831194         -1.527292          1.355936  ...   \n",
      "\n",
      "      x_fine_emb_MFXL758  x_fine_emb_MFXL759  x_fine_emb_MFXL760  \\\n",
      "1192           -1.117933            2.174261           -1.373366   \n",
      "1145           -2.224494            2.834726           -1.993815   \n",
      "841            -3.379367            3.047782           -1.733128   \n",
      "1146           -2.551050            2.637986           -1.633309   \n",
      "755            -3.372566            2.768433           -1.676417   \n",
      "...                  ...                 ...                 ...   \n",
      "1147           -0.321255            2.172985           -0.011186   \n",
      "174            -0.204461            1.177312            0.095418   \n",
      "1193           -1.284695            1.836633           -1.545386   \n",
      "122            -0.692843            1.338598           -1.659924   \n",
      "201            -0.022711           -0.817836            1.759791   \n",
      "\n",
      "      x_fine_emb_MFXL761  x_fine_emb_MFXL762  x_fine_emb_MFXL763  \\\n",
      "1192           -0.340619           -1.014842           -1.886554   \n",
      "1145           -1.089232           -0.754110           -0.065039   \n",
      "841            -0.439290           -2.217576           -0.547052   \n",
      "1146           -1.440142           -1.730826           -0.742176   \n",
      "755            -0.901384           -2.095378           -0.827348   \n",
      "...                  ...                 ...                 ...   \n",
      "1147            0.482989           -0.647298           -2.232294   \n",
      "174            -4.315770           -1.146564           -0.357660   \n",
      "1193           -0.355382           -1.261443           -1.803061   \n",
      "122             0.508099            1.404201           -4.118768   \n",
      "201            -1.093112            0.860230            0.224163   \n",
      "\n",
      "      x_fine_emb_MFXL764  x_fine_emb_MFXL765  x_fine_emb_MFXL766  \\\n",
      "1192            1.402676            2.352400            0.282934   \n",
      "1145            0.789885            0.639027           -0.544002   \n",
      "841             2.665555            0.737522           -1.150064   \n",
      "1146            1.311888            0.362385           -0.549036   \n",
      "755             2.044574            0.918320           -1.139046   \n",
      "...                  ...                 ...                 ...   \n",
      "1147            1.282146            1.606716            1.467822   \n",
      "174             2.761389           -0.447501            1.219025   \n",
      "1193            1.555169            2.209891            0.572275   \n",
      "122             5.337712            0.294290            2.863697   \n",
      "201             1.027944           -1.467143           -0.770435   \n",
      "\n",
      "      x_fine_emb_MFXL767  \n",
      "1192            3.243281  \n",
      "1145            1.456025  \n",
      "841             2.119310  \n",
      "1146            1.921803  \n",
      "755             1.936370  \n",
      "...                  ...  \n",
      "1147            1.972242  \n",
      "174            -0.154299  \n",
      "1193            3.061655  \n",
      "122             4.873982  \n",
      "201            -1.373395  \n",
      "\n",
      "[1392 rows x 758 columns]\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "4765     3  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.000000   \n",
      "4772     4  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.100000   \n",
      "4766     6  CC(C)C[C@@H]1NC(=O)[C@H](CC(C)C)NC(=O)[C@H](Cc...     -7.300000   \n",
      "4767     7  CC(C)C[C@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C@...     -7.300000   \n",
      "4585    11  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.470000   \n",
      "...    ...                                                ...           ...   \n",
      "607   8512  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](C(...     -6.640000   \n",
      "726   8514  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -5.929053   \n",
      "772   8516  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.199359   \n",
      "5468  8517  CC[C@@H]1c2nnc(o2)[C@H](Cc2ccccc2)NC(=O)CNC(=O...     -6.780000   \n",
      "5397  8518  NCCCC[C@@H]1NC(=O)[C@H](Cc2ccccc2)NC(=O)[C@H](...     -5.505000   \n",
      "\n",
      "      Degree_Cl   Degree_N  Degree_C  Degree_F  Degree_S  Degree_O  Aromatic  \\\n",
      "4765  -0.102598   0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -0.259371   \n",
      "4772  -0.102598   0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -0.259371   \n",
      "4766  -0.102598   0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -0.259371   \n",
      "4767  -0.102598   0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -0.259371   \n",
      "4585  -0.102598   0.091271 -0.173109 -0.102598 -0.211545  1.359442 -0.259371   \n",
      "...         ...        ...       ...       ...       ...       ...       ...   \n",
      "607   -0.102598   0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -1.193911   \n",
      "726   -0.102598   0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -0.415128   \n",
      "772   -0.102598   0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -0.415128   \n",
      "5468  -0.102598   0.091271 -0.173109 -0.102598 -0.211545  1.359442  0.519413   \n",
      "5397  -0.102598 -10.956435 -0.173109 -0.102598 -0.211545 -0.735596  0.675170   \n",
      "\n",
      "      Overall_Formal_Charge  Is_Aromatic  \n",
      "4765              -0.850527     0.640295  \n",
      "4772              -0.850527     0.640295  \n",
      "4766              -0.850527     0.640295  \n",
      "4767              -0.850527     0.640295  \n",
      "4585              -0.850527     0.640295  \n",
      "...                     ...          ...  \n",
      "607                0.729383    -1.561781  \n",
      "726                0.357640     0.640295  \n",
      "772                0.357640     0.640295  \n",
      "5468              -1.222271     0.640295  \n",
      "5397              -0.339380     0.640295  \n",
      "\n",
      "[5568 rows x 12 columns]\n",
      "        ID                                             SMILES  Permeability  \\\n",
      "1192     8  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -7.300000   \n",
      "1145    15  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -6.520000   \n",
      "841     17  COc1ccc(C[C@@H]2NC(=O)[C@@H](CC(C)C)NC(=O)[C@@...     -7.290000   \n",
      "1146    18  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@@H...     -6.750000   \n",
      "755     19  COc1ccc(C[C@@H]2NC(=O)[C@H](CC(C)C)NC(=O)[C@H]...     -6.950000   \n",
      "...    ...                                                ...           ...   \n",
      "1147  8475  CC(C)C[C@@H]1NC(=O)[C@@H](CC(C)C)N(C)C(=O)[C@H...     -5.960000   \n",
      "174   8483  CC(C)C[C@@H]1NC(=O)[C@H](C)N(C)C(=O)[C@H](CC2C...     -6.009141   \n",
      "1193  8487  CC(C)C[C@@H]1NC(=O)[C@H](Cc2ccc(O)cc2)NC(=O)[C...     -6.495000   \n",
      "122   8513  CCC[C@@H]1NC(=O)[C@H](CC(C)C)N(C)C(=O)[C@H](CC...     -8.200000   \n",
      "201   8515  CCC[C@H]1C(=O)N[C@@H](CC2CCCCC2)C(=O)N(C)[C@@H...     -6.566211   \n",
      "\n",
      "      Degree_Cl  Degree_N  Degree_C  Degree_F  Degree_S  Degree_O  Aromatic  \\\n",
      "1192  -0.102598  0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -0.259371   \n",
      "1145  -0.102598  0.091271 -0.173109 -0.102598 -0.211545  1.359442 -0.259371   \n",
      "841   -0.102598  0.091271 -0.173109 -0.102598 -0.211545  1.359442 -0.259371   \n",
      "1146  -0.102598  0.091271 -0.173109 -0.102598 -0.211545  1.359442 -0.259371   \n",
      "755   -0.102598  0.091271 -0.173109 -0.102598 -0.211545  1.359442 -0.259371   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "1147  -0.102598  0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -0.259371   \n",
      "174   -0.102598  0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -0.415128   \n",
      "1193  -0.102598  0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -0.259371   \n",
      "122   -0.102598  0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -1.193911   \n",
      "201   -0.102598  0.091271 -0.173109 -0.102598 -0.211545 -0.735596 -0.415128   \n",
      "\n",
      "      Overall_Formal_Charge  Is_Aromatic  \n",
      "1192              -0.850527     0.640295  \n",
      "1145              -0.850527     0.640295  \n",
      "841               -0.432315     0.640295  \n",
      "1146              -0.850527     0.640295  \n",
      "755               -0.432315     0.640295  \n",
      "...                     ...          ...  \n",
      "1147              -0.850527     0.640295  \n",
      "174                0.357640     0.640295  \n",
      "1193              -0.850527     0.640295  \n",
      "122                1.101127    -1.561781  \n",
      "201                0.357640     0.640295  \n",
      "\n",
      "[1392 rows x 12 columns]\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    }
   ],
   "source": [
    "# 2D and 3D descriptors dataframes\n",
    "df_desc_train = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Descriptors/Train_2d_3d_all_descriptors.csv')\n",
    "df_train = df_desc_train.sort_values(by='ID')\n",
    "df_train =df_train.dropna()\n",
    "train = df_train.drop(['ID','SMILES'],axis=1)\n",
    "train = train.select_dtypes(include=['number'])\n",
    "train, _ = remove_low_variance_columns(train)\n",
    "selected_features = features(train, \"Permeability\")\n",
    "df_desc_train = pd.concat( [df_train[['ID','SMILES','Permeability']],df_train[selected_features] ], axis=1)\n",
    "df_desc_test = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Descriptors/Test_2d_3d_all_descriptors.csv')\n",
    "df_desc_test = df_desc_test.sort_values(by='ID')\n",
    "df_desc_test =df_desc_test.dropna()\n",
    "df_desc_test =  df_desc_test[df_desc_train.columns]\n",
    "\n",
    "\n",
    "# Fingerprints\n",
    "df_fp_train = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Fingerprints/Train/All_fingerprints_train.csv')\n",
    "df_train = df_fp_train.sort_values(by='ID')\n",
    "df_train = df_train.dropna()\n",
    "train = df_train.drop(['ID','SMILES'],axis=1)\n",
    "train = train.select_dtypes(include=['number'])\n",
    "train, _ = remove_low_variance_columns(train)\n",
    "selected_features = features(train, \"Permeability\")\n",
    "df_fp_train = pd.concat( [df_train[['ID','SMILES','Permeability']],df_train[selected_features] ], axis=1)\n",
    "df_fp_test = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Fingerprints/Test/All_fingerprints_test.csv')\n",
    "df_fp_test = df_fp_test.sort_values(by='ID')\n",
    "df_fp_test = df_fp_test.dropna()\n",
    "df_fp_test =  df_fp_test[df_fp_train.columns]\n",
    "\n",
    "\n",
    "#Smiles Embeddings\n",
    "df_emb_train = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Embeddings/Train_MoLFormer-XL-both-10pct_model_1_fine_tuned_embeddings.csv')\n",
    "df_train = df_emb_train.sort_values(by='ID')\n",
    "df_train = df_train.dropna()\n",
    "train = df_train.drop(['ID','SMILES'],axis=1)\n",
    "train = train.select_dtypes(include=['number'])\n",
    "train, _ = remove_low_variance_columns(train)\n",
    "selected_features = features(train, \"Permeability\")\n",
    "df_emb_train = pd.concat( [df_train[['ID','SMILES','Permeability']],df_train[selected_features] ], axis=1)\n",
    "df_emb_test = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Embeddings/Test_MoLFormer-XL-both-10pct_model_1_fine_tuned_embeddings.csv')\n",
    "df_emb_test = df_emb_test.sort_values(by='ID')\n",
    "df_emb_test = df_emb_test.dropna()\n",
    "df_emb_test =  df_emb_test[df_emb_train.columns]\n",
    "\n",
    "#ATomic features\n",
    "df_atomic_train = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Atomic/Train_all_atomic_desc.csv')\n",
    "df_train = df_atomic_train.sort_values(by='ID')\n",
    "df_train = df_train.dropna()\n",
    "train = df_train.drop(['ID','SMILES'],axis=1)\n",
    "train = train.select_dtypes(include=['number'])\n",
    "train, _ = remove_low_variance_columns(train)\n",
    "selected_features = features(train, \"Permeability\")\n",
    "df_atomic_train = pd.concat( [df_train[['ID','SMILES','Permeability']],df_train[selected_features] ], axis=1)\n",
    "# df_atomic_train =pd.concat( [df_train['SMILES'], df_train.select_dtypes(include=['number'])], axis=1)\n",
    "df_atomic_test = pd.read_csv('/home/users/akshay/PCPpred/PAMPA/features/Atomic/Test_all_atomic_desc.csv')\n",
    "df_atomic_test = df_atomic_test.sort_values(by='ID')\n",
    "df_atomic_test = df_atomic_test.dropna()\n",
    "df_atomic_test =  df_atomic_test[df_atomic_train.columns]\n",
    "\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Data Loading completed')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "df_fp_test = df_fp_test[df_fp_test['ID'].isin(df_desc_test['ID'])]\n",
    "df_fp_train = df_fp_train[df_fp_train['ID'].isin(df_desc_train['ID'])]\n",
    "\n",
    "df_emb_test = df_emb_test[df_emb_test['ID'].isin(df_desc_test['ID'])]\n",
    "df_emb_train = df_emb_train[df_emb_train['ID'].isin(df_desc_train['ID'])]\n",
    "\n",
    "df_atomic_test = df_atomic_test[df_atomic_test['ID'].isin(df_desc_test['ID'])]\n",
    "df_atomic_train = df_atomic_train[df_atomic_train['ID'].isin(df_desc_train['ID'])]\n",
    "\n",
    "print(df_desc_train.shape)\n",
    "print(df_desc_test.shape)\n",
    "print(df_fp_train.shape)\n",
    "print(df_fp_test.shape)\n",
    "print(df_emb_train.shape)\n",
    "print(df_emb_test.shape)\n",
    "print(df_atomic_train.shape)\n",
    "print(df_atomic_test.shape)\n",
    "\n",
    "def scale_features(df_train, df_test):\n",
    "    scaler = StandardScaler()\n",
    "    train_features = df_train.drop(columns=['ID', 'SMILES', target_column])\n",
    "    test_features = df_test.drop(columns=['ID', 'SMILES', target_column])\n",
    "    scaler.fit(train_features)\n",
    "    train_scaled = pd.DataFrame(scaler.transform(train_features), columns=train_features.columns, index=df_train.index)\n",
    "    test_scaled = pd.DataFrame(scaler.transform(test_features), columns=test_features.columns, index=df_test.index)\n",
    "    df_train_scaled = pd.concat([df_train[['ID', 'SMILES', target_column]], train_scaled], axis=1)\n",
    "    df_test_scaled = pd.concat([df_test[['ID', 'SMILES', target_column]], test_scaled], axis=1)\n",
    "    return df_train_scaled, df_test_scaled\n",
    "\n",
    "target_column = 'Permeability'\n",
    "df_desc_train, df_desc_test = scale_features(df_desc_train, df_desc_test)\n",
    "df_fp_train, df_fp_test = scale_features(df_fp_train, df_fp_test)\n",
    "df_emb_train, df_emb_test = scale_features(df_emb_train, df_emb_test)\n",
    "df_atomic_train, df_atomic_test = scale_features(df_atomic_train, df_atomic_test)\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Data Processing completed')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print(df_desc_train.shape)\n",
    "print(df_desc_test.shape)\n",
    "print(df_fp_train.shape)\n",
    "print(df_fp_test.shape)\n",
    "print(df_emb_train.shape)\n",
    "print(df_emb_test.shape)\n",
    "print(df_atomic_train.shape)\n",
    "print(df_atomic_test.shape)\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print(df_desc_train)\n",
    "print(df_desc_test)\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print(df_fp_train)\n",
    "print(df_fp_test)\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print(df_emb_train)\n",
    "print(df_emb_test)\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print(df_atomic_train)\n",
    "print(df_atomic_test)\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c5e7b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_weak = [\n",
    "    lgb.LGBMRegressor(objective='regression', metric='rmse', boosting_type='gbdt', num_leaves=31, learning_rate=0.05, random_state=101),\n",
    "    RandomForestRegressor(n_jobs=-1, random_state=101),\n",
    "    GradientBoostingRegressor(random_state=101),\n",
    "    AdaBoostRegressor(random_state=101),\n",
    "    xgb.XGBRegressor(random_state=101, eval_metric='rmse'),\n",
    "    ExtraTreesRegressor(n_jobs=-1, n_estimators=100, random_state=101),\n",
    "    KNeighborsRegressor(),\n",
    "    SVR(),\n",
    "    MLPRegressor(random_state=101, max_iter=500),\n",
    "    DecisionTreeRegressor(random_state=101),\n",
    "\n",
    "]\n",
    "models_meta = [\n",
    "    lgb.LGBMRegressor(objective='regression', metric='rmse', boosting_type='gbdt', num_leaves=31, learning_rate=0.05, random_state=101),\n",
    "    DecisionTreeRegressor(random_state=101),\n",
    "    RandomForestRegressor(n_jobs=-1, random_state=101),\n",
    "    GradientBoostingRegressor(random_state=101),\n",
    "    AdaBoostRegressor(random_state=101),\n",
    "    xgb.XGBRegressor(random_state=101, eval_metric='rmse'),\n",
    "    ExtraTreesRegressor(n_jobs=-1, n_estimators=100, random_state=101),\n",
    "    LinearRegression(),\n",
    "    KNeighborsRegressor(),\n",
    "    SVR(),\n",
    "    MLPRegressor(random_state=101, max_iter=500)\n",
    "]\n",
    "\n",
    "dataframes = [(df_desc_train, df_desc_test), (df_fp_train, df_fp_test), (df_emb_train, df_emb_test), (df_atomic_train, df_atomic_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2bed1e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Ablation: Excluding feature at index 0 ==========\n",
      "========== Ablation: Excluding feature :-- Descriptor ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ablated dataframes:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Training weak models:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039811 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4196\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.747671\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040476 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4207\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.741826\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4200\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.740654\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040652 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4188\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1103\n",
      "[LightGBM] [Info] Start training from score -5.749020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040810 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4196\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.737478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041187 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4197\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.747212\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040892 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4200\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.750687\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041093 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4199\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.743084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4192\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 1104\n",
      "[LightGBM] [Info] Start training from score -5.737254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4189\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  10%|█         | 1/10 [00:11<01:41, 11.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0: LGBMRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  20%|██        | 2/10 [00:17<01:06,  8.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1: RandomForestRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  30%|███       | 3/10 [01:28<04:19, 37.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2: GradientBoostingRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  40%|████      | 4/10 [02:53<05:34, 55.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3: AdaBoostRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  50%|█████     | 5/10 [03:00<03:12, 38.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4: XGBRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  60%|██████    | 6/10 [03:06<01:49, 27.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5: ExtraTreesRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  70%|███████   | 7/10 [03:08<00:57, 19.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6: KNeighborsRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  80%|████████  | 8/10 [04:57<01:34, 47.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7: SVR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  90%|█████████ | 9/10 [05:51<00:49, 49.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8: MLPRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models: 100%|██████████| 10/10 [05:54<00:00, 35.49s/it]\u001b[A\n",
      "Processing ablated dataframes:  33%|███▎      | 1/3 [05:54<11:49, 354.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 9: DecisionTreeRegressor\n",
      "Dataframe training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018971 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.747671\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018651 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.741826\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018486 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.740654\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017927 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.749020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018645 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.737478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018619 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.747212\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018499 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.750687\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019043 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.743084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045566 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.737254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018544 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  10%|█         | 1/10 [00:08<01:13,  8.21s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0: LGBMRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  20%|██        | 2/10 [01:04<04:52, 36.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1: RandomForestRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  30%|███       | 3/10 [24:26<1:16:59, 659.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2: GradientBoostingRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  40%|████      | 4/10 [29:44<52:29, 524.98s/it]  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3: AdaBoostRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  50%|█████     | 5/10 [30:11<28:47, 345.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4: XGBRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  60%|██████    | 6/10 [30:24<15:30, 232.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5: ExtraTreesRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  70%|███████   | 7/10 [30:26<07:50, 156.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6: KNeighborsRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  80%|████████  | 8/10 [31:27<04:12, 126.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7: SVR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  90%|█████████ | 9/10 [32:13<01:41, 101.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8: MLPRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models: 100%|██████████| 10/10 [33:07<00:00, 198.79s/it][A\n",
      "Processing ablated dataframes:  67%|██████▋   | 2/3 [39:02<21:55, 1315.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 9: DecisionTreeRegressor\n",
      "Dataframe training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.747671\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.741826\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000733 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 151\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.740654\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 150\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.749020\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.737478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.747212\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000946 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.750687\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000694 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.743084\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000717 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.737254\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 151\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  10%|█         | 1/10 [00:05<00:47,  5.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0: LGBMRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  20%|██        | 2/10 [00:09<00:35,  4.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1: RandomForestRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  30%|███       | 3/10 [00:10<00:21,  3.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2: GradientBoostingRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  40%|████      | 4/10 [00:11<00:12,  2.08s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3: AdaBoostRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  50%|█████     | 5/10 [00:12<00:09,  1.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4: XGBRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  60%|██████    | 6/10 [00:15<00:09,  2.32s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5: ExtraTreesRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  70%|███████   | 7/10 [00:16<00:05,  1.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6: KNeighborsRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  80%|████████  | 8/10 [00:24<00:07,  3.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7: SVR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models: 100%|██████████| 10/10 [00:39<00:00,  3.99s/it][A\n",
      "Processing ablated dataframes: 100%|██████████| 3/3 [39:42<00:00, 794.23s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8: MLPRegressor\n",
      "Model training done 9: DecisionTreeRegressor\n",
      "Dataframe training completed\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Stage 1 completed (Weak Learners)\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002024 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7559\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.747671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001415 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7558\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.741826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7554\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.740654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001349 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7555\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.749020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001418 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7552\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.737478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001369 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7551\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.747212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001407 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7553\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.750687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001357 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7555\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.743084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001389 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7548\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.737254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7555\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Ablation: Excluding feature at index 1 ==========\n",
      "========== Ablation: Excluding feature :-- Fingerprints ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ablated dataframes:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Training weak models:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56912\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.747671\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007627 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56937\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.741826\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56933\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.740654\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007562 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56935\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.749020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008498 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56925\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.737478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007493 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56928\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.747212\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007517 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56933\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.750687\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007732 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56914\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.743084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007613 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56925\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.737254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007940 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56933\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  10%|█         | 1/10 [00:07<01:09,  7.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0: LGBMRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  20%|██        | 2/10 [00:22<01:36, 12.11s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1: RandomForestRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  30%|███       | 3/10 [04:32<14:02, 120.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2: GradientBoostingRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  40%|████      | 4/10 [05:47<10:16, 102.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3: AdaBoostRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  50%|█████     | 5/10 [05:55<05:43, 68.60s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4: XGBRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  60%|██████    | 6/10 [06:01<03:08, 47.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5: ExtraTreesRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  70%|███████   | 7/10 [06:02<01:36, 32.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6: KNeighborsRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  80%|████████  | 8/10 [06:32<01:02, 31.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7: SVR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  90%|█████████ | 9/10 [06:58<00:29, 29.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8: MLPRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models: 100%|██████████| 10/10 [07:09<00:00, 42.91s/it]\u001b[A\n",
      "Processing ablated dataframes:  33%|███▎      | 1/3 [07:09<14:18, 429.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 9: DecisionTreeRegressor\n",
      "Dataframe training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089580 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.747671\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017225 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.741826\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017579 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.740654\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017698 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.749020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.737478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.097414 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.747212\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017059 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.750687\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017445 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.743084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017520 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.737254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017280 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  10%|█         | 1/10 [00:08<01:14,  8.31s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0: LGBMRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  20%|██        | 2/10 [01:08<05:09, 38.63s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1: RandomForestRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  30%|███       | 3/10 [24:30<1:17:08, 661.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2: GradientBoostingRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  40%|████      | 4/10 [29:44<52:24, 524.13s/it]  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3: AdaBoostRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  50%|█████     | 5/10 [30:09<28:41, 344.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4: XGBRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  60%|██████    | 6/10 [30:23<15:27, 231.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5: ExtraTreesRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  70%|███████   | 7/10 [30:24<07:49, 156.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6: KNeighborsRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  80%|████████  | 8/10 [31:24<04:11, 125.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7: SVR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  90%|█████████ | 9/10 [32:08<01:40, 100.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8: MLPRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models: 100%|██████████| 10/10 [33:03<00:00, 198.31s/it][A\n",
      "Processing ablated dataframes:  67%|██████▋   | 2/3 [40:12<22:23, 1343.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 9: DecisionTreeRegressor\n",
      "Dataframe training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.747671\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.741826\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000717 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 151\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.740654\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000754 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 150\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.749020\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000737 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.737478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000961 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.747212\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.750687\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.743084\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.737254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000996 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 151\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  10%|█         | 1/10 [00:05<00:46,  5.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0: LGBMRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  20%|██        | 2/10 [00:09<00:35,  4.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1: RandomForestRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  30%|███       | 3/10 [00:10<00:21,  3.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2: GradientBoostingRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  40%|████      | 4/10 [00:11<00:12,  2.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3: AdaBoostRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  50%|█████     | 5/10 [00:12<00:09,  1.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4: XGBRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  60%|██████    | 6/10 [00:15<00:09,  2.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5: ExtraTreesRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  70%|███████   | 7/10 [00:16<00:05,  1.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6: KNeighborsRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  80%|████████  | 8/10 [00:24<00:07,  3.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7: SVR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models: 100%|██████████| 10/10 [00:39<00:00,  3.98s/it][A\n",
      "Processing ablated dataframes: 100%|██████████| 3/3 [40:51<00:00, 817.33s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8: MLPRegressor\n",
      "Model training done 9: DecisionTreeRegressor\n",
      "Dataframe training completed\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Stage 1 completed (Weak Learners)\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.747671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001368 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7552\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.741826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001382 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7547\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.740654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001186 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7548\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.749020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7544\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.737478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001382 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7544\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.747212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001421 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7546\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.750687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001373 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7549\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.743084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001310 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7543\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.737254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001348 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7548\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Ablation: Excluding feature at index 2 ==========\n",
      "========== Ablation: Excluding feature :-- Embeddings ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ablated dataframes:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Training weak models:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56912\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.747671\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56937\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.741826\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008350 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56933\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.740654\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56935\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.749020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007416 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56925\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.737478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56928\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.747212\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007362 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56933\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.750687\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007982 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56914\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.743084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007433 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56925\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.737254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008239 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56933\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  10%|█         | 1/10 [00:08<01:14,  8.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0: LGBMRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  20%|██        | 2/10 [00:22<01:34, 11.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1: RandomForestRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  30%|███       | 3/10 [04:31<13:59, 119.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2: GradientBoostingRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  40%|████      | 4/10 [05:46<10:12, 102.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3: AdaBoostRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  50%|█████     | 5/10 [05:53<05:39, 67.93s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4: XGBRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  60%|██████    | 6/10 [05:58<03:06, 46.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5: ExtraTreesRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  70%|███████   | 7/10 [05:59<01:35, 31.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6: KNeighborsRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  80%|████████  | 8/10 [06:30<01:02, 31.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7: SVR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  90%|█████████ | 9/10 [06:54<00:29, 29.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8: MLPRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models: 100%|██████████| 10/10 [07:05<00:00, 42.53s/it]\u001b[A\n",
      "Processing ablated dataframes:  33%|███▎      | 1/3 [07:05<14:10, 425.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 9: DecisionTreeRegressor\n",
      "Dataframe training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4196\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.747671\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041559 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4207\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.741826\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041879 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4200\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.740654\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4188\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1103\n",
      "[LightGBM] [Info] Start training from score -5.749020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041871 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4196\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.737478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041982 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4197\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.747212\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041869 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4200\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.750687\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4199\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.743084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042631 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4192\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 1104\n",
      "[LightGBM] [Info] Start training from score -5.737254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042635 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4189\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  10%|█         | 1/10 [00:11<01:40, 11.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0: LGBMRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  20%|██        | 2/10 [00:17<01:06,  8.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1: RandomForestRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  30%|███       | 3/10 [01:27<04:16, 36.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2: GradientBoostingRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  40%|████      | 4/10 [02:26<04:32, 45.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3: AdaBoostRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  50%|█████     | 5/10 [02:34<02:39, 31.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4: XGBRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  60%|██████    | 6/10 [02:40<01:32, 23.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5: ExtraTreesRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  70%|███████   | 7/10 [02:42<00:48, 16.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6: KNeighborsRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  80%|████████  | 8/10 [04:27<01:29, 44.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7: SVR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  90%|█████████ | 9/10 [05:21<00:47, 47.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8: MLPRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models: 100%|██████████| 10/10 [05:25<00:00, 32.50s/it]\u001b[A\n",
      "Processing ablated dataframes:  67%|██████▋   | 2/3 [12:30<06:06, 366.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 9: DecisionTreeRegressor\n",
      "Dataframe training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.747671\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.741826\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000733 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 151\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.740654\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000729 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 150\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.749020\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.737478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000719 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.747212\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000762 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.750687\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.743084\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 153\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.737254\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000764 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 151\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  10%|█         | 1/10 [00:05<00:47,  5.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0: LGBMRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  20%|██        | 2/10 [00:09<00:36,  4.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1: RandomForestRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  30%|███       | 3/10 [00:10<00:21,  3.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2: GradientBoostingRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  40%|████      | 4/10 [00:11<00:12,  2.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3: AdaBoostRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  50%|█████     | 5/10 [00:12<00:09,  1.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4: XGBRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  60%|██████    | 6/10 [00:15<00:09,  2.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5: ExtraTreesRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  70%|███████   | 7/10 [00:16<00:04,  1.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6: KNeighborsRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  80%|████████  | 8/10 [00:24<00:07,  3.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7: SVR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models: 100%|██████████| 10/10 [00:39<00:00,  3.97s/it][A\n",
      "Processing ablated dataframes: 100%|██████████| 3/3 [13:10<00:00, 263.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8: MLPRegressor\n",
      "Model training done 9: DecisionTreeRegressor\n",
      "Dataframe training completed\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Stage 1 completed (Weak Learners)\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064722 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.747671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001266 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7560\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.741826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001192 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7553\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.740654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001154 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7557\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.749020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001196 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7552\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.737478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001295 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7552\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.747212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001320 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7553\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.750687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001245 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7555\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.743084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7553\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.737254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001327 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7557\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Ablation: Excluding feature at index 3 ==========\n",
      "========== Ablation: Excluding feature :-- Atomic ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ablated dataframes:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Training weak models:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008297 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56912\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.747671\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007953 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56937\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.741826\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008067 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56933\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.740654\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007914 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56935\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.749020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007593 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56925\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.737478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008201 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56928\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.747212\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007651 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56933\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.750687\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007411 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56914\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 262\n",
      "[LightGBM] [Info] Start training from score -5.743084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008408 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56925\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.737254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008498 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56933\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 263\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  10%|█         | 1/10 [00:07<01:10,  7.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0: LGBMRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  20%|██        | 2/10 [00:22<01:35, 11.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1: RandomForestRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  30%|███       | 3/10 [04:31<13:59, 119.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2: GradientBoostingRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  40%|████      | 4/10 [05:45<10:12, 102.08s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3: AdaBoostRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  50%|█████     | 5/10 [05:53<05:40, 68.00s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4: XGBRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  60%|██████    | 6/10 [05:59<03:07, 46.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5: ExtraTreesRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  70%|███████   | 7/10 [06:00<01:35, 31.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6: KNeighborsRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  80%|████████  | 8/10 [06:30<01:02, 31.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7: SVR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  90%|█████████ | 9/10 [06:54<00:29, 29.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8: MLPRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models: 100%|██████████| 10/10 [07:05<00:00, 42.58s/it]\u001b[A\n",
      "Processing ablated dataframes:  33%|███▎      | 1/3 [07:05<14:11, 425.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 9: DecisionTreeRegressor\n",
      "Dataframe training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042201 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4196\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.747671\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041756 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4207\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.741826\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041741 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4200\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.740654\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041779 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4188\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1103\n",
      "[LightGBM] [Info] Start training from score -5.749020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061481 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4196\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.737478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041959 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4197\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1106\n",
      "[LightGBM] [Info] Start training from score -5.747212\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061762 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4200\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.750687\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042924 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4199\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.743084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041927 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4192\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 1104\n",
      "[LightGBM] [Info] Start training from score -5.737254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042263 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4189\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 1105\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  10%|█         | 1/10 [00:10<01:36, 10.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0: LGBMRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  20%|██        | 2/10 [00:17<01:04,  8.11s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1: RandomForestRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  30%|███       | 3/10 [01:27<04:15, 36.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2: GradientBoostingRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  40%|████      | 4/10 [02:25<04:31, 45.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3: AdaBoostRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  50%|█████     | 5/10 [02:33<02:38, 31.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4: XGBRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  60%|██████    | 6/10 [02:39<01:31, 22.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5: ExtraTreesRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  70%|███████   | 7/10 [02:41<00:47, 15.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6: KNeighborsRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  80%|████████  | 8/10 [04:20<01:25, 42.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7: SVR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  90%|█████████ | 9/10 [05:16<00:46, 46.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8: MLPRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models: 100%|██████████| 10/10 [05:19<00:00, 31.94s/it]\u001b[A\n",
      "Processing ablated dataframes:  67%|██████▋   | 2/3 [12:25<06:03, 363.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 9: DecisionTreeRegressor\n",
      "Dataframe training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.747671\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018087 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.741826\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018714 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.740654\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018215 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.749020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017664 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.737478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018108 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.747212\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090737 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.750687\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017928 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.743084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017957 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.737254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 192525\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 755\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  10%|█         | 1/10 [00:08<01:13,  8.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 0: LGBMRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  20%|██        | 2/10 [01:07<05:05, 38.22s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 1: RandomForestRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  30%|███       | 3/10 [24:27<1:17:01, 660.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 2: GradientBoostingRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  40%|████      | 4/10 [29:40<52:18, 523.06s/it]  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 3: AdaBoostRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  50%|█████     | 5/10 [30:05<28:37, 343.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 4: XGBRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  60%|██████    | 6/10 [30:18<15:24, 231.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 5: ExtraTreesRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  70%|███████   | 7/10 [30:20<07:48, 156.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 6: KNeighborsRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  80%|████████  | 8/10 [31:22<04:12, 126.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 7: SVR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models:  90%|█████████ | 9/10 [32:10<01:41, 101.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 8: MLPRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training weak models: 100%|██████████| 10/10 [33:05<00:00, 198.50s/it][A\n",
      "Processing ablated dataframes: 100%|██████████| 3/3 [45:30<00:00, 910.07s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done 9: DecisionTreeRegressor\n",
      "Dataframe training completed\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Stage 1 completed (Weak Learners)\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074774 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7633\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.747671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7633\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.741826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7631\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.740654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027438 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7631\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.749020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7631\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.737478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001289 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7629\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.747212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7630\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.750687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7632\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.743084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001247 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7629\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.737254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001309 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7631\n",
      "[LightGBM] [Info] Number of data points in the train set: 5012, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -5.734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/users/akshay/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Ablation Study Completed\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    }
   ],
   "source": [
    "ablation_results = {}\n",
    "\n",
    "for ablation_idx in range(len(dataframes)):\n",
    "    print(f\"========== Ablation: Excluding feature at index {ablation_idx} ==========\")\n",
    "    feature_names = ['Descriptor', 'Fingerprints', 'Embeddings', 'Atomic']\n",
    "    print(f\"========== Ablation: Excluding feature :-- {feature_names[ablation_idx]} ==========\")\n",
    "\n",
    "    ablated_dataframes = [pair for i, pair in enumerate(dataframes) if i != ablation_idx]\n",
    "\n",
    "    meta_features_train = []\n",
    "    meta_features_test = []\n",
    "\n",
    "    # Stage 1\n",
    "    for df_train, df_test in tqdm(ablated_dataframes, desc=\"Processing ablated dataframes\"):\n",
    "        X_weak = df_train.drop(columns=['ID', 'SMILES', target_column])\n",
    "        y_weak = df_train[target_column]\n",
    "        X_eval = df_test.drop(columns=['ID', 'SMILES', target_column])\n",
    "        y_eval = df_test[target_column]\n",
    "\n",
    "        kf = KFold(n_splits=10, shuffle=True, random_state=101)\n",
    "\n",
    "        fold_meta_features_train = np.zeros((X_weak.shape[0], len(models_weak)))\n",
    "        fold_meta_features_test = np.zeros((X_eval.shape[0], len(models_weak)))\n",
    "\n",
    "        for i, model in tqdm(enumerate(models_weak), desc=\"Training weak models\", total=len(models_weak)):\n",
    "            fold_predictions = np.zeros(X_weak.shape[0])\n",
    "            test_predictions_folds = []\n",
    "\n",
    "            for train_index, val_index in kf.split(X_weak):\n",
    "                X_train, X_val = X_weak.iloc[train_index], X_weak.iloc[val_index]\n",
    "                y_train, y_val = y_weak.iloc[train_index], y_weak.iloc[val_index]\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                fold_predictions[val_index] = np.clip(model.predict(X_val), -10, -3.9)\n",
    "                test_predictions_fold = np.clip(model.predict(X_eval), -10, -3.9)\n",
    "                test_predictions_folds.append(test_predictions_fold)\n",
    "\n",
    "            fold_meta_features_train[:, i] = fold_predictions\n",
    "            fold_meta_features_test[:, i] = np.mean(test_predictions_folds, axis=0)\n",
    "            print(f'Model training done {i}: {model.__class__.__name__}')\n",
    "\n",
    "        meta_features_train.append(fold_meta_features_train)\n",
    "        meta_features_test.append(fold_meta_features_test)\n",
    "        print('Dataframe training completed')\n",
    "\n",
    "    # Stack all meta-features\n",
    "    meta_features_train = np.hstack(meta_features_train)\n",
    "    meta_features_test = np.hstack(meta_features_test)\n",
    "\n",
    "    print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "    print('Stage 1 completed (Weak Learners)')\n",
    "    print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "\n",
    "    # Stage 2\n",
    "    results = {}\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=101)\n",
    "\n",
    "    for model in models_meta:\n",
    "        model_name = model.__class__.__name__\n",
    "        predictions_train = []\n",
    "        actual_y_train = []\n",
    "        test_predictions_folds = []\n",
    "\n",
    "        for train_index, val_index in kf.split(meta_features_train):\n",
    "            X_fold_train, X_fold_val = meta_features_train[train_index], meta_features_train[val_index]\n",
    "            y_fold_train, y_fold_val = y_weak.iloc[train_index], y_weak.iloc[val_index]\n",
    "\n",
    "            model.fit(X_fold_train, y_fold_train)\n",
    "            y_pred_fold = np.clip(model.predict(X_fold_val), -10, -3.9)\n",
    "\n",
    "            predictions_train.extend(y_pred_fold)\n",
    "            actual_y_train.extend(y_fold_val)\n",
    "\n",
    "            test_predictions_fold = model.predict(meta_features_test)\n",
    "            test_predictions_fold = np.clip(test_predictions_fold, -10, -3.9)\n",
    "            test_predictions_folds.append(test_predictions_fold)\n",
    "\n",
    "        predictions_test_mean = np.mean(test_predictions_folds, axis=0)\n",
    "        predictions_test_std = np.std(test_predictions_folds, axis=0)\n",
    "\n",
    "        mse_train = mean_squared_error(actual_y_train, predictions_train)\n",
    "        mae_train = mean_absolute_error(actual_y_train, predictions_train)\n",
    "        rmse_train = np.sqrt(mse_train)\n",
    "        r2_train = r2_score(actual_y_train, predictions_train)\n",
    "        pearson_train, _ = pearsonr(actual_y_train, predictions_train)\n",
    "        spearman_train, _ = spearmanr(actual_y_train, predictions_train)\n",
    "\n",
    "        mse_test = mean_squared_error(y_eval, predictions_test_mean)\n",
    "        mae_test = mean_absolute_error(y_eval, predictions_test_mean)\n",
    "        rmse_test = np.sqrt(mse_test)\n",
    "        r2_test = r2_score(y_eval, predictions_test_mean)\n",
    "        pearson_test, _ = pearsonr(y_eval, predictions_test_mean)\n",
    "        spearman_test, _ = spearmanr(y_eval, predictions_test_mean)\n",
    "\n",
    "        results[model_name] = {\n",
    "            'Train MSE (10 fold CV)': mse_train,\n",
    "            'Train MAE (10 fold CV)': mae_train,\n",
    "            'Train RMSE (10 fold CV)': rmse_train,\n",
    "            'Train R2 (10 fold CV)': r2_train,\n",
    "            'Train PCC (10 fold CV)': pearson_train,\n",
    "            'Train SCC (10 fold CV)': spearman_train,\n",
    "            'Test MSE': mse_test,\n",
    "            'Test MAE': mae_test,\n",
    "            'Test RMSE': rmse_test,\n",
    "            'Test R2': r2_test,\n",
    "            'Test PCC': pearson_test,\n",
    "            'Test SCC': spearman_test,\n",
    "        }\n",
    "\n",
    "    ablation_results[f\"Ablation_{feature_names[ablation_idx]}\"] = pd.DataFrame(results).T\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print('Ablation Study Completed')\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "\n",
    "# To view the results\n",
    "ablation_results_df = {key: value for key, value in ablation_results.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c89499d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ablation_Descriptor':                            Train MSE (10 fold CV)  Train MAE (10 fold CV)  \\\n",
       " LGBMRegressor                            0.124547                0.257339   \n",
       " DecisionTreeRegressor                    0.271159                0.378883   \n",
       " RandomForestRegressor                    0.127370                0.258899   \n",
       " GradientBoostingRegressor                0.124091                0.256223   \n",
       " AdaBoostRegressor                        0.173011                0.317938   \n",
       " XGBRegressor                             0.141366                0.274843   \n",
       " ExtraTreesRegressor                      0.127268                0.258200   \n",
       " LinearRegression                         0.124801                0.255590   \n",
       " KNeighborsRegressor                      0.151503                0.284705   \n",
       " SVR                                      0.134686                0.262531   \n",
       " MLPRegressor                             0.135549                0.271661   \n",
       " \n",
       "                            Train RMSE (10 fold CV)  Train R2 (10 fold CV)  \\\n",
       " LGBMRegressor                             0.352913               0.800023   \n",
       " DecisionTreeRegressor                     0.520729               0.564619   \n",
       " RandomForestRegressor                     0.356889               0.795491   \n",
       " GradientBoostingRegressor                 0.352266               0.800755   \n",
       " AdaBoostRegressor                         0.415946               0.722208   \n",
       " XGBRegressor                              0.375987               0.773019   \n",
       " ExtraTreesRegressor                       0.356747               0.795654   \n",
       " LinearRegression                          0.353271               0.799617   \n",
       " KNeighborsRegressor                       0.389234               0.756742   \n",
       " SVR                                       0.366995               0.783745   \n",
       " MLPRegressor                              0.368170               0.782359   \n",
       " \n",
       "                            Train PCC (10 fold CV)  Train SCC (10 fold CV)  \\\n",
       " LGBMRegressor                            0.894462                0.877640   \n",
       " DecisionTreeRegressor                    0.784426                0.759031   \n",
       " RandomForestRegressor                    0.891936                0.875326   \n",
       " GradientBoostingRegressor                0.894851                0.876724   \n",
       " AdaBoostRegressor                        0.859052                0.850397   \n",
       " XGBRegressor                             0.880031                0.862635   \n",
       " ExtraTreesRegressor                      0.892023                0.874573   \n",
       " LinearRegression                         0.894215                0.877585   \n",
       " KNeighborsRegressor                      0.869998                0.849301   \n",
       " SVR                                      0.885559                0.874330   \n",
       " MLPRegressor                             0.885030                0.866893   \n",
       " \n",
       "                            Test MSE  Test MAE  Test RMSE   Test R2  Test PCC  \\\n",
       " LGBMRegressor              0.193359  0.309692   0.439726  0.694453  0.833456   \n",
       " DecisionTreeRegressor      0.216569  0.327040   0.465369  0.657777  0.812709   \n",
       " RandomForestRegressor      0.196710  0.311360   0.443520  0.689158  0.830406   \n",
       " GradientBoostingRegressor  0.194348  0.310251   0.440849  0.692890  0.832579   \n",
       " AdaBoostRegressor          0.220874  0.352171   0.469972  0.650974  0.814813   \n",
       " XGBRegressor               0.197695  0.313840   0.444629  0.687601  0.829688   \n",
       " ExtraTreesRegressor        0.196641  0.311836   0.443442  0.689267  0.830498   \n",
       " LinearRegression           0.195866  0.310431   0.442568  0.690491  0.831149   \n",
       " KNeighborsRegressor        0.199434  0.320926   0.446581  0.684853  0.827842   \n",
       " SVR                        0.195332  0.307070   0.441963  0.691336  0.832594   \n",
       " MLPRegressor               0.194889  0.313915   0.441462  0.692035  0.832066   \n",
       " \n",
       "                            Test SCC  \n",
       " LGBMRegressor              0.820650  \n",
       " DecisionTreeRegressor      0.799445  \n",
       " RandomForestRegressor      0.816779  \n",
       " GradientBoostingRegressor  0.819734  \n",
       " AdaBoostRegressor          0.809462  \n",
       " XGBRegressor               0.816433  \n",
       " ExtraTreesRegressor        0.816118  \n",
       " LinearRegression           0.816576  \n",
       " KNeighborsRegressor        0.805840  \n",
       " SVR                        0.818707  \n",
       " MLPRegressor               0.817271  ,\n",
       " 'Ablation_Fingerprints':                            Train MSE (10 fold CV)  Train MAE (10 fold CV)  \\\n",
       " LGBMRegressor                            0.125781                0.257481   \n",
       " DecisionTreeRegressor                    0.252359                0.368848   \n",
       " RandomForestRegressor                    0.127854                0.259588   \n",
       " GradientBoostingRegressor                0.125087                0.257019   \n",
       " AdaBoostRegressor                        0.173866                0.319790   \n",
       " XGBRegressor                             0.141224                0.276066   \n",
       " ExtraTreesRegressor                      0.127932                0.259208   \n",
       " LinearRegression                         0.125036                0.255569   \n",
       " KNeighborsRegressor                      0.149706                0.283514   \n",
       " SVR                                      0.132610                0.259854   \n",
       " MLPRegressor                             0.138850                0.276482   \n",
       " \n",
       "                            Train RMSE (10 fold CV)  Train R2 (10 fold CV)  \\\n",
       " LGBMRegressor                             0.354656               0.798042   \n",
       " DecisionTreeRegressor                     0.502353               0.594805   \n",
       " RandomForestRegressor                     0.357567               0.794714   \n",
       " GradientBoostingRegressor                 0.353676               0.799157   \n",
       " AdaBoostRegressor                         0.416972               0.720836   \n",
       " XGBRegressor                              0.375798               0.773246   \n",
       " ExtraTreesRegressor                       0.357676               0.794588   \n",
       " LinearRegression                          0.353604               0.799239   \n",
       " KNeighborsRegressor                       0.386919               0.759627   \n",
       " SVR                                       0.364156               0.787078   \n",
       " MLPRegressor                              0.372625               0.777059   \n",
       " \n",
       "                            Train PCC (10 fold CV)  Train SCC (10 fold CV)  \\\n",
       " LGBMRegressor                            0.893349                0.876897   \n",
       " DecisionTreeRegressor                    0.799321                0.775558   \n",
       " RandomForestRegressor                    0.891494                0.874756   \n",
       " GradientBoostingRegressor                0.893964                0.876321   \n",
       " AdaBoostRegressor                        0.859506                0.851227   \n",
       " XGBRegressor                             0.879863                0.859806   \n",
       " ExtraTreesRegressor                      0.891426                0.874096   \n",
       " LinearRegression                         0.894004                0.877131   \n",
       " KNeighborsRegressor                      0.871711                0.852315   \n",
       " SVR                                      0.887462                0.876649   \n",
       " MLPRegressor                             0.882955                0.864167   \n",
       " \n",
       "                            Test MSE  Test MAE  Test RMSE   Test R2  Test PCC  \\\n",
       " LGBMRegressor              0.191068  0.306364   0.437114  0.698072  0.835614   \n",
       " DecisionTreeRegressor      0.208692  0.322135   0.456828  0.670224  0.820248   \n",
       " RandomForestRegressor      0.194917  0.309333   0.441494  0.691991  0.832140   \n",
       " GradientBoostingRegressor  0.192349  0.307660   0.438576  0.696049  0.834473   \n",
       " AdaBoostRegressor          0.214561  0.347395   0.463208  0.660949  0.821440   \n",
       " XGBRegressor               0.195996  0.309806   0.442714  0.690286  0.831231   \n",
       " ExtraTreesRegressor        0.193109  0.308256   0.439442  0.694847  0.833804   \n",
       " LinearRegression           0.192681  0.308454   0.438955  0.695524  0.834174   \n",
       " KNeighborsRegressor        0.207293  0.323132   0.455295  0.672434  0.821086   \n",
       " SVR                        0.194822  0.306770   0.441386  0.692141  0.833169   \n",
       " MLPRegressor               0.192956  0.315153   0.439268  0.695089  0.834604   \n",
       " \n",
       "                            Test SCC  \n",
       " LGBMRegressor              0.820365  \n",
       " DecisionTreeRegressor      0.799778  \n",
       " RandomForestRegressor      0.815763  \n",
       " GradientBoostingRegressor  0.820331  \n",
       " AdaBoostRegressor          0.812258  \n",
       " XGBRegressor               0.815661  \n",
       " ExtraTreesRegressor        0.817990  \n",
       " LinearRegression           0.819747  \n",
       " KNeighborsRegressor        0.803682  \n",
       " SVR                        0.818860  \n",
       " MLPRegressor               0.820176  ,\n",
       " 'Ablation_Embeddings':                            Train MSE (10 fold CV)  Train MAE (10 fold CV)  \\\n",
       " LGBMRegressor                            0.197204                0.325799   \n",
       " DecisionTreeRegressor                    0.400688                0.458130   \n",
       " RandomForestRegressor                    0.193206                0.322189   \n",
       " GradientBoostingRegressor                0.198992                0.327857   \n",
       " AdaBoostRegressor                        0.259560                0.402047   \n",
       " XGBRegressor                             0.219899                0.344418   \n",
       " ExtraTreesRegressor                      0.186845                0.315897   \n",
       " LinearRegression                         0.196513                0.325000   \n",
       " KNeighborsRegressor                      0.222483                0.343583   \n",
       " SVR                                      0.208088                0.328868   \n",
       " MLPRegressor                             0.208874                0.340886   \n",
       " \n",
       "                            Train RMSE (10 fold CV)  Train R2 (10 fold CV)  \\\n",
       " LGBMRegressor                             0.444077               0.683363   \n",
       " DecisionTreeRegressor                     0.632999               0.356643   \n",
       " RandomForestRegressor                     0.439552               0.689782   \n",
       " GradientBoostingRegressor                 0.446085               0.680493   \n",
       " AdaBoostRegressor                         0.509470               0.583243   \n",
       " XGBRegressor                              0.468934               0.646924   \n",
       " ExtraTreesRegressor                       0.432256               0.699996   \n",
       " LinearRegression                          0.443298               0.684472   \n",
       " KNeighborsRegressor                       0.471681               0.642775   \n",
       " SVR                                       0.456167               0.665888   \n",
       " MLPRegressor                              0.457027               0.664626   \n",
       " \n",
       "                            Train PCC (10 fold CV)  Train SCC (10 fold CV)  \\\n",
       " LGBMRegressor                            0.826659                0.798828   \n",
       " DecisionTreeRegressor                    0.680875                0.656305   \n",
       " RandomForestRegressor                    0.830666                0.804122   \n",
       " GradientBoostingRegressor                0.824943                0.795793   \n",
       " AdaBoostRegressor                        0.789216                0.761066   \n",
       " XGBRegressor                             0.806869                0.778609   \n",
       " ExtraTreesRegressor                      0.836786                0.811588   \n",
       " LinearRegression                         0.827342                0.797443   \n",
       " KNeighborsRegressor                      0.802678                0.773852   \n",
       " SVR                                      0.817145                0.792233   \n",
       " MLPRegressor                             0.816629                0.784871   \n",
       " \n",
       "                            Test MSE  Test MAE  Test RMSE   Test R2  Test PCC  \\\n",
       " LGBMRegressor              0.207748  0.326902   0.455794  0.671715  0.819593   \n",
       " DecisionTreeRegressor      0.249740  0.355437   0.499740  0.605359  0.780632   \n",
       " RandomForestRegressor      0.208753  0.326254   0.456895  0.670127  0.818713   \n",
       " GradientBoostingRegressor  0.203710  0.324691   0.451342  0.678096  0.823480   \n",
       " AdaBoostRegressor          0.247968  0.388195   0.497964  0.608159  0.804846   \n",
       " XGBRegressor               0.219158  0.334292   0.468143  0.653685  0.809387   \n",
       " ExtraTreesRegressor        0.211241  0.328015   0.459610  0.666195  0.816328   \n",
       " LinearRegression           0.198451  0.318054   0.445478  0.686406  0.828584   \n",
       " KNeighborsRegressor        0.235183  0.350971   0.484956  0.628363  0.794114   \n",
       " SVR                        0.206904  0.320719   0.454867  0.673048  0.821811   \n",
       " MLPRegressor               0.202372  0.326251   0.449858  0.680210  0.825356   \n",
       " \n",
       "                            Test SCC  \n",
       " LGBMRegressor              0.806872  \n",
       " DecisionTreeRegressor      0.762524  \n",
       " RandomForestRegressor      0.803257  \n",
       " GradientBoostingRegressor  0.808707  \n",
       " AdaBoostRegressor          0.791111  \n",
       " XGBRegressor               0.795459  \n",
       " ExtraTreesRegressor        0.802155  \n",
       " LinearRegression           0.814382  \n",
       " KNeighborsRegressor        0.776606  \n",
       " SVR                        0.811838  \n",
       " MLPRegressor               0.808979  ,\n",
       " 'Ablation_Atomic':                            Train MSE (10 fold CV)  Train MAE (10 fold CV)  \\\n",
       " LGBMRegressor                            0.125275                0.257694   \n",
       " DecisionTreeRegressor                    0.263743                0.374735   \n",
       " RandomForestRegressor                    0.127749                0.259039   \n",
       " GradientBoostingRegressor                0.125566                0.257346   \n",
       " AdaBoostRegressor                        0.178632                0.325035   \n",
       " XGBRegressor                             0.141496                0.275541   \n",
       " ExtraTreesRegressor                      0.126231                0.257413   \n",
       " LinearRegression                         0.124419                0.255014   \n",
       " KNeighborsRegressor                      0.151292                0.285708   \n",
       " SVR                                      0.131419                0.259541   \n",
       " MLPRegressor                             0.134841                0.268003   \n",
       " \n",
       "                            Train RMSE (10 fold CV)  Train R2 (10 fold CV)  \\\n",
       " LGBMRegressor                             0.353942               0.798855   \n",
       " DecisionTreeRegressor                     0.513559               0.576526   \n",
       " RandomForestRegressor                     0.357421               0.794882   \n",
       " GradientBoostingRegressor                 0.354353               0.798388   \n",
       " AdaBoostRegressor                         0.422649               0.713183   \n",
       " XGBRegressor                              0.376159               0.772810   \n",
       " ExtraTreesRegressor                       0.355290               0.797320   \n",
       " LinearRegression                          0.352731               0.800229   \n",
       " KNeighborsRegressor                       0.388962               0.757082   \n",
       " SVR                                       0.362517               0.788990   \n",
       " MLPRegressor                              0.367207               0.783495   \n",
       " \n",
       "                            Train PCC (10 fold CV)  Train SCC (10 fold CV)  \\\n",
       " LGBMRegressor                            0.893805                0.877471   \n",
       " DecisionTreeRegressor                    0.792243                0.768514   \n",
       " RandomForestRegressor                    0.891594                0.875051   \n",
       " GradientBoostingRegressor                0.893527                0.876989   \n",
       " AdaBoostRegressor                        0.857310                0.848489   \n",
       " XGBRegressor                             0.879515                0.862526   \n",
       " ExtraTreesRegressor                      0.892954                0.875876   \n",
       " LinearRegression                         0.894557                0.877798   \n",
       " KNeighborsRegressor                      0.870208                0.848718   \n",
       " SVR                                      0.888530                0.877179   \n",
       " MLPRegressor                             0.885973                0.865999   \n",
       " \n",
       "                            Test MSE  Test MAE  Test RMSE   Test R2  Test PCC  \\\n",
       " LGBMRegressor              0.193031  0.308192   0.439353  0.694971  0.833787   \n",
       " DecisionTreeRegressor      0.212271  0.322619   0.460729  0.664568  0.816842   \n",
       " RandomForestRegressor      0.195506  0.309744   0.442161  0.691060  0.831602   \n",
       " GradientBoostingRegressor  0.191852  0.308384   0.438009  0.696834  0.834908   \n",
       " AdaBoostRegressor          0.220008  0.355082   0.469050  0.652342  0.817703   \n",
       " XGBRegressor               0.197481  0.312857   0.444389  0.687939  0.829882   \n",
       " ExtraTreesRegressor        0.194766  0.309874   0.441323  0.692229  0.832291   \n",
       " LinearRegression           0.192792  0.307843   0.439081  0.695348  0.834029   \n",
       " KNeighborsRegressor        0.201650  0.322872   0.449054  0.681351  0.825972   \n",
       " SVR                        0.192793  0.305884   0.439083  0.695346  0.834871   \n",
       " MLPRegressor               0.196292  0.312627   0.443049  0.689818  0.831816   \n",
       " \n",
       "                            Test SCC  \n",
       " LGBMRegressor              0.820005  \n",
       " DecisionTreeRegressor      0.803776  \n",
       " RandomForestRegressor      0.816087  \n",
       " GradientBoostingRegressor  0.819767  \n",
       " AdaBoostRegressor          0.807913  \n",
       " XGBRegressor               0.815774  \n",
       " ExtraTreesRegressor        0.815771  \n",
       " LinearRegression           0.817838  \n",
       " KNeighborsRegressor        0.803488  \n",
       " SVR                        0.818940  \n",
       " MLPRegressor               0.817597  }"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ablation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c115d502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ablation_Descriptor':                            Train MSE (10 fold CV)  Train MAE (10 fold CV)  \\\n",
       " LGBMRegressor                            0.124547                0.257339   \n",
       " DecisionTreeRegressor                    0.271159                0.378883   \n",
       " RandomForestRegressor                    0.127370                0.258899   \n",
       " GradientBoostingRegressor                0.124091                0.256223   \n",
       " AdaBoostRegressor                        0.173011                0.317938   \n",
       " XGBRegressor                             0.141366                0.274843   \n",
       " ExtraTreesRegressor                      0.127268                0.258200   \n",
       " LinearRegression                         0.124801                0.255590   \n",
       " KNeighborsRegressor                      0.151503                0.284705   \n",
       " SVR                                      0.134686                0.262531   \n",
       " MLPRegressor                             0.135549                0.271661   \n",
       " \n",
       "                            Train RMSE (10 fold CV)  Train R2 (10 fold CV)  \\\n",
       " LGBMRegressor                             0.352913               0.800023   \n",
       " DecisionTreeRegressor                     0.520729               0.564619   \n",
       " RandomForestRegressor                     0.356889               0.795491   \n",
       " GradientBoostingRegressor                 0.352266               0.800755   \n",
       " AdaBoostRegressor                         0.415946               0.722208   \n",
       " XGBRegressor                              0.375987               0.773019   \n",
       " ExtraTreesRegressor                       0.356747               0.795654   \n",
       " LinearRegression                          0.353271               0.799617   \n",
       " KNeighborsRegressor                       0.389234               0.756742   \n",
       " SVR                                       0.366995               0.783745   \n",
       " MLPRegressor                              0.368170               0.782359   \n",
       " \n",
       "                            Train PCC (10 fold CV)  Train SCC (10 fold CV)  \\\n",
       " LGBMRegressor                            0.894462                0.877640   \n",
       " DecisionTreeRegressor                    0.784426                0.759031   \n",
       " RandomForestRegressor                    0.891936                0.875326   \n",
       " GradientBoostingRegressor                0.894851                0.876724   \n",
       " AdaBoostRegressor                        0.859052                0.850397   \n",
       " XGBRegressor                             0.880031                0.862635   \n",
       " ExtraTreesRegressor                      0.892023                0.874573   \n",
       " LinearRegression                         0.894215                0.877585   \n",
       " KNeighborsRegressor                      0.869998                0.849301   \n",
       " SVR                                      0.885559                0.874330   \n",
       " MLPRegressor                             0.885030                0.866893   \n",
       " \n",
       "                            Test MSE  Test MAE  Test RMSE   Test R2  Test PCC  \\\n",
       " LGBMRegressor              0.193359  0.309692   0.439726  0.694453  0.833456   \n",
       " DecisionTreeRegressor      0.216569  0.327040   0.465369  0.657777  0.812709   \n",
       " RandomForestRegressor      0.196710  0.311360   0.443520  0.689158  0.830406   \n",
       " GradientBoostingRegressor  0.194348  0.310251   0.440849  0.692890  0.832579   \n",
       " AdaBoostRegressor          0.220874  0.352171   0.469972  0.650974  0.814813   \n",
       " XGBRegressor               0.197695  0.313840   0.444629  0.687601  0.829688   \n",
       " ExtraTreesRegressor        0.196641  0.311836   0.443442  0.689267  0.830498   \n",
       " LinearRegression           0.195866  0.310431   0.442568  0.690491  0.831149   \n",
       " KNeighborsRegressor        0.199434  0.320926   0.446581  0.684853  0.827842   \n",
       " SVR                        0.195332  0.307070   0.441963  0.691336  0.832594   \n",
       " MLPRegressor               0.194889  0.313915   0.441462  0.692035  0.832066   \n",
       " \n",
       "                            Test SCC  \n",
       " LGBMRegressor              0.820650  \n",
       " DecisionTreeRegressor      0.799445  \n",
       " RandomForestRegressor      0.816779  \n",
       " GradientBoostingRegressor  0.819734  \n",
       " AdaBoostRegressor          0.809462  \n",
       " XGBRegressor               0.816433  \n",
       " ExtraTreesRegressor        0.816118  \n",
       " LinearRegression           0.816576  \n",
       " KNeighborsRegressor        0.805840  \n",
       " SVR                        0.818707  \n",
       " MLPRegressor               0.817271  ,\n",
       " 'Ablation_Fingerprints':                            Train MSE (10 fold CV)  Train MAE (10 fold CV)  \\\n",
       " LGBMRegressor                            0.125781                0.257481   \n",
       " DecisionTreeRegressor                    0.252359                0.368848   \n",
       " RandomForestRegressor                    0.127854                0.259588   \n",
       " GradientBoostingRegressor                0.125087                0.257019   \n",
       " AdaBoostRegressor                        0.173866                0.319790   \n",
       " XGBRegressor                             0.141224                0.276066   \n",
       " ExtraTreesRegressor                      0.127932                0.259208   \n",
       " LinearRegression                         0.125036                0.255569   \n",
       " KNeighborsRegressor                      0.149706                0.283514   \n",
       " SVR                                      0.132610                0.259854   \n",
       " MLPRegressor                             0.138850                0.276482   \n",
       " \n",
       "                            Train RMSE (10 fold CV)  Train R2 (10 fold CV)  \\\n",
       " LGBMRegressor                             0.354656               0.798042   \n",
       " DecisionTreeRegressor                     0.502353               0.594805   \n",
       " RandomForestRegressor                     0.357567               0.794714   \n",
       " GradientBoostingRegressor                 0.353676               0.799157   \n",
       " AdaBoostRegressor                         0.416972               0.720836   \n",
       " XGBRegressor                              0.375798               0.773246   \n",
       " ExtraTreesRegressor                       0.357676               0.794588   \n",
       " LinearRegression                          0.353604               0.799239   \n",
       " KNeighborsRegressor                       0.386919               0.759627   \n",
       " SVR                                       0.364156               0.787078   \n",
       " MLPRegressor                              0.372625               0.777059   \n",
       " \n",
       "                            Train PCC (10 fold CV)  Train SCC (10 fold CV)  \\\n",
       " LGBMRegressor                            0.893349                0.876897   \n",
       " DecisionTreeRegressor                    0.799321                0.775558   \n",
       " RandomForestRegressor                    0.891494                0.874756   \n",
       " GradientBoostingRegressor                0.893964                0.876321   \n",
       " AdaBoostRegressor                        0.859506                0.851227   \n",
       " XGBRegressor                             0.879863                0.859806   \n",
       " ExtraTreesRegressor                      0.891426                0.874096   \n",
       " LinearRegression                         0.894004                0.877131   \n",
       " KNeighborsRegressor                      0.871711                0.852315   \n",
       " SVR                                      0.887462                0.876649   \n",
       " MLPRegressor                             0.882955                0.864167   \n",
       " \n",
       "                            Test MSE  Test MAE  Test RMSE   Test R2  Test PCC  \\\n",
       " LGBMRegressor              0.191068  0.306364   0.437114  0.698072  0.835614   \n",
       " DecisionTreeRegressor      0.208692  0.322135   0.456828  0.670224  0.820248   \n",
       " RandomForestRegressor      0.194917  0.309333   0.441494  0.691991  0.832140   \n",
       " GradientBoostingRegressor  0.192349  0.307660   0.438576  0.696049  0.834473   \n",
       " AdaBoostRegressor          0.214561  0.347395   0.463208  0.660949  0.821440   \n",
       " XGBRegressor               0.195996  0.309806   0.442714  0.690286  0.831231   \n",
       " ExtraTreesRegressor        0.193109  0.308256   0.439442  0.694847  0.833804   \n",
       " LinearRegression           0.192681  0.308454   0.438955  0.695524  0.834174   \n",
       " KNeighborsRegressor        0.207293  0.323132   0.455295  0.672434  0.821086   \n",
       " SVR                        0.194822  0.306770   0.441386  0.692141  0.833169   \n",
       " MLPRegressor               0.192956  0.315153   0.439268  0.695089  0.834604   \n",
       " \n",
       "                            Test SCC  \n",
       " LGBMRegressor              0.820365  \n",
       " DecisionTreeRegressor      0.799778  \n",
       " RandomForestRegressor      0.815763  \n",
       " GradientBoostingRegressor  0.820331  \n",
       " AdaBoostRegressor          0.812258  \n",
       " XGBRegressor               0.815661  \n",
       " ExtraTreesRegressor        0.817990  \n",
       " LinearRegression           0.819747  \n",
       " KNeighborsRegressor        0.803682  \n",
       " SVR                        0.818860  \n",
       " MLPRegressor               0.820176  ,\n",
       " 'Ablation_Embeddings':                            Train MSE (10 fold CV)  Train MAE (10 fold CV)  \\\n",
       " LGBMRegressor                            0.197204                0.325799   \n",
       " DecisionTreeRegressor                    0.400688                0.458130   \n",
       " RandomForestRegressor                    0.193206                0.322189   \n",
       " GradientBoostingRegressor                0.198992                0.327857   \n",
       " AdaBoostRegressor                        0.259560                0.402047   \n",
       " XGBRegressor                             0.219899                0.344418   \n",
       " ExtraTreesRegressor                      0.186845                0.315897   \n",
       " LinearRegression                         0.196513                0.325000   \n",
       " KNeighborsRegressor                      0.222483                0.343583   \n",
       " SVR                                      0.208088                0.328868   \n",
       " MLPRegressor                             0.208874                0.340886   \n",
       " \n",
       "                            Train RMSE (10 fold CV)  Train R2 (10 fold CV)  \\\n",
       " LGBMRegressor                             0.444077               0.683363   \n",
       " DecisionTreeRegressor                     0.632999               0.356643   \n",
       " RandomForestRegressor                     0.439552               0.689782   \n",
       " GradientBoostingRegressor                 0.446085               0.680493   \n",
       " AdaBoostRegressor                         0.509470               0.583243   \n",
       " XGBRegressor                              0.468934               0.646924   \n",
       " ExtraTreesRegressor                       0.432256               0.699996   \n",
       " LinearRegression                          0.443298               0.684472   \n",
       " KNeighborsRegressor                       0.471681               0.642775   \n",
       " SVR                                       0.456167               0.665888   \n",
       " MLPRegressor                              0.457027               0.664626   \n",
       " \n",
       "                            Train PCC (10 fold CV)  Train SCC (10 fold CV)  \\\n",
       " LGBMRegressor                            0.826659                0.798828   \n",
       " DecisionTreeRegressor                    0.680875                0.656305   \n",
       " RandomForestRegressor                    0.830666                0.804122   \n",
       " GradientBoostingRegressor                0.824943                0.795793   \n",
       " AdaBoostRegressor                        0.789216                0.761066   \n",
       " XGBRegressor                             0.806869                0.778609   \n",
       " ExtraTreesRegressor                      0.836786                0.811588   \n",
       " LinearRegression                         0.827342                0.797443   \n",
       " KNeighborsRegressor                      0.802678                0.773852   \n",
       " SVR                                      0.817145                0.792233   \n",
       " MLPRegressor                             0.816629                0.784871   \n",
       " \n",
       "                            Test MSE  Test MAE  Test RMSE   Test R2  Test PCC  \\\n",
       " LGBMRegressor              0.207748  0.326902   0.455794  0.671715  0.819593   \n",
       " DecisionTreeRegressor      0.249740  0.355437   0.499740  0.605359  0.780632   \n",
       " RandomForestRegressor      0.208753  0.326254   0.456895  0.670127  0.818713   \n",
       " GradientBoostingRegressor  0.203710  0.324691   0.451342  0.678096  0.823480   \n",
       " AdaBoostRegressor          0.247968  0.388195   0.497964  0.608159  0.804846   \n",
       " XGBRegressor               0.219158  0.334292   0.468143  0.653685  0.809387   \n",
       " ExtraTreesRegressor        0.211241  0.328015   0.459610  0.666195  0.816328   \n",
       " LinearRegression           0.198451  0.318054   0.445478  0.686406  0.828584   \n",
       " KNeighborsRegressor        0.235183  0.350971   0.484956  0.628363  0.794114   \n",
       " SVR                        0.206904  0.320719   0.454867  0.673048  0.821811   \n",
       " MLPRegressor               0.202372  0.326251   0.449858  0.680210  0.825356   \n",
       " \n",
       "                            Test SCC  \n",
       " LGBMRegressor              0.806872  \n",
       " DecisionTreeRegressor      0.762524  \n",
       " RandomForestRegressor      0.803257  \n",
       " GradientBoostingRegressor  0.808707  \n",
       " AdaBoostRegressor          0.791111  \n",
       " XGBRegressor               0.795459  \n",
       " ExtraTreesRegressor        0.802155  \n",
       " LinearRegression           0.814382  \n",
       " KNeighborsRegressor        0.776606  \n",
       " SVR                        0.811838  \n",
       " MLPRegressor               0.808979  ,\n",
       " 'Ablation_Atomic':                            Train MSE (10 fold CV)  Train MAE (10 fold CV)  \\\n",
       " LGBMRegressor                            0.125275                0.257694   \n",
       " DecisionTreeRegressor                    0.263743                0.374735   \n",
       " RandomForestRegressor                    0.127749                0.259039   \n",
       " GradientBoostingRegressor                0.125566                0.257346   \n",
       " AdaBoostRegressor                        0.178632                0.325035   \n",
       " XGBRegressor                             0.141496                0.275541   \n",
       " ExtraTreesRegressor                      0.126231                0.257413   \n",
       " LinearRegression                         0.124419                0.255014   \n",
       " KNeighborsRegressor                      0.151292                0.285708   \n",
       " SVR                                      0.131419                0.259541   \n",
       " MLPRegressor                             0.134841                0.268003   \n",
       " \n",
       "                            Train RMSE (10 fold CV)  Train R2 (10 fold CV)  \\\n",
       " LGBMRegressor                             0.353942               0.798855   \n",
       " DecisionTreeRegressor                     0.513559               0.576526   \n",
       " RandomForestRegressor                     0.357421               0.794882   \n",
       " GradientBoostingRegressor                 0.354353               0.798388   \n",
       " AdaBoostRegressor                         0.422649               0.713183   \n",
       " XGBRegressor                              0.376159               0.772810   \n",
       " ExtraTreesRegressor                       0.355290               0.797320   \n",
       " LinearRegression                          0.352731               0.800229   \n",
       " KNeighborsRegressor                       0.388962               0.757082   \n",
       " SVR                                       0.362517               0.788990   \n",
       " MLPRegressor                              0.367207               0.783495   \n",
       " \n",
       "                            Train PCC (10 fold CV)  Train SCC (10 fold CV)  \\\n",
       " LGBMRegressor                            0.893805                0.877471   \n",
       " DecisionTreeRegressor                    0.792243                0.768514   \n",
       " RandomForestRegressor                    0.891594                0.875051   \n",
       " GradientBoostingRegressor                0.893527                0.876989   \n",
       " AdaBoostRegressor                        0.857310                0.848489   \n",
       " XGBRegressor                             0.879515                0.862526   \n",
       " ExtraTreesRegressor                      0.892954                0.875876   \n",
       " LinearRegression                         0.894557                0.877798   \n",
       " KNeighborsRegressor                      0.870208                0.848718   \n",
       " SVR                                      0.888530                0.877179   \n",
       " MLPRegressor                             0.885973                0.865999   \n",
       " \n",
       "                            Test MSE  Test MAE  Test RMSE   Test R2  Test PCC  \\\n",
       " LGBMRegressor              0.193031  0.308192   0.439353  0.694971  0.833787   \n",
       " DecisionTreeRegressor      0.212271  0.322619   0.460729  0.664568  0.816842   \n",
       " RandomForestRegressor      0.195506  0.309744   0.442161  0.691060  0.831602   \n",
       " GradientBoostingRegressor  0.191852  0.308384   0.438009  0.696834  0.834908   \n",
       " AdaBoostRegressor          0.220008  0.355082   0.469050  0.652342  0.817703   \n",
       " XGBRegressor               0.197481  0.312857   0.444389  0.687939  0.829882   \n",
       " ExtraTreesRegressor        0.194766  0.309874   0.441323  0.692229  0.832291   \n",
       " LinearRegression           0.192792  0.307843   0.439081  0.695348  0.834029   \n",
       " KNeighborsRegressor        0.201650  0.322872   0.449054  0.681351  0.825972   \n",
       " SVR                        0.192793  0.305884   0.439083  0.695346  0.834871   \n",
       " MLPRegressor               0.196292  0.312627   0.443049  0.689818  0.831816   \n",
       " \n",
       "                            Test SCC  \n",
       " LGBMRegressor              0.820005  \n",
       " DecisionTreeRegressor      0.803776  \n",
       " RandomForestRegressor      0.816087  \n",
       " GradientBoostingRegressor  0.819767  \n",
       " AdaBoostRegressor          0.807913  \n",
       " XGBRegressor               0.815774  \n",
       " ExtraTreesRegressor        0.815771  \n",
       " LinearRegression           0.817838  \n",
       " KNeighborsRegressor        0.803488  \n",
       " SVR                        0.818940  \n",
       " MLPRegressor               0.817597  }"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "ablation_result_dir = '/home/users/akshay/PCPpred/PAMPA/Results/Ablation/'\n",
    "os.makedirs(ablation_result_dir, exist_ok=True)\n",
    "\n",
    "pickle_path = os.path.join(ablation_result_dir, 'ablation_results.pkl')\n",
    "with open(pickle_path, 'wb') as f:\n",
    "    pickle.dump(ablation_results, f)\n",
    "\n",
    "\n",
    "with open(pickle_path, 'rb') as f:\n",
    "    ablation_results = pickle.load(f)\n",
    "\n",
    "\n",
    "ablation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97549c82-4b7b-4c96-80be-ec7807b25e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Ablation_Descriptor: \n",
      "\n",
      "Results for Ablation_Fingerprints: \n",
      "\n",
      "Results for Ablation_Embeddings: \n",
      "\n",
      "Results for Ablation_Atomic: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ablation_result_dir = '/home/users/akshay/PCPpred/PAMPA/Results/Ablation/'\n",
    "os.makedirs(ablation_result_dir, exist_ok=True)\n",
    "\n",
    "for ablation_label, df in ablation_results.items():\n",
    "    print(f\"Results for {ablation_label}: \\n\")\n",
    "    safe_label = ablation_label.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    file_path = os.path.join(ablation_result_dir, f\"{safe_label}.csv\")\n",
    "    df.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40fae01e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Ablation_Descriptor: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train MSE (10 fold CV)</th>\n",
       "      <th>Train MAE (10 fold CV)</th>\n",
       "      <th>Train RMSE (10 fold CV)</th>\n",
       "      <th>Train R2 (10 fold CV)</th>\n",
       "      <th>Train PCC (10 fold CV)</th>\n",
       "      <th>Train SCC (10 fold CV)</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Test MAE</th>\n",
       "      <th>Test RMSE</th>\n",
       "      <th>Test R2</th>\n",
       "      <th>Test PCC</th>\n",
       "      <th>Test SCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <td>0.124547</td>\n",
       "      <td>0.257339</td>\n",
       "      <td>0.352913</td>\n",
       "      <td>0.800023</td>\n",
       "      <td>0.894462</td>\n",
       "      <td>0.877640</td>\n",
       "      <td>0.193359</td>\n",
       "      <td>0.309692</td>\n",
       "      <td>0.439726</td>\n",
       "      <td>0.694453</td>\n",
       "      <td>0.833456</td>\n",
       "      <td>0.820650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>0.271159</td>\n",
       "      <td>0.378883</td>\n",
       "      <td>0.520729</td>\n",
       "      <td>0.564619</td>\n",
       "      <td>0.784426</td>\n",
       "      <td>0.759031</td>\n",
       "      <td>0.216569</td>\n",
       "      <td>0.327040</td>\n",
       "      <td>0.465369</td>\n",
       "      <td>0.657777</td>\n",
       "      <td>0.812709</td>\n",
       "      <td>0.799445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>0.127370</td>\n",
       "      <td>0.258899</td>\n",
       "      <td>0.356889</td>\n",
       "      <td>0.795491</td>\n",
       "      <td>0.891936</td>\n",
       "      <td>0.875326</td>\n",
       "      <td>0.196710</td>\n",
       "      <td>0.311360</td>\n",
       "      <td>0.443520</td>\n",
       "      <td>0.689158</td>\n",
       "      <td>0.830406</td>\n",
       "      <td>0.816779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>0.124091</td>\n",
       "      <td>0.256223</td>\n",
       "      <td>0.352266</td>\n",
       "      <td>0.800755</td>\n",
       "      <td>0.894851</td>\n",
       "      <td>0.876724</td>\n",
       "      <td>0.194348</td>\n",
       "      <td>0.310251</td>\n",
       "      <td>0.440849</td>\n",
       "      <td>0.692890</td>\n",
       "      <td>0.832579</td>\n",
       "      <td>0.819734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostRegressor</th>\n",
       "      <td>0.173011</td>\n",
       "      <td>0.317938</td>\n",
       "      <td>0.415946</td>\n",
       "      <td>0.722208</td>\n",
       "      <td>0.859052</td>\n",
       "      <td>0.850397</td>\n",
       "      <td>0.220874</td>\n",
       "      <td>0.352171</td>\n",
       "      <td>0.469972</td>\n",
       "      <td>0.650974</td>\n",
       "      <td>0.814813</td>\n",
       "      <td>0.809462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBRegressor</th>\n",
       "      <td>0.141366</td>\n",
       "      <td>0.274843</td>\n",
       "      <td>0.375987</td>\n",
       "      <td>0.773019</td>\n",
       "      <td>0.880031</td>\n",
       "      <td>0.862635</td>\n",
       "      <td>0.197695</td>\n",
       "      <td>0.313840</td>\n",
       "      <td>0.444629</td>\n",
       "      <td>0.687601</td>\n",
       "      <td>0.829688</td>\n",
       "      <td>0.816433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesRegressor</th>\n",
       "      <td>0.127268</td>\n",
       "      <td>0.258200</td>\n",
       "      <td>0.356747</td>\n",
       "      <td>0.795654</td>\n",
       "      <td>0.892023</td>\n",
       "      <td>0.874573</td>\n",
       "      <td>0.196641</td>\n",
       "      <td>0.311836</td>\n",
       "      <td>0.443442</td>\n",
       "      <td>0.689267</td>\n",
       "      <td>0.830498</td>\n",
       "      <td>0.816118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>0.124801</td>\n",
       "      <td>0.255590</td>\n",
       "      <td>0.353271</td>\n",
       "      <td>0.799617</td>\n",
       "      <td>0.894215</td>\n",
       "      <td>0.877585</td>\n",
       "      <td>0.195866</td>\n",
       "      <td>0.310431</td>\n",
       "      <td>0.442568</td>\n",
       "      <td>0.690491</td>\n",
       "      <td>0.831149</td>\n",
       "      <td>0.816576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsRegressor</th>\n",
       "      <td>0.151503</td>\n",
       "      <td>0.284705</td>\n",
       "      <td>0.389234</td>\n",
       "      <td>0.756742</td>\n",
       "      <td>0.869998</td>\n",
       "      <td>0.849301</td>\n",
       "      <td>0.199434</td>\n",
       "      <td>0.320926</td>\n",
       "      <td>0.446581</td>\n",
       "      <td>0.684853</td>\n",
       "      <td>0.827842</td>\n",
       "      <td>0.805840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>0.134686</td>\n",
       "      <td>0.262531</td>\n",
       "      <td>0.366995</td>\n",
       "      <td>0.783745</td>\n",
       "      <td>0.885559</td>\n",
       "      <td>0.874330</td>\n",
       "      <td>0.195332</td>\n",
       "      <td>0.307070</td>\n",
       "      <td>0.441963</td>\n",
       "      <td>0.691336</td>\n",
       "      <td>0.832594</td>\n",
       "      <td>0.818707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPRegressor</th>\n",
       "      <td>0.135549</td>\n",
       "      <td>0.271661</td>\n",
       "      <td>0.368170</td>\n",
       "      <td>0.782359</td>\n",
       "      <td>0.885030</td>\n",
       "      <td>0.866893</td>\n",
       "      <td>0.194889</td>\n",
       "      <td>0.313915</td>\n",
       "      <td>0.441462</td>\n",
       "      <td>0.692035</td>\n",
       "      <td>0.832066</td>\n",
       "      <td>0.817271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Train MSE (10 fold CV)  Train MAE (10 fold CV)  \\\n",
       "LGBMRegressor                            0.124547                0.257339   \n",
       "DecisionTreeRegressor                    0.271159                0.378883   \n",
       "RandomForestRegressor                    0.127370                0.258899   \n",
       "GradientBoostingRegressor                0.124091                0.256223   \n",
       "AdaBoostRegressor                        0.173011                0.317938   \n",
       "XGBRegressor                             0.141366                0.274843   \n",
       "ExtraTreesRegressor                      0.127268                0.258200   \n",
       "LinearRegression                         0.124801                0.255590   \n",
       "KNeighborsRegressor                      0.151503                0.284705   \n",
       "SVR                                      0.134686                0.262531   \n",
       "MLPRegressor                             0.135549                0.271661   \n",
       "\n",
       "                           Train RMSE (10 fold CV)  Train R2 (10 fold CV)  \\\n",
       "LGBMRegressor                             0.352913               0.800023   \n",
       "DecisionTreeRegressor                     0.520729               0.564619   \n",
       "RandomForestRegressor                     0.356889               0.795491   \n",
       "GradientBoostingRegressor                 0.352266               0.800755   \n",
       "AdaBoostRegressor                         0.415946               0.722208   \n",
       "XGBRegressor                              0.375987               0.773019   \n",
       "ExtraTreesRegressor                       0.356747               0.795654   \n",
       "LinearRegression                          0.353271               0.799617   \n",
       "KNeighborsRegressor                       0.389234               0.756742   \n",
       "SVR                                       0.366995               0.783745   \n",
       "MLPRegressor                              0.368170               0.782359   \n",
       "\n",
       "                           Train PCC (10 fold CV)  Train SCC (10 fold CV)  \\\n",
       "LGBMRegressor                            0.894462                0.877640   \n",
       "DecisionTreeRegressor                    0.784426                0.759031   \n",
       "RandomForestRegressor                    0.891936                0.875326   \n",
       "GradientBoostingRegressor                0.894851                0.876724   \n",
       "AdaBoostRegressor                        0.859052                0.850397   \n",
       "XGBRegressor                             0.880031                0.862635   \n",
       "ExtraTreesRegressor                      0.892023                0.874573   \n",
       "LinearRegression                         0.894215                0.877585   \n",
       "KNeighborsRegressor                      0.869998                0.849301   \n",
       "SVR                                      0.885559                0.874330   \n",
       "MLPRegressor                             0.885030                0.866893   \n",
       "\n",
       "                           Test MSE  Test MAE  Test RMSE   Test R2  Test PCC  \\\n",
       "LGBMRegressor              0.193359  0.309692   0.439726  0.694453  0.833456   \n",
       "DecisionTreeRegressor      0.216569  0.327040   0.465369  0.657777  0.812709   \n",
       "RandomForestRegressor      0.196710  0.311360   0.443520  0.689158  0.830406   \n",
       "GradientBoostingRegressor  0.194348  0.310251   0.440849  0.692890  0.832579   \n",
       "AdaBoostRegressor          0.220874  0.352171   0.469972  0.650974  0.814813   \n",
       "XGBRegressor               0.197695  0.313840   0.444629  0.687601  0.829688   \n",
       "ExtraTreesRegressor        0.196641  0.311836   0.443442  0.689267  0.830498   \n",
       "LinearRegression           0.195866  0.310431   0.442568  0.690491  0.831149   \n",
       "KNeighborsRegressor        0.199434  0.320926   0.446581  0.684853  0.827842   \n",
       "SVR                        0.195332  0.307070   0.441963  0.691336  0.832594   \n",
       "MLPRegressor               0.194889  0.313915   0.441462  0.692035  0.832066   \n",
       "\n",
       "                           Test SCC  \n",
       "LGBMRegressor              0.820650  \n",
       "DecisionTreeRegressor      0.799445  \n",
       "RandomForestRegressor      0.816779  \n",
       "GradientBoostingRegressor  0.819734  \n",
       "AdaBoostRegressor          0.809462  \n",
       "XGBRegressor               0.816433  \n",
       "ExtraTreesRegressor        0.816118  \n",
       "LinearRegression           0.816576  \n",
       "KNeighborsRegressor        0.805840  \n",
       "SVR                        0.818707  \n",
       "MLPRegressor               0.817271  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Ablation_Fingerprints: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train MSE (10 fold CV)</th>\n",
       "      <th>Train MAE (10 fold CV)</th>\n",
       "      <th>Train RMSE (10 fold CV)</th>\n",
       "      <th>Train R2 (10 fold CV)</th>\n",
       "      <th>Train PCC (10 fold CV)</th>\n",
       "      <th>Train SCC (10 fold CV)</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Test MAE</th>\n",
       "      <th>Test RMSE</th>\n",
       "      <th>Test R2</th>\n",
       "      <th>Test PCC</th>\n",
       "      <th>Test SCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <td>0.125781</td>\n",
       "      <td>0.257481</td>\n",
       "      <td>0.354656</td>\n",
       "      <td>0.798042</td>\n",
       "      <td>0.893349</td>\n",
       "      <td>0.876897</td>\n",
       "      <td>0.191068</td>\n",
       "      <td>0.306364</td>\n",
       "      <td>0.437114</td>\n",
       "      <td>0.698072</td>\n",
       "      <td>0.835614</td>\n",
       "      <td>0.820365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>0.252359</td>\n",
       "      <td>0.368848</td>\n",
       "      <td>0.502353</td>\n",
       "      <td>0.594805</td>\n",
       "      <td>0.799321</td>\n",
       "      <td>0.775558</td>\n",
       "      <td>0.208692</td>\n",
       "      <td>0.322135</td>\n",
       "      <td>0.456828</td>\n",
       "      <td>0.670224</td>\n",
       "      <td>0.820248</td>\n",
       "      <td>0.799778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>0.127854</td>\n",
       "      <td>0.259588</td>\n",
       "      <td>0.357567</td>\n",
       "      <td>0.794714</td>\n",
       "      <td>0.891494</td>\n",
       "      <td>0.874756</td>\n",
       "      <td>0.194917</td>\n",
       "      <td>0.309333</td>\n",
       "      <td>0.441494</td>\n",
       "      <td>0.691991</td>\n",
       "      <td>0.832140</td>\n",
       "      <td>0.815763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>0.125087</td>\n",
       "      <td>0.257019</td>\n",
       "      <td>0.353676</td>\n",
       "      <td>0.799157</td>\n",
       "      <td>0.893964</td>\n",
       "      <td>0.876321</td>\n",
       "      <td>0.192349</td>\n",
       "      <td>0.307660</td>\n",
       "      <td>0.438576</td>\n",
       "      <td>0.696049</td>\n",
       "      <td>0.834473</td>\n",
       "      <td>0.820331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostRegressor</th>\n",
       "      <td>0.173866</td>\n",
       "      <td>0.319790</td>\n",
       "      <td>0.416972</td>\n",
       "      <td>0.720836</td>\n",
       "      <td>0.859506</td>\n",
       "      <td>0.851227</td>\n",
       "      <td>0.214561</td>\n",
       "      <td>0.347395</td>\n",
       "      <td>0.463208</td>\n",
       "      <td>0.660949</td>\n",
       "      <td>0.821440</td>\n",
       "      <td>0.812258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBRegressor</th>\n",
       "      <td>0.141224</td>\n",
       "      <td>0.276066</td>\n",
       "      <td>0.375798</td>\n",
       "      <td>0.773246</td>\n",
       "      <td>0.879863</td>\n",
       "      <td>0.859806</td>\n",
       "      <td>0.195996</td>\n",
       "      <td>0.309806</td>\n",
       "      <td>0.442714</td>\n",
       "      <td>0.690286</td>\n",
       "      <td>0.831231</td>\n",
       "      <td>0.815661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesRegressor</th>\n",
       "      <td>0.127932</td>\n",
       "      <td>0.259208</td>\n",
       "      <td>0.357676</td>\n",
       "      <td>0.794588</td>\n",
       "      <td>0.891426</td>\n",
       "      <td>0.874096</td>\n",
       "      <td>0.193109</td>\n",
       "      <td>0.308256</td>\n",
       "      <td>0.439442</td>\n",
       "      <td>0.694847</td>\n",
       "      <td>0.833804</td>\n",
       "      <td>0.817990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>0.125036</td>\n",
       "      <td>0.255569</td>\n",
       "      <td>0.353604</td>\n",
       "      <td>0.799239</td>\n",
       "      <td>0.894004</td>\n",
       "      <td>0.877131</td>\n",
       "      <td>0.192681</td>\n",
       "      <td>0.308454</td>\n",
       "      <td>0.438955</td>\n",
       "      <td>0.695524</td>\n",
       "      <td>0.834174</td>\n",
       "      <td>0.819747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsRegressor</th>\n",
       "      <td>0.149706</td>\n",
       "      <td>0.283514</td>\n",
       "      <td>0.386919</td>\n",
       "      <td>0.759627</td>\n",
       "      <td>0.871711</td>\n",
       "      <td>0.852315</td>\n",
       "      <td>0.207293</td>\n",
       "      <td>0.323132</td>\n",
       "      <td>0.455295</td>\n",
       "      <td>0.672434</td>\n",
       "      <td>0.821086</td>\n",
       "      <td>0.803682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>0.132610</td>\n",
       "      <td>0.259854</td>\n",
       "      <td>0.364156</td>\n",
       "      <td>0.787078</td>\n",
       "      <td>0.887462</td>\n",
       "      <td>0.876649</td>\n",
       "      <td>0.194822</td>\n",
       "      <td>0.306770</td>\n",
       "      <td>0.441386</td>\n",
       "      <td>0.692141</td>\n",
       "      <td>0.833169</td>\n",
       "      <td>0.818860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPRegressor</th>\n",
       "      <td>0.138850</td>\n",
       "      <td>0.276482</td>\n",
       "      <td>0.372625</td>\n",
       "      <td>0.777059</td>\n",
       "      <td>0.882955</td>\n",
       "      <td>0.864167</td>\n",
       "      <td>0.192956</td>\n",
       "      <td>0.315153</td>\n",
       "      <td>0.439268</td>\n",
       "      <td>0.695089</td>\n",
       "      <td>0.834604</td>\n",
       "      <td>0.820176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Train MSE (10 fold CV)  Train MAE (10 fold CV)  \\\n",
       "LGBMRegressor                            0.125781                0.257481   \n",
       "DecisionTreeRegressor                    0.252359                0.368848   \n",
       "RandomForestRegressor                    0.127854                0.259588   \n",
       "GradientBoostingRegressor                0.125087                0.257019   \n",
       "AdaBoostRegressor                        0.173866                0.319790   \n",
       "XGBRegressor                             0.141224                0.276066   \n",
       "ExtraTreesRegressor                      0.127932                0.259208   \n",
       "LinearRegression                         0.125036                0.255569   \n",
       "KNeighborsRegressor                      0.149706                0.283514   \n",
       "SVR                                      0.132610                0.259854   \n",
       "MLPRegressor                             0.138850                0.276482   \n",
       "\n",
       "                           Train RMSE (10 fold CV)  Train R2 (10 fold CV)  \\\n",
       "LGBMRegressor                             0.354656               0.798042   \n",
       "DecisionTreeRegressor                     0.502353               0.594805   \n",
       "RandomForestRegressor                     0.357567               0.794714   \n",
       "GradientBoostingRegressor                 0.353676               0.799157   \n",
       "AdaBoostRegressor                         0.416972               0.720836   \n",
       "XGBRegressor                              0.375798               0.773246   \n",
       "ExtraTreesRegressor                       0.357676               0.794588   \n",
       "LinearRegression                          0.353604               0.799239   \n",
       "KNeighborsRegressor                       0.386919               0.759627   \n",
       "SVR                                       0.364156               0.787078   \n",
       "MLPRegressor                              0.372625               0.777059   \n",
       "\n",
       "                           Train PCC (10 fold CV)  Train SCC (10 fold CV)  \\\n",
       "LGBMRegressor                            0.893349                0.876897   \n",
       "DecisionTreeRegressor                    0.799321                0.775558   \n",
       "RandomForestRegressor                    0.891494                0.874756   \n",
       "GradientBoostingRegressor                0.893964                0.876321   \n",
       "AdaBoostRegressor                        0.859506                0.851227   \n",
       "XGBRegressor                             0.879863                0.859806   \n",
       "ExtraTreesRegressor                      0.891426                0.874096   \n",
       "LinearRegression                         0.894004                0.877131   \n",
       "KNeighborsRegressor                      0.871711                0.852315   \n",
       "SVR                                      0.887462                0.876649   \n",
       "MLPRegressor                             0.882955                0.864167   \n",
       "\n",
       "                           Test MSE  Test MAE  Test RMSE   Test R2  Test PCC  \\\n",
       "LGBMRegressor              0.191068  0.306364   0.437114  0.698072  0.835614   \n",
       "DecisionTreeRegressor      0.208692  0.322135   0.456828  0.670224  0.820248   \n",
       "RandomForestRegressor      0.194917  0.309333   0.441494  0.691991  0.832140   \n",
       "GradientBoostingRegressor  0.192349  0.307660   0.438576  0.696049  0.834473   \n",
       "AdaBoostRegressor          0.214561  0.347395   0.463208  0.660949  0.821440   \n",
       "XGBRegressor               0.195996  0.309806   0.442714  0.690286  0.831231   \n",
       "ExtraTreesRegressor        0.193109  0.308256   0.439442  0.694847  0.833804   \n",
       "LinearRegression           0.192681  0.308454   0.438955  0.695524  0.834174   \n",
       "KNeighborsRegressor        0.207293  0.323132   0.455295  0.672434  0.821086   \n",
       "SVR                        0.194822  0.306770   0.441386  0.692141  0.833169   \n",
       "MLPRegressor               0.192956  0.315153   0.439268  0.695089  0.834604   \n",
       "\n",
       "                           Test SCC  \n",
       "LGBMRegressor              0.820365  \n",
       "DecisionTreeRegressor      0.799778  \n",
       "RandomForestRegressor      0.815763  \n",
       "GradientBoostingRegressor  0.820331  \n",
       "AdaBoostRegressor          0.812258  \n",
       "XGBRegressor               0.815661  \n",
       "ExtraTreesRegressor        0.817990  \n",
       "LinearRegression           0.819747  \n",
       "KNeighborsRegressor        0.803682  \n",
       "SVR                        0.818860  \n",
       "MLPRegressor               0.820176  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Ablation_Embeddings: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train MSE (10 fold CV)</th>\n",
       "      <th>Train MAE (10 fold CV)</th>\n",
       "      <th>Train RMSE (10 fold CV)</th>\n",
       "      <th>Train R2 (10 fold CV)</th>\n",
       "      <th>Train PCC (10 fold CV)</th>\n",
       "      <th>Train SCC (10 fold CV)</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Test MAE</th>\n",
       "      <th>Test RMSE</th>\n",
       "      <th>Test R2</th>\n",
       "      <th>Test PCC</th>\n",
       "      <th>Test SCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <td>0.197204</td>\n",
       "      <td>0.325799</td>\n",
       "      <td>0.444077</td>\n",
       "      <td>0.683363</td>\n",
       "      <td>0.826659</td>\n",
       "      <td>0.798828</td>\n",
       "      <td>0.207748</td>\n",
       "      <td>0.326902</td>\n",
       "      <td>0.455794</td>\n",
       "      <td>0.671715</td>\n",
       "      <td>0.819593</td>\n",
       "      <td>0.806872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>0.400688</td>\n",
       "      <td>0.458130</td>\n",
       "      <td>0.632999</td>\n",
       "      <td>0.356643</td>\n",
       "      <td>0.680875</td>\n",
       "      <td>0.656305</td>\n",
       "      <td>0.249740</td>\n",
       "      <td>0.355437</td>\n",
       "      <td>0.499740</td>\n",
       "      <td>0.605359</td>\n",
       "      <td>0.780632</td>\n",
       "      <td>0.762524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>0.193206</td>\n",
       "      <td>0.322189</td>\n",
       "      <td>0.439552</td>\n",
       "      <td>0.689782</td>\n",
       "      <td>0.830666</td>\n",
       "      <td>0.804122</td>\n",
       "      <td>0.208753</td>\n",
       "      <td>0.326254</td>\n",
       "      <td>0.456895</td>\n",
       "      <td>0.670127</td>\n",
       "      <td>0.818713</td>\n",
       "      <td>0.803257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>0.198992</td>\n",
       "      <td>0.327857</td>\n",
       "      <td>0.446085</td>\n",
       "      <td>0.680493</td>\n",
       "      <td>0.824943</td>\n",
       "      <td>0.795793</td>\n",
       "      <td>0.203710</td>\n",
       "      <td>0.324691</td>\n",
       "      <td>0.451342</td>\n",
       "      <td>0.678096</td>\n",
       "      <td>0.823480</td>\n",
       "      <td>0.808707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostRegressor</th>\n",
       "      <td>0.259560</td>\n",
       "      <td>0.402047</td>\n",
       "      <td>0.509470</td>\n",
       "      <td>0.583243</td>\n",
       "      <td>0.789216</td>\n",
       "      <td>0.761066</td>\n",
       "      <td>0.247968</td>\n",
       "      <td>0.388195</td>\n",
       "      <td>0.497964</td>\n",
       "      <td>0.608159</td>\n",
       "      <td>0.804846</td>\n",
       "      <td>0.791111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBRegressor</th>\n",
       "      <td>0.219899</td>\n",
       "      <td>0.344418</td>\n",
       "      <td>0.468934</td>\n",
       "      <td>0.646924</td>\n",
       "      <td>0.806869</td>\n",
       "      <td>0.778609</td>\n",
       "      <td>0.219158</td>\n",
       "      <td>0.334292</td>\n",
       "      <td>0.468143</td>\n",
       "      <td>0.653685</td>\n",
       "      <td>0.809387</td>\n",
       "      <td>0.795459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesRegressor</th>\n",
       "      <td>0.186845</td>\n",
       "      <td>0.315897</td>\n",
       "      <td>0.432256</td>\n",
       "      <td>0.699996</td>\n",
       "      <td>0.836786</td>\n",
       "      <td>0.811588</td>\n",
       "      <td>0.211241</td>\n",
       "      <td>0.328015</td>\n",
       "      <td>0.459610</td>\n",
       "      <td>0.666195</td>\n",
       "      <td>0.816328</td>\n",
       "      <td>0.802155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>0.196513</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.443298</td>\n",
       "      <td>0.684472</td>\n",
       "      <td>0.827342</td>\n",
       "      <td>0.797443</td>\n",
       "      <td>0.198451</td>\n",
       "      <td>0.318054</td>\n",
       "      <td>0.445478</td>\n",
       "      <td>0.686406</td>\n",
       "      <td>0.828584</td>\n",
       "      <td>0.814382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsRegressor</th>\n",
       "      <td>0.222483</td>\n",
       "      <td>0.343583</td>\n",
       "      <td>0.471681</td>\n",
       "      <td>0.642775</td>\n",
       "      <td>0.802678</td>\n",
       "      <td>0.773852</td>\n",
       "      <td>0.235183</td>\n",
       "      <td>0.350971</td>\n",
       "      <td>0.484956</td>\n",
       "      <td>0.628363</td>\n",
       "      <td>0.794114</td>\n",
       "      <td>0.776606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>0.208088</td>\n",
       "      <td>0.328868</td>\n",
       "      <td>0.456167</td>\n",
       "      <td>0.665888</td>\n",
       "      <td>0.817145</td>\n",
       "      <td>0.792233</td>\n",
       "      <td>0.206904</td>\n",
       "      <td>0.320719</td>\n",
       "      <td>0.454867</td>\n",
       "      <td>0.673048</td>\n",
       "      <td>0.821811</td>\n",
       "      <td>0.811838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPRegressor</th>\n",
       "      <td>0.208874</td>\n",
       "      <td>0.340886</td>\n",
       "      <td>0.457027</td>\n",
       "      <td>0.664626</td>\n",
       "      <td>0.816629</td>\n",
       "      <td>0.784871</td>\n",
       "      <td>0.202372</td>\n",
       "      <td>0.326251</td>\n",
       "      <td>0.449858</td>\n",
       "      <td>0.680210</td>\n",
       "      <td>0.825356</td>\n",
       "      <td>0.808979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Train MSE (10 fold CV)  Train MAE (10 fold CV)  \\\n",
       "LGBMRegressor                            0.197204                0.325799   \n",
       "DecisionTreeRegressor                    0.400688                0.458130   \n",
       "RandomForestRegressor                    0.193206                0.322189   \n",
       "GradientBoostingRegressor                0.198992                0.327857   \n",
       "AdaBoostRegressor                        0.259560                0.402047   \n",
       "XGBRegressor                             0.219899                0.344418   \n",
       "ExtraTreesRegressor                      0.186845                0.315897   \n",
       "LinearRegression                         0.196513                0.325000   \n",
       "KNeighborsRegressor                      0.222483                0.343583   \n",
       "SVR                                      0.208088                0.328868   \n",
       "MLPRegressor                             0.208874                0.340886   \n",
       "\n",
       "                           Train RMSE (10 fold CV)  Train R2 (10 fold CV)  \\\n",
       "LGBMRegressor                             0.444077               0.683363   \n",
       "DecisionTreeRegressor                     0.632999               0.356643   \n",
       "RandomForestRegressor                     0.439552               0.689782   \n",
       "GradientBoostingRegressor                 0.446085               0.680493   \n",
       "AdaBoostRegressor                         0.509470               0.583243   \n",
       "XGBRegressor                              0.468934               0.646924   \n",
       "ExtraTreesRegressor                       0.432256               0.699996   \n",
       "LinearRegression                          0.443298               0.684472   \n",
       "KNeighborsRegressor                       0.471681               0.642775   \n",
       "SVR                                       0.456167               0.665888   \n",
       "MLPRegressor                              0.457027               0.664626   \n",
       "\n",
       "                           Train PCC (10 fold CV)  Train SCC (10 fold CV)  \\\n",
       "LGBMRegressor                            0.826659                0.798828   \n",
       "DecisionTreeRegressor                    0.680875                0.656305   \n",
       "RandomForestRegressor                    0.830666                0.804122   \n",
       "GradientBoostingRegressor                0.824943                0.795793   \n",
       "AdaBoostRegressor                        0.789216                0.761066   \n",
       "XGBRegressor                             0.806869                0.778609   \n",
       "ExtraTreesRegressor                      0.836786                0.811588   \n",
       "LinearRegression                         0.827342                0.797443   \n",
       "KNeighborsRegressor                      0.802678                0.773852   \n",
       "SVR                                      0.817145                0.792233   \n",
       "MLPRegressor                             0.816629                0.784871   \n",
       "\n",
       "                           Test MSE  Test MAE  Test RMSE   Test R2  Test PCC  \\\n",
       "LGBMRegressor              0.207748  0.326902   0.455794  0.671715  0.819593   \n",
       "DecisionTreeRegressor      0.249740  0.355437   0.499740  0.605359  0.780632   \n",
       "RandomForestRegressor      0.208753  0.326254   0.456895  0.670127  0.818713   \n",
       "GradientBoostingRegressor  0.203710  0.324691   0.451342  0.678096  0.823480   \n",
       "AdaBoostRegressor          0.247968  0.388195   0.497964  0.608159  0.804846   \n",
       "XGBRegressor               0.219158  0.334292   0.468143  0.653685  0.809387   \n",
       "ExtraTreesRegressor        0.211241  0.328015   0.459610  0.666195  0.816328   \n",
       "LinearRegression           0.198451  0.318054   0.445478  0.686406  0.828584   \n",
       "KNeighborsRegressor        0.235183  0.350971   0.484956  0.628363  0.794114   \n",
       "SVR                        0.206904  0.320719   0.454867  0.673048  0.821811   \n",
       "MLPRegressor               0.202372  0.326251   0.449858  0.680210  0.825356   \n",
       "\n",
       "                           Test SCC  \n",
       "LGBMRegressor              0.806872  \n",
       "DecisionTreeRegressor      0.762524  \n",
       "RandomForestRegressor      0.803257  \n",
       "GradientBoostingRegressor  0.808707  \n",
       "AdaBoostRegressor          0.791111  \n",
       "XGBRegressor               0.795459  \n",
       "ExtraTreesRegressor        0.802155  \n",
       "LinearRegression           0.814382  \n",
       "KNeighborsRegressor        0.776606  \n",
       "SVR                        0.811838  \n",
       "MLPRegressor               0.808979  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Ablation_Atomic: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train MSE (10 fold CV)</th>\n",
       "      <th>Train MAE (10 fold CV)</th>\n",
       "      <th>Train RMSE (10 fold CV)</th>\n",
       "      <th>Train R2 (10 fold CV)</th>\n",
       "      <th>Train PCC (10 fold CV)</th>\n",
       "      <th>Train SCC (10 fold CV)</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Test MAE</th>\n",
       "      <th>Test RMSE</th>\n",
       "      <th>Test R2</th>\n",
       "      <th>Test PCC</th>\n",
       "      <th>Test SCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <td>0.125275</td>\n",
       "      <td>0.257694</td>\n",
       "      <td>0.353942</td>\n",
       "      <td>0.798855</td>\n",
       "      <td>0.893805</td>\n",
       "      <td>0.877471</td>\n",
       "      <td>0.193031</td>\n",
       "      <td>0.308192</td>\n",
       "      <td>0.439353</td>\n",
       "      <td>0.694971</td>\n",
       "      <td>0.833787</td>\n",
       "      <td>0.820005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>0.263743</td>\n",
       "      <td>0.374735</td>\n",
       "      <td>0.513559</td>\n",
       "      <td>0.576526</td>\n",
       "      <td>0.792243</td>\n",
       "      <td>0.768514</td>\n",
       "      <td>0.212271</td>\n",
       "      <td>0.322619</td>\n",
       "      <td>0.460729</td>\n",
       "      <td>0.664568</td>\n",
       "      <td>0.816842</td>\n",
       "      <td>0.803776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>0.127749</td>\n",
       "      <td>0.259039</td>\n",
       "      <td>0.357421</td>\n",
       "      <td>0.794882</td>\n",
       "      <td>0.891594</td>\n",
       "      <td>0.875051</td>\n",
       "      <td>0.195506</td>\n",
       "      <td>0.309744</td>\n",
       "      <td>0.442161</td>\n",
       "      <td>0.691060</td>\n",
       "      <td>0.831602</td>\n",
       "      <td>0.816087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>0.125566</td>\n",
       "      <td>0.257346</td>\n",
       "      <td>0.354353</td>\n",
       "      <td>0.798388</td>\n",
       "      <td>0.893527</td>\n",
       "      <td>0.876989</td>\n",
       "      <td>0.191852</td>\n",
       "      <td>0.308384</td>\n",
       "      <td>0.438009</td>\n",
       "      <td>0.696834</td>\n",
       "      <td>0.834908</td>\n",
       "      <td>0.819767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostRegressor</th>\n",
       "      <td>0.178632</td>\n",
       "      <td>0.325035</td>\n",
       "      <td>0.422649</td>\n",
       "      <td>0.713183</td>\n",
       "      <td>0.857310</td>\n",
       "      <td>0.848489</td>\n",
       "      <td>0.220008</td>\n",
       "      <td>0.355082</td>\n",
       "      <td>0.469050</td>\n",
       "      <td>0.652342</td>\n",
       "      <td>0.817703</td>\n",
       "      <td>0.807913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBRegressor</th>\n",
       "      <td>0.141496</td>\n",
       "      <td>0.275541</td>\n",
       "      <td>0.376159</td>\n",
       "      <td>0.772810</td>\n",
       "      <td>0.879515</td>\n",
       "      <td>0.862526</td>\n",
       "      <td>0.197481</td>\n",
       "      <td>0.312857</td>\n",
       "      <td>0.444389</td>\n",
       "      <td>0.687939</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.815774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesRegressor</th>\n",
       "      <td>0.126231</td>\n",
       "      <td>0.257413</td>\n",
       "      <td>0.355290</td>\n",
       "      <td>0.797320</td>\n",
       "      <td>0.892954</td>\n",
       "      <td>0.875876</td>\n",
       "      <td>0.194766</td>\n",
       "      <td>0.309874</td>\n",
       "      <td>0.441323</td>\n",
       "      <td>0.692229</td>\n",
       "      <td>0.832291</td>\n",
       "      <td>0.815771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>0.124419</td>\n",
       "      <td>0.255014</td>\n",
       "      <td>0.352731</td>\n",
       "      <td>0.800229</td>\n",
       "      <td>0.894557</td>\n",
       "      <td>0.877798</td>\n",
       "      <td>0.192792</td>\n",
       "      <td>0.307843</td>\n",
       "      <td>0.439081</td>\n",
       "      <td>0.695348</td>\n",
       "      <td>0.834029</td>\n",
       "      <td>0.817838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsRegressor</th>\n",
       "      <td>0.151292</td>\n",
       "      <td>0.285708</td>\n",
       "      <td>0.388962</td>\n",
       "      <td>0.757082</td>\n",
       "      <td>0.870208</td>\n",
       "      <td>0.848718</td>\n",
       "      <td>0.201650</td>\n",
       "      <td>0.322872</td>\n",
       "      <td>0.449054</td>\n",
       "      <td>0.681351</td>\n",
       "      <td>0.825972</td>\n",
       "      <td>0.803488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>0.131419</td>\n",
       "      <td>0.259541</td>\n",
       "      <td>0.362517</td>\n",
       "      <td>0.788990</td>\n",
       "      <td>0.888530</td>\n",
       "      <td>0.877179</td>\n",
       "      <td>0.192793</td>\n",
       "      <td>0.305884</td>\n",
       "      <td>0.439083</td>\n",
       "      <td>0.695346</td>\n",
       "      <td>0.834871</td>\n",
       "      <td>0.818940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPRegressor</th>\n",
       "      <td>0.134841</td>\n",
       "      <td>0.268003</td>\n",
       "      <td>0.367207</td>\n",
       "      <td>0.783495</td>\n",
       "      <td>0.885973</td>\n",
       "      <td>0.865999</td>\n",
       "      <td>0.196292</td>\n",
       "      <td>0.312627</td>\n",
       "      <td>0.443049</td>\n",
       "      <td>0.689818</td>\n",
       "      <td>0.831816</td>\n",
       "      <td>0.817597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Train MSE (10 fold CV)  Train MAE (10 fold CV)  \\\n",
       "LGBMRegressor                            0.125275                0.257694   \n",
       "DecisionTreeRegressor                    0.263743                0.374735   \n",
       "RandomForestRegressor                    0.127749                0.259039   \n",
       "GradientBoostingRegressor                0.125566                0.257346   \n",
       "AdaBoostRegressor                        0.178632                0.325035   \n",
       "XGBRegressor                             0.141496                0.275541   \n",
       "ExtraTreesRegressor                      0.126231                0.257413   \n",
       "LinearRegression                         0.124419                0.255014   \n",
       "KNeighborsRegressor                      0.151292                0.285708   \n",
       "SVR                                      0.131419                0.259541   \n",
       "MLPRegressor                             0.134841                0.268003   \n",
       "\n",
       "                           Train RMSE (10 fold CV)  Train R2 (10 fold CV)  \\\n",
       "LGBMRegressor                             0.353942               0.798855   \n",
       "DecisionTreeRegressor                     0.513559               0.576526   \n",
       "RandomForestRegressor                     0.357421               0.794882   \n",
       "GradientBoostingRegressor                 0.354353               0.798388   \n",
       "AdaBoostRegressor                         0.422649               0.713183   \n",
       "XGBRegressor                              0.376159               0.772810   \n",
       "ExtraTreesRegressor                       0.355290               0.797320   \n",
       "LinearRegression                          0.352731               0.800229   \n",
       "KNeighborsRegressor                       0.388962               0.757082   \n",
       "SVR                                       0.362517               0.788990   \n",
       "MLPRegressor                              0.367207               0.783495   \n",
       "\n",
       "                           Train PCC (10 fold CV)  Train SCC (10 fold CV)  \\\n",
       "LGBMRegressor                            0.893805                0.877471   \n",
       "DecisionTreeRegressor                    0.792243                0.768514   \n",
       "RandomForestRegressor                    0.891594                0.875051   \n",
       "GradientBoostingRegressor                0.893527                0.876989   \n",
       "AdaBoostRegressor                        0.857310                0.848489   \n",
       "XGBRegressor                             0.879515                0.862526   \n",
       "ExtraTreesRegressor                      0.892954                0.875876   \n",
       "LinearRegression                         0.894557                0.877798   \n",
       "KNeighborsRegressor                      0.870208                0.848718   \n",
       "SVR                                      0.888530                0.877179   \n",
       "MLPRegressor                             0.885973                0.865999   \n",
       "\n",
       "                           Test MSE  Test MAE  Test RMSE   Test R2  Test PCC  \\\n",
       "LGBMRegressor              0.193031  0.308192   0.439353  0.694971  0.833787   \n",
       "DecisionTreeRegressor      0.212271  0.322619   0.460729  0.664568  0.816842   \n",
       "RandomForestRegressor      0.195506  0.309744   0.442161  0.691060  0.831602   \n",
       "GradientBoostingRegressor  0.191852  0.308384   0.438009  0.696834  0.834908   \n",
       "AdaBoostRegressor          0.220008  0.355082   0.469050  0.652342  0.817703   \n",
       "XGBRegressor               0.197481  0.312857   0.444389  0.687939  0.829882   \n",
       "ExtraTreesRegressor        0.194766  0.309874   0.441323  0.692229  0.832291   \n",
       "LinearRegression           0.192792  0.307843   0.439081  0.695348  0.834029   \n",
       "KNeighborsRegressor        0.201650  0.322872   0.449054  0.681351  0.825972   \n",
       "SVR                        0.192793  0.305884   0.439083  0.695346  0.834871   \n",
       "MLPRegressor               0.196292  0.312627   0.443049  0.689818  0.831816   \n",
       "\n",
       "                           Test SCC  \n",
       "LGBMRegressor              0.820005  \n",
       "DecisionTreeRegressor      0.803776  \n",
       "RandomForestRegressor      0.816087  \n",
       "GradientBoostingRegressor  0.819767  \n",
       "AdaBoostRegressor          0.807913  \n",
       "XGBRegressor               0.815774  \n",
       "ExtraTreesRegressor        0.815771  \n",
       "LinearRegression           0.817838  \n",
       "KNeighborsRegressor        0.803488  \n",
       "SVR                        0.818940  \n",
       "MLPRegressor               0.817597  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "for ablation_label, df in ablation_results.items():\n",
    "    print(f\"Results for {ablation_label}: \\n\")\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42634a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a4a4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289747c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df891e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6532c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe5c0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13fa809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b56741d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4bf1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4098e5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688abf8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a10404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b65813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478d871d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d052087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4df83d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065e68e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d61f55a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d2c6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c592e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e9bba0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python py312",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
